<?xml version="1.0" encoding="utf-8"?>
<AbstractBook>
<Conference>
Montenegro, Budva, Becici, 25 September - 29 September 2017
</Conference>
<abstract>
<Id>111</Id>
<Title>
Heavy ion induced nuclear reaction experiments with DSSSD detectors: an equivalent circuit for p-n junction side neighbor strips and specific application features.
</Title>
<Content>
It was namely the Dubna Gas-Filled Separator (DGFRS) team, which successfully and uncontested discovered new superheavy elements with Z=114,115-118. During the DGFRS experiment different type of silicon radiation detectors were used. For example, n-Si (Au) surface barrier detectors, resistive strip position sensitive PIPS detectors were used in the first experiments. Large area DSSSD (Double Side Silicon Strip Detector) detectors are in use now. The higher granularity is a main advantage of this type detector in comparison with the resistive layer PIPS (Passivated Implanted Planar Silicon) one. From the other hands, some specifics of its application, like edge effects definitely one should take into account, especially, when processing the signals signal from p-n junction side of the detector [1-5]. An equivalent circuit for two neighbor strips of p-n junction side is proposed. It predicts a small non-linear ballistic effect for signals originating in inter-strip p-n junction area. natYb+48Ca *Th experiment has been performed at main U-400 FLNR cyclotron to extract correlated sequences ER-α for 217Th implanted into a focal plane DSSSD detector nuclei. It confirms a conclusion following from equivalent circuit consideration about a ballistic nature of non-linear effect. Results of 240Pu +48Ca Fl* experiment [6] are considered from the viewpoint of registered signal amplitudes of Z=114 ER’s (Evaporation Residues) measured with the DGFRS DSSSD Micron Semiconductors (UK) focal plane detector. A role of SiO2 –Si p-n junction side interface surface states density value and, consequently, located here negative charge value, is under discussion in a qualitative form. A PC Builder C++ based algorithm to take into account the edge effect to search for short ER-α (-α) … α- sequences in a real-time mode is presented in details too. References [1] Yu.S.Tsyganov // Phys. Part. Nucl. Lett. 2016. Vol.13, No.5. pp. 5567-572 [2] Yu.S.Tsyganov // Phys. Part. Nucl. Lett. 2014. Vol.11, No.6. pp. 1-6 [3] Yu.Tsyganov and A.Polyakov // Cybernetics and Physics. 2014. Vol.3, No.2.pp.85-89 [4] Yu.S. Tsyganov // Report at IEEE-2016 NSS-MIC Symposium. 26 Oct.- 05 Nov. 2016, Strasbourg, France. Talk_N64-7. [5] Yu.S.Tsyganov // arXiv.2016. ID-1611.07787 [6] Yu.Ts. Oganessian et al. // Phys. Rev. C 92. 034609(2015).
</Content>
<field id="content">
It was namely the Dubna Gas-Filled Separator (DGFRS) team, which successfully and uncontested discovered new superheavy elements with Z=114,115-118. During the DGFRS experiment different type of silicon radiation detectors were used. For example, n-Si (Au) surface barrier detectors, resistive strip position sensitive PIPS detectors were used in the first experiments. Large area DSSSD (Double Side Silicon Strip Detector) detectors are in use now. The higher granularity is a main advantage of this type detector in comparison with the resistive layer PIPS (Passivated Implanted Planar Silicon) one. From the other hands, some specifics of its application, like edge effects definitely one should take into account, especially, when processing the signals signal from p-n junction side of the detector [1-5]. An equivalent circuit for two neighbor strips of p-n junction side is proposed. It predicts a small non-linear ballistic effect for signals originating in inter-strip p-n junction area. natYb+48Ca *Th experiment has been performed at main U-400 FLNR cyclotron to extract correlated sequences ER-α for 217Th implanted into a focal plane DSSSD detector nuclei. It confirms a conclusion following from equivalent circuit consideration about a ballistic nature of non-linear effect. Results of 240Pu +48Ca Fl* experiment [6] are considered from the viewpoint of registered signal amplitudes of Z=114 ER’s (Evaporation Residues) measured with the DGFRS DSSSD Micron Semiconductors (UK) focal plane detector. A role of SiO2 –Si p-n junction side interface surface states density value and, consequently, located here negative charge value, is under discussion in a qualitative form. A PC Builder C++ based algorithm to take into account the edge effect to search for short ER-α (-α) … α- sequences in a real-time mode is presented in details too. References [1] Yu.S.Tsyganov // Phys. Part. Nucl. Lett. 2016. Vol.13, No.5. pp. 5567-572 [2] Yu.S.Tsyganov // Phys. Part. Nucl. Lett. 2014. Vol.11, No.6. pp. 1-6 [3] Yu.Tsyganov and A.Polyakov // Cybernetics and Physics. 2014. Vol.3, No.2.pp.85-89 [4] Yu.S. Tsyganov // Report at IEEE-2016 NSS-MIC Symposium. 26 Oct.- 05 Nov. 2016, Strasbourg, France. Talk_N64-7. [5] Yu.S.Tsyganov // arXiv.2016. ID-1611.07787 [6] Yu.Ts. Oganessian et al. // Phys. Rev. C 92. 034609(2015).
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Yury</FirstName>
<FamilyName>Tsyganov</FamilyName>
<Email>tyra@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Yury</FirstName>
<FamilyName>Tsyganov</FamilyName>
<Email>tyra@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>Oral</ContributionType>
<Track>Detector &amp; Nuclear Electronics</Track>
</abstract>
<abstract>
<Id>112</Id>
<Title>
An upgraded TOF-ΔE1-ΔE2-E (DSSSD) based spectrometer for heavy-element research at the Dubna Gas-Filled Recoil Separator
</Title>
<Content>
Two scenarios of modifying the DGFRS (the Dubna Gas Filled Recoil Separator) spectrometer of rare alpha decays are under consideration. Both of them imply use of integral 1M CAMAC analog-to-digital processor TekhInvest ADP-16 [1,2] as a basic unit in the spectrometer design. In scenario a) special unit (PKK-05) [3] will be used to measure horizontal position of the signal, without measuring its energy, whereas in scenario b) a complete amount (12 modules ADP-16 for 48x128 strips of DSSSD) are used to measure both energy and position signals. To measure signals of charged particles coming from cyclotron an upgraded gaseous low pressure TOF-ΔE1-ΔE2 module is used. To store TOF-ΔE1-ΔE2 information specific 1M module TekhInvest PA-3n-tof is used. First results of trial runs using the specific TekhInvest IMI-2011 pulser and test nuclear reaction natYb+48CaTh* are presented. New algorithm to search for ER-α-α…α(SF) sequences in a real-time mode is discussed taking into account commissioning in the nearest future of the new FLNR DC-280 cyclotron that is to provide beams of very high intensity [4]. An equivalent circuit for two neighbor strips of p-n junction side is proposed. It predicts a small non-linear ballistic effect for signals originating in inter-strip p-n junction area. Additionally, authors define abstract mathematical objects, like correlation graph and incoming event matrixes of a different nature to construct in a simple form a rare event detection procedure in a more exhaustive relatively the present one, using real-time detection mode. In that case one can use every from n∙(n-1)/2 correlation graph edges are used as a “trigger” for beam irradiation pauses to provide a “background free” condition to search for ultra rare alpha decays. Here n is a correlation graph nodes number. Schematics of these algorithms are considered. References [1] Yu.S.Tsyganov //Lett. to ECHAYA, 2016. Vol.13(203) pp.898-904 [2] A.N.Kuznetsov // ADP-16 TekhInvest manual. [3] V.G.Subbotin, A.M.Zubareva,A.A.Voinov, A.N.Zubarev, L.Schlattauer //Lett. to ECHAYA, 2016. Vol.13(203) pp.885-889 [4] G.G.Gulbekyan et al. // Project of DC-280 cyclotron Report at JINR Nucl. Phys. PAC
</Content>
<field id="content">
Two scenarios of modifying the DGFRS (the Dubna Gas Filled Recoil Separator) spectrometer of rare alpha decays are under consideration. Both of them imply use of integral 1M CAMAC analog-to-digital processor TekhInvest ADP-16 [1,2] as a basic unit in the spectrometer design. In scenario a) special unit (PKK-05) [3] will be used to measure horizontal position of the signal, without measuring its energy, whereas in scenario b) a complete amount (12 modules ADP-16 for 48x128 strips of DSSSD) are used to measure both energy and position signals. To measure signals of charged particles coming from cyclotron an upgraded gaseous low pressure TOF-ΔE1-ΔE2 module is used. To store TOF-ΔE1-ΔE2 information specific 1M module TekhInvest PA-3n-tof is used. First results of trial runs using the specific TekhInvest IMI-2011 pulser and test nuclear reaction natYb+48CaTh* are presented. New algorithm to search for ER-α-α…α(SF) sequences in a real-time mode is discussed taking into account commissioning in the nearest future of the new FLNR DC-280 cyclotron that is to provide beams of very high intensity [4]. An equivalent circuit for two neighbor strips of p-n junction side is proposed. It predicts a small non-linear ballistic effect for signals originating in inter-strip p-n junction area. Additionally, authors define abstract mathematical objects, like correlation graph and incoming event matrixes of a different nature to construct in a simple form a rare event detection procedure in a more exhaustive relatively the present one, using real-time detection mode. In that case one can use every from n∙(n-1)/2 correlation graph edges are used as a “trigger” for beam irradiation pauses to provide a “background free” condition to search for ultra rare alpha decays. Here n is a correlation graph nodes number. Schematics of these algorithms are considered. References [1] Yu.S.Tsyganov //Lett. to ECHAYA, 2016. Vol.13(203) pp.898-904 [2] A.N.Kuznetsov // ADP-16 TekhInvest manual. [3] V.G.Subbotin, A.M.Zubareva,A.A.Voinov, A.N.Zubarev, L.Schlattauer //Lett. to ECHAYA, 2016. Vol.13(203) pp.885-889 [4] G.G.Gulbekyan et al. // Project of DC-280 cyclotron Report at JINR Nucl. Phys. PAC
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Yury</FirstName>
<FamilyName>Tsyganov</FamilyName>
<Email>tyra@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Alexandr</FirstName>
<FamilyName>Polyakov</FamilyName>
<Email>polyakov@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Alexey</FirstName>
<FamilyName>Voinov</FamilyName>
<Email>voinov@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Leo</FirstName>
<FamilyName>Schlattauer</FamilyName>
<Email>schlattauer@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Maxim</FirstName>
<FamilyName>Shumeiko</FamilyName>
<Email>eastwoodknight@rambler.ru</Email>
<Affiliation>FLNR,JINR</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Yury</FirstName>
<FamilyName>Tsyganov</FamilyName>
<Email>tyra@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>Detector &amp; Nuclear Electronics</Track>
</abstract>
<abstract>
<Id>113</Id>
<Title>
Control System Design for New Electronic Cooler of HIRFL-CSRm
</Title>
<Content>
This paper details the architecture, design, and testing of an accurate and usable electronic cooler control system for the cooling storage main ring at the Heavy Ion Research Facility, Lanzhou, China (HIRFL-CSRm), and present the results of its use. The software for the system, based on Visual C++, was developed following the model-view-controller architecture pattern. On the hardware side, control system based on network and standard bus concert with distributed computing. The communication protocol was analyzed, discussed, and implemented, and the control system was then tested with the 12C3+ carbon beam at HIRFL-CSRm. The fast reaction time and high-precision data processing exhibited during beam tuning verified the stability and maintainability of the proposed control system.
</Content>
<field id="content">
This paper details the architecture, design, and testing of an accurate and usable electronic cooler control system for the cooling storage main ring at the Heavy Ion Research Facility, Lanzhou, China (HIRFL-CSRm), and present the results of its use. The software for the system, based on Visual C++, was developed following the model-view-controller architecture pattern. On the hardware side, control system based on network and standard bus concert with distributed computing. The communication protocol was analyzed, discussed, and implemented, and the control system was then tested with the 12C3+ carbon beam at HIRFL-CSRm. The fast reaction time and high-precision data processing exhibited during beam tuning verified the stability and maintainability of the proposed control system.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Yanyu</FirstName>
<FamilyName>Wang</FamilyName>
<Email>yanyu@impcas.ac.cn</Email>
<Affiliation>
Institute of Modern Physics, Chinese Academy of Sciences
</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Jianjun</FirstName>
<FamilyName>Su</FamilyName>
<Email>sujianjun@impcas.ac.cn</Email>
<Affiliation>
Institute of Modern Physics, Chinese Academy of Sciences
</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Jianjun</FirstName>
<FamilyName>Su</FamilyName>
<Email>sujianjun@impcas.ac.cn</Email>
<Affiliation>
Institute of Modern Physics, Chinese Academy of Sciences
</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Triggering, Data Acquisition, Control Systems</Track>
</abstract>
<abstract>
<Id>114</Id>
<Title>
Embedding of containerization technology in the core of the virtual computer lab
</Title>
<Content>
When training highly skilled IT professionals, it is an important challenge for the university to teach professional competencies to graduates that they will be able to use to successfully solve a broad range of substantive problems that arise at all stages of the lifecycle of corporate information systems. Such information systems in practice, as a rule, are used for enterprise management, workflow management in technological processes, IT infrastructure management, creating web-solutions for high availability, data collection, and data analysis and storage. It is obvious that for students to learn these professional competencies, they need to master a large amount of theoretical material and to carry out practical exercises and research on the development of modern information systems, their deployment and support, the effective implementation of solutions for problem-oriented tasks, etc. The virtual computer lab provides a set of software and hardware-based virtualization and containerization tools that enable the flexible and on-demand provision and use of computing resources in the form of "cloud" Internet services for carrying out research projects, resource-intensive computational calculations and tasks related to the development of complex corporate and other distributed information systems. The service also provides dedicated virtual servers for innovative projects that are carried out by students and staff at the Institute of System Analysis and Control. The introduction of containerization technology serves to improve the process of corporate information systems deployment and use in the training of IT professionals. Compared to classical virtualization, the underlying operating system kernel can be used for all containers. On the one hand, it introduces restrictions on the use of other operating systems while, on the other hand, it improves payload on the north of a similar configuration. This can be achieved due to the specifics of the containerization architecture, which we will examine on the example of Dockers. Docker uses a client-server architecture in which the Docker-client interacts with the Docker daemon, enabling the operations of creating and launching containers on the server and providing them to students. In general terms, a containerization system can be represented in the form of three key components: images, registries, and containers. Images represent read-only templates that contain an operating system based on the same kernel version as the host system with necessary pre-configured and adapted software. These images are created, modified if necessary and then used for generating individual solitary containers. The images are stored in the registry (the registry is a constituent of the storage and distribution of images) and formed on the basis of the curriculum of courses and laboratory work plans prepared by the teaching staff. However, public hubs containing a large collection of images created by independent enthusiasts can be used to download the required images. The containers per se are, in fact, similar to catalogs (directories) of an operating system, where all the changes made by the user and the system software in the course of work are stored. Each container is created from an image, providing the capacity to quickly create, start, stop, move, and delete, and is a safe sandbox for running applications, allowing the student to carry out any experiments without compromising the base operating system, while maintaining the highest level of performance. The virtual computer lab has helped us provide an optimal and sustainable technological, educational-organizational, scientific methodological, and regulatory-administrative environment for supporting innovative approaches to computer education. It promotes the integration of the scientific and educational potential of Dubna State University and the formation of industry and academic research partnerships with leading international companies that are potential employers of graduates of the Institute of System Analysis and Control.
</Content>
<field id="content">
When training highly skilled IT professionals, it is an important challenge for the university to teach professional competencies to graduates that they will be able to use to successfully solve a broad range of substantive problems that arise at all stages of the lifecycle of corporate information systems. Such information systems in practice, as a rule, are used for enterprise management, workflow management in technological processes, IT infrastructure management, creating web-solutions for high availability, data collection, and data analysis and storage. It is obvious that for students to learn these professional competencies, they need to master a large amount of theoretical material and to carry out practical exercises and research on the development of modern information systems, their deployment and support, the effective implementation of solutions for problem-oriented tasks, etc. The virtual computer lab provides a set of software and hardware-based virtualization and containerization tools that enable the flexible and on-demand provision and use of computing resources in the form of "cloud" Internet services for carrying out research projects, resource-intensive computational calculations and tasks related to the development of complex corporate and other distributed information systems. The service also provides dedicated virtual servers for innovative projects that are carried out by students and staff at the Institute of System Analysis and Control. The introduction of containerization technology serves to improve the process of corporate information systems deployment and use in the training of IT professionals. Compared to classical virtualization, the underlying operating system kernel can be used for all containers. On the one hand, it introduces restrictions on the use of other operating systems while, on the other hand, it improves payload on the north of a similar configuration. This can be achieved due to the specifics of the containerization architecture, which we will examine on the example of Dockers. Docker uses a client-server architecture in which the Docker-client interacts with the Docker daemon, enabling the operations of creating and launching containers on the server and providing them to students. In general terms, a containerization system can be represented in the form of three key components: images, registries, and containers. Images represent read-only templates that contain an operating system based on the same kernel version as the host system with necessary pre-configured and adapted software. These images are created, modified if necessary and then used for generating individual solitary containers. The images are stored in the registry (the registry is a constituent of the storage and distribution of images) and formed on the basis of the curriculum of courses and laboratory work plans prepared by the teaching staff. However, public hubs containing a large collection of images created by independent enthusiasts can be used to download the required images. The containers per se are, in fact, similar to catalogs (directories) of an operating system, where all the changes made by the user and the system software in the course of work are stored. Each container is created from an image, providing the capacity to quickly create, start, stop, move, and delete, and is a safe sandbox for running applications, allowing the student to carry out any experiments without compromising the base operating system, while maintaining the highest level of performance. The virtual computer lab has helped us provide an optimal and sustainable technological, educational-organizational, scientific methodological, and regulatory-administrative environment for supporting innovative approaches to computer education. It promotes the integration of the scientific and educational potential of Dubna State University and the formation of industry and academic research partnerships with leading international companies that are potential employers of graduates of the Institute of System Analysis and Control.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Evgenia</FirstName>
<FamilyName>Cheremisina</FamilyName>
<Email>kirpicheva77@gmail.com</Email>
<Affiliation>State Dubna University</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Mikhail</FirstName>
<FamilyName>Belov</FamilyName>
<Email>belov@uni-dubna.ru</Email>
<Affiliation>State Dubna Univeristy</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Nadezhda</FirstName>
<FamilyName>Tokareva</FamilyName>
<Email>tokareva@uni-dubna.ru</Email>
<Affiliation>State Dubna Univeristy</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Stanislav</FirstName>
<FamilyName>Grishko</FamilyName>
<Email>grishkostanislavrus@gmail.com</Email>
<Affiliation>State Dubna University</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Andrew</FirstName>
<FamilyName>Nabiullin</FamilyName>
<Email>itcomusic@gmail.com</Email>
<Affiliation>State Dubna University</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Alexander</FirstName>
<FamilyName>Sorokin</FamilyName>
<Email>alexander_sorokin@ru.ibm.com</Email>
<Affiliation>IBM East Europe/Asia</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Evgenia</FirstName>
<FamilyName>Cheremisina</FamilyName>
<Email>kirpicheva77@gmail.com</Email>
<Affiliation>State Dubna University</Affiliation>
</Speaker>
<Speaker>
<FirstName>Mikhail</FirstName>
<FamilyName>Belov</FamilyName>
<Email>belov@uni-dubna.ru</Email>
<Affiliation>State Dubna Univeristy</Affiliation>
</Speaker>
<Speaker>
<FirstName>Nadezhda</FirstName>
<FamilyName>Tokareva</FamilyName>
<Email>tokareva@uni-dubna.ru</Email>
<Affiliation>State Dubna Univeristy</Affiliation>
</Speaker>
<Speaker>
<FirstName>Stanislav</FirstName>
<FamilyName>Grishko</FamilyName>
<Email>grishkostanislavrus@gmail.com</Email>
<Affiliation>State Dubna University</Affiliation>
</Speaker>
<Speaker>
<FirstName>Andrew</FirstName>
<FamilyName>Nabiullin</FamilyName>
<Email>itcomusic@gmail.com</Email>
<Affiliation>State Dubna University</Affiliation>
</Speaker>
<Speaker>
<FirstName>Alexander</FirstName>
<FamilyName>Sorokin</FamilyName>
<Email>alexander_sorokin@ru.ibm.com</Email>
<Affiliation>IBM East Europe/Asia</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Innovative IT Education</Track>
</abstract>
<abstract>
<Id>115</Id>
<Title>
An attempt to build a smart real-time detection system for heavy element research: approaches, mathematical objects and algorithms
</Title>
<Content>
The Dubna Gas-Filled Recoil Separator is the most advanced facility currently in use in the field of research of Superheavy Nuclei (SHN) [1]. During last years, IUPAC established the priority of the DGFRS experiments in the discovery of new Z=114-118 elements. Definitely, the DGFRS detection system and method of “active correlations” have played a significant role in these discoveries [2-4]. Author defines abstract mathematical objects, like correlation graph and incoming event matrixes of a different nature in order to construct a simple procedure of detecting rare events, yet more exhaustive compared to the present one, using real-time detection mode. In this case one can use any of n∙(n-1)/2 correlation graph edges to “trigger” beam irradiation pauses and thus provide “background free” conditions to search for ultra-rare alpha decays. Here n is the number of correlation graph nodes. Schematics of these algorithms are considered. Elapsed time value is used as matrix element for each event type. In the case of DSSSD detector-based system those matrices have dimension (X,Y), where X , Y are the numbers of horizontal and vertical strips, respectively. Proposals for test experiments with heavy-ion-induced reactions that can be used to check these approaches are considered. Some attention is paid to the forthcoming launch of a new FLNR ultra intense heavy ion DC-280 cyclotron for heavy element research. A role of a new protection and parameter monitoring system is discussed in a brief. Method to use more flexible correlation time intervals, using e.g. either V.B. Zlokazov’s BSC (Background Signal Combinations) or K.-H. Schmidt’s LDSC (Linked Decay Signal Combinations) approach [5-7], except for fixed time intervals, is reported. In the last case, condition for the beam stop can be considered in the form of equation:Pcorr(t,ν,i)&lt;ε , where Pcorr is random probability value for a given correlation chain (i) measured in a real-time mode at the moment t, ν is mean rate parameter for a given DSSSD pixel, and ε is a preset small positive value. Of course, in this case one should additionally define one extra matrix related to event rate for each pixel of DSSSD. References [1] P.J.Carol,R.C.Barber, B.M.Sherill, E.Vardaci and T.Yamazaki // IUPAC Technical Rep., Pure.Appl.Chem, 2016; aop. (see also, JINR press-relize, Dec.2016) [2] Yu.S.Tsyganov, A.N.Polyakov and A.M.Sukhov //Nucl. Instrum. And Meth. In Phys. Res. 2003. A513. Pp.413-416 [3] Yu.S.Tsyganov // Phys. Part. Nucl. 2016.vol.47, No.1. pp. 73-107 [4] Yu.S.Tsyganov // Phys. Part. Nucl. Lett. 2014. Vol.45, nos.5-6. Pp 1485-1531 [5] V.B.Zlokazov // Eur. Phys. J. 2002. A 14. P.147 [6] K.-H. Schmidt et al. // Z. Phys. A- Atoms and Nuclei. 1984. Vol.316. pp.19-26 [7] V.B.Zlokazov and Yu.S.Tsyganov // Lett. to ECHAYA. 2010. Vol.7, №6 (162), pp.658-666 /in Russian/
</Content>
<field id="content">
The Dubna Gas-Filled Recoil Separator is the most advanced facility currently in use in the field of research of Superheavy Nuclei (SHN) [1]. During last years, IUPAC established the priority of the DGFRS experiments in the discovery of new Z=114-118 elements. Definitely, the DGFRS detection system and method of “active correlations” have played a significant role in these discoveries [2-4]. Author defines abstract mathematical objects, like correlation graph and incoming event matrixes of a different nature in order to construct a simple procedure of detecting rare events, yet more exhaustive compared to the present one, using real-time detection mode. In this case one can use any of n∙(n-1)/2 correlation graph edges to “trigger” beam irradiation pauses and thus provide “background free” conditions to search for ultra-rare alpha decays. Here n is the number of correlation graph nodes. Schematics of these algorithms are considered. Elapsed time value is used as matrix element for each event type. In the case of DSSSD detector-based system those matrices have dimension (X,Y), where X , Y are the numbers of horizontal and vertical strips, respectively. Proposals for test experiments with heavy-ion-induced reactions that can be used to check these approaches are considered. Some attention is paid to the forthcoming launch of a new FLNR ultra intense heavy ion DC-280 cyclotron for heavy element research. A role of a new protection and parameter monitoring system is discussed in a brief. Method to use more flexible correlation time intervals, using e.g. either V.B. Zlokazov’s BSC (Background Signal Combinations) or K.-H. Schmidt’s LDSC (Linked Decay Signal Combinations) approach [5-7], except for fixed time intervals, is reported. In the last case, condition for the beam stop can be considered in the form of equation:Pcorr(t,ν,i)&lt;ε , where Pcorr is random probability value for a given correlation chain (i) measured in a real-time mode at the moment t, ν is mean rate parameter for a given DSSSD pixel, and ε is a preset small positive value. Of course, in this case one should additionally define one extra matrix related to event rate for each pixel of DSSSD. References [1] P.J.Carol,R.C.Barber, B.M.Sherill, E.Vardaci and T.Yamazaki // IUPAC Technical Rep., Pure.Appl.Chem, 2016; aop. (see also, JINR press-relize, Dec.2016) [2] Yu.S.Tsyganov, A.N.Polyakov and A.M.Sukhov //Nucl. Instrum. And Meth. In Phys. Res. 2003. A513. Pp.413-416 [3] Yu.S.Tsyganov // Phys. Part. Nucl. 2016.vol.47, No.1. pp. 73-107 [4] Yu.S.Tsyganov // Phys. Part. Nucl. Lett. 2014. Vol.45, nos.5-6. Pp 1485-1531 [5] V.B.Zlokazov // Eur. Phys. J. 2002. A 14. P.147 [6] K.-H. Schmidt et al. // Z. Phys. A- Atoms and Nuclei. 1984. Vol.316. pp.19-26 [7] V.B.Zlokazov and Yu.S.Tsyganov // Lett. to ECHAYA. 2010. Vol.7, №6 (162), pp.658-666 /in Russian/
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Yury</FirstName>
<FamilyName>Tsyganov</FamilyName>
<Email>tyra@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Yury</FirstName>
<FamilyName>Tsyganov</FamilyName>
<Email>tyra@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Detector &amp; Nuclear Electronics</Track>
</abstract>
<abstract>
<Id>116</Id>
<Title>
The development of Online Event Display using ATLAS TDAQ for the NICA experiments
</Title>
<Content>
One of the problems to be solved in high energy physics experiments on particle collisions and fixed target experiments is online visual presentation of the events during the experiment run. The report describes the implementation of this task, so called Online Event Display, for the current BM@N experiment and the future experiment MPD (Multi-Purpose Detector) at the Nuclotron-based Ion Collider facility (NICA) under construction at the Joint Institute for Nuclear Research. One of the main aspects of the development, which will be shown in the presentation, is the integration of ATLAS TDAQ components to transfer raw event data for visualization in the Online Event Display. The report includes brief description of these TDAQ components. Another important issue that will be discussed is dedicated to speeding up the track reconstruction to increase the number of the events viewed in the monitoring system per second. The implemented event display designed for use in offline and online modes with its options and features as well as integration with our software environments (BmnRoot and MpdRoot) are considered. The examples of graphical representation of simulated and reconstructed points and particle tracks with BM@N and MPD geometries will be shown for collisions with different energies and particles, such as deuterons, carbons and gold ions.
</Content>
<field id="content">
One of the problems to be solved in high energy physics experiments on particle collisions and fixed target experiments is online visual presentation of the events during the experiment run. The report describes the implementation of this task, so called Online Event Display, for the current BM@N experiment and the future experiment MPD (Multi-Purpose Detector) at the Nuclotron-based Ion Collider facility (NICA) under construction at the Joint Institute for Nuclear Research. One of the main aspects of the development, which will be shown in the presentation, is the integration of ATLAS TDAQ components to transfer raw event data for visualization in the Online Event Display. The report includes brief description of these TDAQ components. Another important issue that will be discussed is dedicated to speeding up the track reconstruction to increase the number of the events viewed in the monitoring system per second. The implemented event display designed for use in offline and online modes with its options and features as well as integration with our software environments (BmnRoot and MpdRoot) are considered. The examples of graphical representation of simulated and reconstructed points and particle tracks with BM@N and MPD geometries will be shown for collisions with different energies and particles, such as deuterons, carbons and gold ions.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Konstantin</FirstName>
<FamilyName>Gertsenberger</FamilyName>
<Email>gertsen@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Mikhail</FirstName>
<FamilyName>Mineev</FamilyName>
<Email>mineev@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Konstantin</FirstName>
<FamilyName>Gertsenberger</FamilyName>
<Email>gertsen@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>
Computing for Large Scale Facilities (LHC, FAIR, NICA, SKA, PIC, XFEL, ELI, etc.)
</Track>
</abstract>
<abstract>
<Id>117</Id>
<Title>Trigger electronics for BM@N setup in 2017</Title>
<Content>
The BM@N facility is a fixed target experiment based on heavy ion beams of the Nuclotron-M accelerator. The aim of the BM@N is to study nucleus – nucleus collisions at energies up to 4.5 GeV per nucleon.	Our group is responsible to develop triggers system for this experiment. The described trigger system has been developed at LHEP/JINR for trigger generation in the BM@N experiments. The trigger and start detectors fast signals of MCP-PMTs and SiPMs are used as input signals for the trigger processing. The trigger system consist of detectors with fast front-end electronics (FEE), power supplies for detectors and FEE and a level 0 trigger processor unit (Trigger L0 unit, T0U). The T0U is used to generate a BM@N zero level trigger and a TOF detector precise start. T0U generates trigger signal based on the beam line, the target area and the barrel detector signals. This report presents a concept, characteristics and a performance of the trigger system during the B@MN last runs.
</Content>
<field id="content">
The BM@N facility is a fixed target experiment based on heavy ion beams of the Nuclotron-M accelerator. The aim of the BM@N is to study nucleus – nucleus collisions at energies up to 4.5 GeV per nucleon.	Our group is responsible to develop triggers system for this experiment. The described trigger system has been developed at LHEP/JINR for trigger generation in the BM@N experiments. The trigger and start detectors fast signals of MCP-PMTs and SiPMs are used as input signals for the trigger processing. The trigger system consist of detectors with fast front-end electronics (FEE), power supplies for detectors and FEE and a level 0 trigger processor unit (Trigger L0 unit, T0U). The T0U is used to generate a BM@N zero level trigger and a TOF detector precise start. T0U generates trigger signal based on the beam line, the target area and the barrel detector signals. This report presents a concept, characteristics and a performance of the trigger system during the B@MN last runs.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Victor</FirstName>
<FamilyName>Rogov</FamilyName>
<Email>rogovictor@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Sergey</FirstName>
<FamilyName>Sergeev</FamilyName>
<Email>serguei.sergueev@mail.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Vladimir</FirstName>
<FamilyName>Yurevich</FamilyName>
<Email>vyurevi@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Dmitriy</FirstName>
<FamilyName>Bogoslovki</FamilyName>
<Email>bogoslovski.dimson@mail.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Victor</FirstName>
<FamilyName>Rogov</FamilyName>
<Email>rogovictor@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>Oral</ContributionType>
<Track>Triggering, Data Acquisition, Control Systems</Track>
</abstract>
<abstract>
<Id>118</Id>
<Title>A control system for the BM@N trigger electronics</Title>
<Content>
To control and monitor the MB@N trigger electronics the program package has been developed. This package provides control and monitoring of the front-end LV and PMT HV power supplies and the trigger electronics itself. To publish spill intensity in a real-time mode a client and a server have been developed. Package also includes a web-server to publish the spill summary information and an interface program for the Detector Control System of the BM@N experiment. All programs do have a graphical user interface to simplify manipulation with the trigger hardware.
</Content>
<field id="content">
To control and monitor the MB@N trigger electronics the program package has been developed. This package provides control and monitoring of the front-end LV and PMT HV power supplies and the trigger electronics itself. To publish spill intensity in a real-time mode a client and a server have been developed. Package also includes a web-server to publish the spill summary information and an interface program for the Detector Control System of the BM@N experiment. All programs do have a graphical user interface to simplify manipulation with the trigger hardware.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Sergey</FirstName>
<FamilyName>Sergeev</FamilyName>
<Email>serguei.sergueev@mail.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Victor</FirstName>
<FamilyName>Rogov</FamilyName>
<Email>rogovictor@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Dmitri</FirstName>
<FamilyName>Bogoslovski</FamilyName>
<Email>bogoslovski.dimson@mail.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Vladimir</FirstName>
<FamilyName>Yurevich</FamilyName>
<Email>vyurevi@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Sergey</FirstName>
<FamilyName>Sergeev</FamilyName>
<Email>serguei.sergueev@mail.ru</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>Oral</ContributionType>
<Track>Triggering, Data Acquisition, Control Systems</Track>
</abstract>
<abstract>
<Id>119</Id>
<Title>
The software for the internal beam diagnostics of Nuclotron
</Title>
<Content>
The superconducting synchrotron Nuclotron is the base of a new accelerator complex NICA designed at the LHEP, JINR. It is very important to monitor dynamics of its internal beam intensity during an acceleration cycle for proper tuning and functioning of the setup. The new parametric current transformer of Bergoz Instrumentation with frequency response DС to 10 kHz is used for measuring the beam DC intensity in the Nuclotron ring. Data acquisition is performed by multifunctional DAQ device NI6284 (18-bit) of National Instruments. The software complex provides an efficient work of intensity monitoring subsystem. It consists of the subsystem’s server and the several clients specialized for operators, stuff member and experimentalists. The software is designed in the TANGO Controls concept and fully integrated into the TANGO system of NICA. The structure, principle of functioning, operational experience in the recent Nuclotron runs and further improvement of the software complex are reported.
</Content>
<field id="content">
The superconducting synchrotron Nuclotron is the base of a new accelerator complex NICA designed at the LHEP, JINR. It is very important to monitor dynamics of its internal beam intensity during an acceleration cycle for proper tuning and functioning of the setup. The new parametric current transformer of Bergoz Instrumentation with frequency response DС to 10 kHz is used for measuring the beam DC intensity in the Nuclotron ring. Data acquisition is performed by multifunctional DAQ device NI6284 (18-bit) of National Instruments. The software complex provides an efficient work of intensity monitoring subsystem. It consists of the subsystem’s server and the several clients specialized for operators, stuff member and experimentalists. The software is designed in the TANGO Controls concept and fully integrated into the TANGO system of NICA. The structure, principle of functioning, operational experience in the recent Nuclotron runs and further improvement of the software complex are reported.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Vasily</FirstName>
<FamilyName>Andreev</FamilyName>
<Email>vandreev@jinr.ru</Email>
<Affiliation>VBLHEP JINR</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Evgeny</FirstName>
<FamilyName>Gorbachev</FamilyName>
<Email>egorbe@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Valery</FirstName>
<FamilyName>Volkov</FamilyName>
<Email>vvolkov@jinr.ru</Email>
<Affiliation>VBLHEP JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Viktor</FirstName>
<FamilyName>Isadov</FamilyName>
<Email>isadov@jinr.ru</Email>
<Affiliation>VBLHEP JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Vladimir</FirstName>
<FamilyName>Elkin</FamilyName>
<Email>elkin@jinr.ru</Email>
<Affiliation>VBLHEP JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Andrey</FirstName>
<FamilyName>Butenko</FamilyName>
<Email>butenko@jinr.ru</Email>
<Affiliation>VBLHEP JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Sergei</FirstName>
<FamilyName>Romanov</FamilyName>
<Email>romanov@jinr.ru</Email>
<Affiliation>VBLHEP JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Alexandr</FirstName>
<FamilyName>Kirichenko</FamilyName>
<Email>akir20041957@mail.ru</Email>
<Affiliation>VBLHEP JINR</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Vasily</FirstName>
<FamilyName>Andreev</FamilyName>
<Email>vandreev@jinr.ru</Email>
<Affiliation>VBLHEP JINR</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Triggering, Data Acquisition, Control Systems</Track>
<Track>
Computing for Large Scale Facilities (LHC, FAIR, NICA, SKA, PIC, XFEL, ELI, etc.)
</Track>
</abstract>
<abstract>
<Id>120</Id>
<Title>COMPASS Grid Production System</Title>
<Content>
LHC Computing Grid was a pioneer integration effort, managed to unite computing and storage resources all over the world, thus made them available to experiments on Large Hadron Collider. During decade of LHC computing, Grid software has learned to effectively utilize different types of computing resources, such as classic computing clusters, clouds and hyper power computers. And while the resources experiments use are the same, data flow differs from experiment to experiment. A crucial part of each experiment computing is a production system, which describes logic and controls data processing of the experiment. COMPASS always relied on CERN facilities, and, when CERN, during hardware and software upgrade, started migration to resources, available only via Grid, faced the problem of insufficiency of resources to process data on. To make COMPASS data processing able to work via Grid, development of the new production system has started. Key features of modern production system for COMPASS are: distributed data processing, support of different type of computing resources, support of arbitrary amount of computing sites. Build blocks for the production system are taken from achievements of LHC experiments, but logic of data processing is COMPASS-specific and unique. Details of implementation of Grid production system for COMPASS are described in the report.
</Content>
<field id="content">
LHC Computing Grid was a pioneer integration effort, managed to unite computing and storage resources all over the world, thus made them available to experiments on Large Hadron Collider. During decade of LHC computing, Grid software has learned to effectively utilize different types of computing resources, such as classic computing clusters, clouds and hyper power computers. And while the resources experiments use are the same, data flow differs from experiment to experiment. A crucial part of each experiment computing is a production system, which describes logic and controls data processing of the experiment. COMPASS always relied on CERN facilities, and, when CERN, during hardware and software upgrade, started migration to resources, available only via Grid, faced the problem of insufficiency of resources to process data on. To make COMPASS data processing able to work via Grid, development of the new production system has started. Key features of modern production system for COMPASS are: distributed data processing, support of different type of computing resources, support of arbitrary amount of computing sites. Build blocks for the production system are taken from achievements of LHC experiments, but logic of data processing is COMPASS-specific and unique. Details of implementation of Grid production system for COMPASS are described in the report.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Artem</FirstName>
<FamilyName>Petrosyan</FamilyName>
<Email>artem.petrosyan@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Artem</FirstName>
<FamilyName>Petrosyan</FamilyName>
<Email>artem.petrosyan@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Distributed Computing. GRID &amp; Cloud Computing</Track>
<Track>
Computing for Large Scale Facilities (LHC, FAIR, NICA, SKA, PIC, XFEL, ELI, etc.)
</Track>
</abstract>
<abstract>
<Id>121</Id>
<Title>
Development of L0 trigger for study of AA- collisions in BM@N/Nuclotron and MPD/NICA experiments
</Title>
<Content>
L0 trigger system plays a crucial role in the fast and effective selection of AA- collisions in both fixed target and collider experiments. The concepts of an active target area of the BM@N/Nuclotron experiment and a fast vertex-trigger system developed for the MPD experiment at NICA collider are considered. The requirements to trigger detectors and electronics as well some test results are discussed.
</Content>
<field id="content">
L0 trigger system plays a crucial role in the fast and effective selection of AA- collisions in both fixed target and collider experiments. The concepts of an active target area of the BM@N/Nuclotron experiment and a fast vertex-trigger system developed for the MPD experiment at NICA collider are considered. The requirements to trigger detectors and electronics as well some test results are discussed.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Vladimir</FirstName>
<FamilyName>Yurevich</FamilyName>
<Email>yurevich@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Sergey</FirstName>
<FamilyName>Sergeev</FamilyName>
<Email>serguei.sergueev@mail.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Victor</FirstName>
<FamilyName>Rogov</FamilyName>
<Email>rogovictor@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Dmitriy</FirstName>
<FamilyName>Bogoslovski</FamilyName>
<Email>bogoslovski@mail.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Vladimir</FirstName>
<FamilyName>Tikhomirov</FamilyName>
<Email>vtikhomirov@mail.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Vladimir</FirstName>
<FamilyName>Yurevich</FamilyName>
<Email>yurevich@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Detector &amp; Nuclear Electronics</Track>
<Track>Triggering, Data Acquisition, Control Systems</Track>
</abstract>
<abstract>
<Id>122</Id>
<Title>
Radiation Damage Studies of Silicon Photomultipliers in Neutrons Field of IBR-2.
</Title>
<Content>
It is reported on the study of radiation resistance of silicon photomultipliers (sipm) produced by HAMAMATSU. SiPM was irradiated in neutron fluxes of the reactor IBR-2 of JINR. The tested SiPM received fluence from 10^12 up to 2x10^14 of neutrons /cm2. Irradiated detectors investigated using a radioactive source and laser flashes at a temperature of -30C. The measurements showed that the sipm remain fully functional as photon detectors up to neutron fluence 2x10^14 despite a significant increase in noise.
</Content>
<field id="content">
It is reported on the study of radiation resistance of silicon photomultipliers (sipm) produced by HAMAMATSU. SiPM was irradiated in neutron fluxes of the reactor IBR-2 of JINR. The tested SiPM received fluence from 10^12 up to 2x10^14 of neutrons /cm2. Irradiated detectors investigated using a radioactive source and laser flashes at a temperature of -30C. The measurements showed that the sipm remain fully functional as photon detectors up to neutron fluence 2x10^14 despite a significant increase in noise.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Sergei</FirstName>
<FamilyName>Afanasiev</FamilyName>
<Email>afanasev@lhe.jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Sergei</FirstName>
<FamilyName>Afanasiev</FamilyName>
<Email>afanasev@lhe.jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Detector &amp; Nuclear Electronics</Track>
</abstract>
<abstract>
<Id>123</Id>
<Title>
Cybersecurity of Internet of Things - Risks and Opportunities
</Title>
<Content>
The Internet of Things (IoT) is developing at a tremendous rate. It is a combination of devices connected via the Internet and other networks, which are capable of receiving information from the outside world, analyzing it and, if necessary, managing external devices as well as provide information for decision-making. The goal is to create a more comfortable, safer and more efficient environment for both personal and public life. But like any rapidly evolving Internet technology, there are increasing risks from the point of view of cybersecurity. The most significant cyber-incidents in the world of IoT, the reasons for the occurrence of such cases and possible ways to improve the situation with cybersecurity of the IoT are considered.
</Content>
<field id="content">
The Internet of Things (IoT) is developing at a tremendous rate. It is a combination of devices connected via the Internet and other networks, which are capable of receiving information from the outside world, analyzing it and, if necessary, managing external devices as well as provide information for decision-making. The goal is to create a more comfortable, safer and more efficient environment for both personal and public life. But like any rapidly evolving Internet technology, there are increasing risks from the point of view of cybersecurity. The most significant cyber-incidents in the world of IoT, the reasons for the occurrence of such cases and possible ways to improve the situation with cybersecurity of the IoT are considered.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Alexandre</FirstName>
<FamilyName>Karlov</FamilyName>
<Email>alexander.karlov@cern.ch</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Alexandre</FirstName>
<FamilyName>Karlov</FamilyName>
<Email>alexander.karlov@cern.ch</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Innovative IT Education</Track>
</abstract>
<abstract>
<Id>124</Id>
<Title>
An ion current intensity recorder for the light emission measurement in interactions of highly Charge ions with solid surfaces
</Title>
<Content>
In order to measure light emission in interactions of highly Charge ions with solid surfaces, a current intensity recorder is developed, and used in the measurement system which are composed of a spectrometer, target chamber, a spectral HUB and an IPC with control software. In this system, intensities of light emission will be collected by spectrometer and sent to spectral HUB via 100M Ethernet interface. Meanwhile, the incident ion current recorded by Faraday cup is also fed into the recorder to be integrated, amplified, shaped and finally to be converted into a serial of pulses of TTL level with fixed width. Then, under the control and synchronization of HUB, these pulses will be processed and transmitted to IPC by the recorder via USB interface. There is a corresponding variation between spectral intensities and ion current, so by means of off-line normalizing the saved spectral data and ion current pulses number data, it will eliminate fluctuation of the secondary electron emission that effects to spectral measurement. This current intensity recorder was been used in an experiment performed at the 320kV platform in Institute of Modern Physics, Chinese Academy of Sciences, which will be introduced in the paper.
</Content>
<field id="content">
In order to measure light emission in interactions of highly Charge ions with solid surfaces, a current intensity recorder is developed, and used in the measurement system which are composed of a spectrometer, target chamber, a spectral HUB and an IPC with control software. In this system, intensities of light emission will be collected by spectrometer and sent to spectral HUB via 100M Ethernet interface. Meanwhile, the incident ion current recorded by Faraday cup is also fed into the recorder to be integrated, amplified, shaped and finally to be converted into a serial of pulses of TTL level with fixed width. Then, under the control and synchronization of HUB, these pulses will be processed and transmitted to IPC by the recorder via USB interface. There is a corresponding variation between spectral intensities and ion current, so by means of off-line normalizing the saved spectral data and ion current pulses number data, it will eliminate fluctuation of the secondary electron emission that effects to spectral measurement. This current intensity recorder was been used in an experiment performed at the 320kV platform in Institute of Modern Physics, Chinese Academy of Sciences, which will be introduced in the paper.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Hongyun</FirstName>
<FamilyName>Zhao</FamilyName>
<Email>zhaohy_06@impcas.ac.cn</Email>
<Affiliation>
Institute of modern physics, Chinese academy of sciences
</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Zhihu</FirstName>
<FamilyName>Yang</FamilyName>
<Email>z.yang@impcas.ac.cn</Email>
<Affiliation>
Institute of modern physics, Chinese academy of sciences
</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Hong</FirstName>
<FamilyName>Su</FamilyName>
<Email>suhong@impcas.ac.cn</Email>
<Affiliation>z.yang@impcas.ac.cn</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Yi</FirstName>
<FamilyName>Qian</FamilyName>
<Email>qianyi@impcas.ac.cn</Email>
<Affiliation>z.yang@impcas.ac.cn</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Haibo</FirstName>
<FamilyName>Yang</FamilyName>
<Email>yanghaibo@impcas.ac.cn</Email>
<Affiliation>
Institute of modern physics, Chinese academy of sciences
</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>jie</FirstName>
<FamilyName>Kong</FamilyName>
<Email>kongjie@impcas.ac.cn</Email>
<Affiliation>
Institute of modern physics, Chinese academy of sciences
</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Qianshun</FirstName>
<FamilyName>She</FamilyName>
<Email>sheshun322@impcas.ac.cn</Email>
<Affiliation>
Institute of modern physics, Chinese academy of sciences
</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Yipan</FirstName>
<FamilyName>Guo</FamilyName>
<Email>guoyipan@impcas.ac.cn</Email>
<Affiliation>
Institute of modern physics, Chinese academy of sciences
</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Hongyun</FirstName>
<FamilyName>Zhao</FamilyName>
<Email>zhaohy_06@impcas.ac.cn</Email>
<Affiliation>
Institute of modern physics, Chinese academy of sciences
</Affiliation>
</Speaker>
<Speaker>
<FirstName>Zhihu</FirstName>
<FamilyName>Yang</FamilyName>
<Email>z.yang@impcas.ac.cn</Email>
<Affiliation>
Institute of modern physics, Chinese academy of sciences
</Affiliation>
</Speaker>
<Speaker>
<FirstName>Hong</FirstName>
<FamilyName>Su</FamilyName>
<Email>suhong@impcas.ac.cn</Email>
<Affiliation>z.yang@impcas.ac.cn</Affiliation>
</Speaker>
<Speaker>
<FirstName>Yi</FirstName>
<FamilyName>Qian</FamilyName>
<Email>qianyi@impcas.ac.cn</Email>
<Affiliation>z.yang@impcas.ac.cn</Affiliation>
</Speaker>
<Speaker>
<FirstName>Yipan</FirstName>
<FamilyName>Guo</FamilyName>
<Email>guoyipan@impcas.ac.cn</Email>
<Affiliation>
Institute of modern physics, Chinese academy of sciences
</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Detector &amp; Nuclear Electronics</Track>
</abstract>
<abstract>
<Id>125</Id>
<Title>
Parallel framework for partial wave analysis for the BES-III experiment
</Title>
<Content>
The partial wave analysis at the BES-III experiment is being done event-by-event using the maximum likelihood estimation, with the typical statistics of the order of 10 billion J/ψ events per year, resulting in huge computation times. On the other hand, the event-by-event analysis can be naturally parallelized. We developed the parallel cross-platform software architecture that can run calculations at various high-performance computing platforms, such as multi-core CPUs, Intel Xeon Phi co-processors, and GPUs. The software supports switching between different minimization algorithms like MINUIT or FUMILI. The wave functions are constructed using covariant tensor formalism. Currently analysis is developed for the J/ψ → K+K-π0 decay channel. The algorithm for caching the intermediate results has been developed, minimizing the amount of calculations performed in each iteration. Besides, a number of software optimizations has been used, including vectorization, memory access linearization, and data alignment. In future we plan adding the analysis for new reaction channels, and possibly adapting our software for use in other experiments.
</Content>
<field id="content">
The partial wave analysis at the BES-III experiment is being done event-by-event using the maximum likelihood estimation, with the typical statistics of the order of 10 billion J/ψ events per year, resulting in huge computation times. On the other hand, the event-by-event analysis can be naturally parallelized. We developed the parallel cross-platform software architecture that can run calculations at various high-performance computing platforms, such as multi-core CPUs, Intel Xeon Phi co-processors, and GPUs. The software supports switching between different minimization algorithms like MINUIT or FUMILI. The wave functions are constructed using covariant tensor formalism. Currently analysis is developed for the J/ψ → K+K-π0 decay channel. The algorithm for caching the intermediate results has been developed, minimizing the amount of calculations performed in each iteration. Besides, a number of software optimizations has been used, including vectorization, memory access linearization, and data alignment. In future we plan adding the analysis for new reaction channels, and possibly adapting our software for use in other experiments.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Victoria</FirstName>
<FamilyName>Tokareva</FamilyName>
<Email>tokareva@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Igor</FirstName>
<FamilyName>Denisenko</FamilyName>
<Email>iden@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Victoria</FirstName>
<FamilyName>Tokareva</FamilyName>
<Email>tokareva@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>
Computations with Hybrid Systems (CPU, GPU, coprocessors)
</Track>
</abstract>
<abstract>
<Id>126</Id>
<Title>
Development of the Readout Electronics System for the Prototype of TOF-PET
</Title>
<Content>
It is necessary to design and develop a new high integration and low cost readout electronics system for TOF-PET. A readout electronics system for our prototype of TOF-PET has been developing based on the application requirements. There are five parts mainly in the system: the detector unit, the front-end electronics module (FEM), data processing board (DPB), coincidence processing unit (CPU), and PC. The FEM and DPB boards developed are mainly introduced in this paper.The analog front-end with five channels is hosted on a module called a FEM, the FEM can process signal from each channel independently, and filter, shaping, amplify, discrimination are implemented by FEM. The output signals of FEM are formatted and sent through a LVDS bus to a module called DPB, one DPB can face two FEMs. The energy signal from FEM tells how much energy deposits into the crystal when the crystal is hit by photon. It is often used for position calculation. The energy signal is digitalized with fast ADC, the digitized energy signals from eight channels are sent to a field programmable gate array (FPGA) to extract pulse height information for calculating hit position. The leading edge discriminators on DPB board discriminate the signals from FEM and create timing signals, the timing signals can be extracted from the detector signal directly. The timing signal is sent to a time-to-digital converter (TDC) constructed inside the FPGA for measuring the time of photon flight. Also, the timing signal can be used to trigger DPB or coincidence circuit on CPU. The data in the FPGA on DPB board can be pre-processed depending on the algorithm selected, the crystal, energy and timing relationship are acquired. The data from DPB is then sent out to a PC for further processing.
</Content>
<field id="content">
It is necessary to design and develop a new high integration and low cost readout electronics system for TOF-PET. A readout electronics system for our prototype of TOF-PET has been developing based on the application requirements. There are five parts mainly in the system: the detector unit, the front-end electronics module (FEM), data processing board (DPB), coincidence processing unit (CPU), and PC. The FEM and DPB boards developed are mainly introduced in this paper.The analog front-end with five channels is hosted on a module called a FEM, the FEM can process signal from each channel independently, and filter, shaping, amplify, discrimination are implemented by FEM. The output signals of FEM are formatted and sent through a LVDS bus to a module called DPB, one DPB can face two FEMs. The energy signal from FEM tells how much energy deposits into the crystal when the crystal is hit by photon. It is often used for position calculation. The energy signal is digitalized with fast ADC, the digitized energy signals from eight channels are sent to a field programmable gate array (FPGA) to extract pulse height information for calculating hit position. The leading edge discriminators on DPB board discriminate the signals from FEM and create timing signals, the timing signals can be extracted from the detector signal directly. The timing signal is sent to a time-to-digital converter (TDC) constructed inside the FPGA for measuring the time of photon flight. Also, the timing signal can be used to trigger DPB or coincidence circuit on CPU. The data in the FPGA on DPB board can be pre-processed depending on the algorithm selected, the crystal, energy and timing relationship are acquired. The data from DPB is then sent out to a PC for further processing.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Haibo</FirstName>
<FamilyName>YANG</FamilyName>
<Email>yanghaibo@impcas.ac.cn</Email>
<Affiliation>
Institute of modern physics, Chinese academy of sciences
</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Jie</FirstName>
<FamilyName>KONG</FamilyName>
<Email>kongjie@impcas.ac.cn</Email>
<Affiliation>
Institute of modern physics, Chinese academy of sciences
</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Hong</FirstName>
<FamilyName>SU</FamilyName>
<Email>suhong@impcas.ac.cn</Email>
<Affiliation>
Institute of modern physics, Chinese academy of sciences
</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Jinda</FirstName>
<FamilyName>CHEN</FamilyName>
<Email>chenjinda@impcas.ac.cn</Email>
<Affiliation>
Institute of modern physics, Chinese academy of sciences
</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Chengming</FirstName>
<FamilyName>DU</FamilyName>
<Email>duchengming@impcas.ac.cn</Email>
<Affiliation>
Institute of modern physics, Chinese academy of sciences
</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Junwei</FirstName>
<FamilyName>YAN</FamilyName>
<Email>1032296950@qq.com</Email>
<Affiliation>
Institute of modern physics, Chinese academy of sciences
</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Yuqin</FirstName>
<FamilyName>WEI</FamilyName>
<Email>940900746@qq.com</Email>
<Affiliation>
Institute of modern physics, Chinese academy of sciences
</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Hongyun</FirstName>
<FamilyName>ZHAO</FamilyName>
<Email>zhaohy_06@impcas.ac.cn</Email>
<Affiliation>
Institute of modern physics, Chinese academy of sciences
</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Qianshun</FirstName>
<FamilyName>SHE</FamilyName>
<Email>sheshun322@impcas.ac.cn</Email>
<Affiliation>
Institute of modern physics, Chinese academy of sciences
</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Haibo</FirstName>
<FamilyName>YANG</FamilyName>
<Email>yanghaibo@impcas.ac.cn</Email>
<Affiliation>
Institute of modern physics, Chinese academy of sciences
</Affiliation>
</Speaker>
<Speaker>
<FirstName>Hong</FirstName>
<FamilyName>SU</FamilyName>
<Email>suhong@impcas.ac.cn</Email>
<Affiliation>
Institute of modern physics, Chinese academy of sciences
</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Detector &amp; Nuclear Electronics</Track>
</abstract>
<abstract>
<Id>127</Id>
<Title>Front-End Electronics for the triple-stack MRPC</Title>
<Content>
Ambitious physics goals of MPD require excellent particle identification capability over as large as possible phase space volume. Charged particles in a large momentum range are identified in the MPD by the Time of Flight (ToF) detector. Overall time resolution should be better than 100 ps. In the Laboratory for High Energy Physics (VBLHEP)(JINR) has been developed the triple-stack Multigap Resistive Plate Chambers (MRPC) for the ToF system of the BM@N and the MPD experiments which required careful design of the FEE, cables and etc to minimise noises. For the full exploitation of the excellent timing properties of the MRPC designed the preamplifier board based on NINO ASIC. The concept of slow control is developed and tested. For the signal acquisition the cable design optimized for the CXP connectors in collaboration with MOLEX. According to the results on test experiments preamplifier board showed a stable work and achieved good time resolution of the system ~40 ps according to one channel.
</Content>
<field id="content">
Ambitious physics goals of MPD require excellent particle identification capability over as large as possible phase space volume. Charged particles in a large momentum range are identified in the MPD by the Time of Flight (ToF) detector. Overall time resolution should be better than 100 ps. In the Laboratory for High Energy Physics (VBLHEP)(JINR) has been developed the triple-stack Multigap Resistive Plate Chambers (MRPC) for the ToF system of the BM@N and the MPD experiments which required careful design of the FEE, cables and etc to minimise noises. For the full exploitation of the excellent timing properties of the MRPC designed the preamplifier board based on NINO ASIC. The concept of slow control is developed and tested. For the signal acquisition the cable design optimized for the CXP connectors in collaboration with MOLEX. According to the results on test experiments preamplifier board showed a stable work and achieved good time resolution of the system ~40 ps according to one channel.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Mikhail</FirstName>
<FamilyName>Buryakov</FamilyName>
<Email>mikhail466@gmail.com</Email>
<Affiliation>JINR, LHEP</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Vadim</FirstName>
<FamilyName>Babkin</FamilyName>
<Email>babkin@jinr.ru</Email>
<Affiliation>Joint Institute for Nuclear Research</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Sergey</FirstName>
<FamilyName>Volgin</FamilyName>
<Email>vs33@yandex.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Mikhail</FirstName>
<FamilyName>Rumyantsev</FamilyName>
<Email>rumyantsev@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Viacheslav</FirstName>
<FamilyName>Golovatyuk</FamilyName>
<Email>slava.golovatyuk@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Mikhail</FirstName>
<FamilyName>Buryakov</FamilyName>
<Email>mikhail466@gmail.com</Email>
<Affiliation>JINR, LHEP</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Detector &amp; Nuclear Electronics</Track>
<Track>Triggering, Data Acquisition, Control Systems</Track>
</abstract>
<abstract>
<Id>128</Id>
<Title>MultiPurpose Detector - MPD</Title>
<Content>
The multipurpose MPD detector is the main tool for studying the properties of hot and dense baryonic matter formed in collisions of heavy ions at the NICA accelerator complex. The sufficiently high luminosity of the collider, the complexity and diversity of the physical tasks make high demands on the performance of detectors and service systems of the MPD. The report gives a brief overview of the physical program of the MPD experiment and some technical characteristics of all the main elements of the experimental setup.
</Content>
<field id="content">
The multipurpose MPD detector is the main tool for studying the properties of hot and dense baryonic matter formed in collisions of heavy ions at the NICA accelerator complex. The sufficiently high luminosity of the collider, the complexity and diversity of the physical tasks make high demands on the performance of detectors and service systems of the MPD. The report gives a brief overview of the physical program of the MPD experiment and some technical characteristics of all the main elements of the experimental setup.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Vadim</FirstName>
<FamilyName>Babkin</FamilyName>
<Email>babkin@jinr.ru</Email>
<Affiliation>Joint Institute for Nuclear Research</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Vadim</FirstName>
<FamilyName>Babkin</FamilyName>
<Email>babkin@jinr.ru</Email>
<Affiliation>Joint Institute for Nuclear Research</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Detector &amp; Nuclear Electronics</Track>
</abstract>
<abstract>
<Id>129</Id>
<Title>Status of the test stand TOF MPD system</Title>
<Content>
This report is devoted to the status of the test stand of the TOF MPD system. The stand is planned to be used to carry out methodical research and mass testing of detectors for the MPD experiment at the NICA collider. The setup is described in detail. The investigation has been performed at the Veksler and Baldin Laboratory of High Energy Physics, JINR.
</Content>
<field id="content">
This report is devoted to the status of the test stand of the TOF MPD system. The stand is planned to be used to carry out methodical research and mass testing of detectors for the MPD experiment at the NICA collider. The setup is described in detail. The investigation has been performed at the Veksler and Baldin Laboratory of High Energy Physics, JINR.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Alexandr</FirstName>
<FamilyName>Dmitriev</FamilyName>
<Email>admitriev@jinr.ru</Email>
<Affiliation>LHEP JINR</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Alexandr</FirstName>
<FamilyName>Dmitriev</FamilyName>
<Email>admitriev@jinr.ru</Email>
<Affiliation>LHEP JINR</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Detector &amp; Nuclear Electronics</Track>
<Track>Triggering, Data Acquisition, Control Systems</Track>
</abstract>
<abstract>
<Id>130</Id>
<Title>
Increasing Bandwidth of Data Acquisition Systems on IBR-2 Reactor Spectrometers in FLNP
</Title>
<Content>
Present trends towards increasing the number of detector channels and the volumes of registered and accumulated data in real time in experiments on IBR-2 reactor spectrometers in FLNP require increasing the bandwidth of data acquisition systems. The paper considers modernization of the data acquisition system based on the MPD and De-Li-DAQ-2D blocks earlier developed in FLNP and being widely used on neutron spectrometers today. Initially, for this system to connect the modules to the computer, the FLINK fiber-optic adapter with an USB2.0 interface was developed. In new projects aimed at development of the FLNP spectrometers, up to 240 detector elements are to be connected to MPD units with a maximum load of up to 8M events/s. This requires increasing the bandwidth of channels for connection with the computer to 50 MB/s, which is not feasible with the existing USB2.0 interface. To achieve the goal several variants of upgrading link interfaces for the modules De-Li-DAQ-2D and MPD used in the data acquisition system for the IBR-2 spectrometers have been developed. A ten times bandwidth increase has been realized by developing a new FLINK–USB3.0 adapter which enables a link between the fiber optic interface of the modules and the interface USB 3.0 of the computer.
</Content>
<field id="content">
Present trends towards increasing the number of detector channels and the volumes of registered and accumulated data in real time in experiments on IBR-2 reactor spectrometers in FLNP require increasing the bandwidth of data acquisition systems. The paper considers modernization of the data acquisition system based on the MPD and De-Li-DAQ-2D blocks earlier developed in FLNP and being widely used on neutron spectrometers today. Initially, for this system to connect the modules to the computer, the FLINK fiber-optic adapter with an USB2.0 interface was developed. In new projects aimed at development of the FLNP spectrometers, up to 240 detector elements are to be connected to MPD units with a maximum load of up to 8M events/s. This requires increasing the bandwidth of channels for connection with the computer to 50 MB/s, which is not feasible with the existing USB2.0 interface. To achieve the goal several variants of upgrading link interfaces for the modules De-Li-DAQ-2D and MPD used in the data acquisition system for the IBR-2 spectrometers have been developed. A ten times bandwidth increase has been realized by developing a new FLINK–USB3.0 adapter which enables a link between the fiber optic interface of the modules and the interface USB 3.0 of the computer.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Vladimir</FirstName>
<FamilyName>Drozdov</FamilyName>
<Email>drozdov@jinr.ru</Email>
<Affiliation>JINR, FLNP</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Vasilii</FirstName>
<FamilyName>Shvetcov</FamilyName>
<Email>shvetc_vas@mail.ru</Email>
<Affiliation>FLNP</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Vladimir</FirstName>
<FamilyName>Drozdov</FamilyName>
<Email>drozdov@jinr.ru</Email>
<Affiliation>JINR, FLNP</Affiliation>
</Speaker>
<Speaker>
<FirstName>Vasilii</FirstName>
<FamilyName>Shvetcov</FamilyName>
<Email>shvetc_vas@mail.ru</Email>
<Affiliation>FLNP</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Triggering, Data Acquisition, Control Systems</Track>
</abstract>
<abstract>
<Id>131</Id>
<Title>
Development of a high-precision current-sence amplifier for measurement DCCTs secondary current
</Title>
<Content>
The magnetic system of the NICA accelerator complex consists of magnets of various types. It is necessary to measure the magnetic field of each magnet. The measurement accuracy of the magnetic field parameters depends on the measurement accuracy of the magnet excitation current. It is required to measure current with accuracy better than 20 ppm. This work describes the development of a high-precision current-sence amplifier for measurement DCCTs secondary current.
</Content>
<field id="content">
The magnetic system of the NICA accelerator complex consists of magnets of various types. It is necessary to measure the magnetic field of each magnet. The measurement accuracy of the magnetic field parameters depends on the measurement accuracy of the magnet excitation current. It is required to measure current with accuracy better than 20 ppm. This work describes the development of a high-precision current-sence amplifier for measurement DCCTs secondary current.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Ilya</FirstName>
<FamilyName>Donguzov</FamilyName>
<Email>donguzov.94@gmail.com</Email>
<Affiliation>Igorevich</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Ilya</FirstName>
<FamilyName>Donguzov</FamilyName>
<Email>donguzov.94@gmail.com</Email>
<Affiliation>Igorevich</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>
Computing for Large Scale Facilities (LHC, FAIR, NICA, SKA, PIC, XFEL, ELI, etc.)
</Track>
</abstract>
<abstract>
<Id>132</Id>
<Title>
The development of front-end readout system for magnetic mass spectrometer
</Title>
<Content>
After nearly a hundred years of development, the magnetic mass spectrometer has gradually become a routine analysis and testing equipment. In this paper, a readout system with four hundred and sixteen channels from the Faraday cup array detector is developed for the magnetic mass spectrometer, which is used to diagnose composition of the beam ions, including H+, D+, Ti+, Ti2+, Ti3+ and other impurity ions. The system consists of a front-end processing circuit and a data acquisition subsystem, and the front-end processing circuit converts the weak current signal into a voltage signal, the data acquisition circuit realizes the digitalization of the voltage signal. This system achieves digital readout of the charge signal from 0.1pC to 120pC, and the nonlinearity error is less than 1.95%. At the same time, it meets the design requirements of the magnetic mass spectrometer, and can also be widely used to readout the weak current or charge signal in experimental nuclear physics and accelerator systems.
</Content>
<field id="content">
After nearly a hundred years of development, the magnetic mass spectrometer has gradually become a routine analysis and testing equipment. In this paper, a readout system with four hundred and sixteen channels from the Faraday cup array detector is developed for the magnetic mass spectrometer, which is used to diagnose composition of the beam ions, including H+, D+, Ti+, Ti2+, Ti3+ and other impurity ions. The system consists of a front-end processing circuit and a data acquisition subsystem, and the front-end processing circuit converts the weak current signal into a voltage signal, the data acquisition circuit realizes the digitalization of the voltage signal. This system achieves digital readout of the charge signal from 0.1pC to 120pC, and the nonlinearity error is less than 1.95%. At the same time, it meets the design requirements of the magnetic mass spectrometer, and can also be widely used to readout the weak current or charge signal in experimental nuclear physics and accelerator systems.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Qianshun</FirstName>
<FamilyName>SHE</FamilyName>
<Email>sheshun322@impcas.ac.cn</Email>
<Affiliation>
Institute of Modern Physics, Chinese Academy of Sciences
</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Hong</FirstName>
<FamilyName>SU</FamilyName>
<Email>suhong@impcas.ac.cn</Email>
<Affiliation>
Institute of Modern Physics, Chinese Academy of Sciences
</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Zhiguo</FirstName>
<FamilyName>XU</FamilyName>
<Email>xuzg@impcas.ac.cn</Email>
<Affiliation>
Institute of Modern Physics, Chinese Academy of Sciences
</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Zulong</FirstName>
<FamilyName>ZHAO</FamilyName>
<Email>zulong@impcas.ac.cn</Email>
<Affiliation>
Institute of Modern Physics, Chinese Academy of Sciences
</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Qianshun</FirstName>
<FamilyName>SHE</FamilyName>
<Email>sheshun322@impcas.ac.cn</Email>
<Affiliation>
Institute of Modern Physics, Chinese Academy of Sciences
</Affiliation>
</Speaker>
<Speaker>
<FirstName>Hong</FirstName>
<FamilyName>SU</FamilyName>
<Email>suhong@impcas.ac.cn</Email>
<Affiliation>
Institute of Modern Physics, Chinese Academy of Sciences
</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Detector &amp; Nuclear Electronics</Track>
</abstract>
<abstract>
<Id>133</Id>
<Title>
Experimental projects dedicated to the research of exotic nuclei in Dubna
</Title>
<Content>
The development of the experimental base of the Flerov Laboratory (JINR, Dubna) assumed for the forthcoming 7-year period includes two principal directions. The first one implies the study of physical and chemical properties of nuclei in the vicinity of the so called “Stability Island”. This activity will be developed mainly on the base of the Super Heavy Elements (SHE) Factory. The factory, comprising the high current cyclotron DC280 and a number of new facilities is expected to be launched by the end of 2017. High intensity of accelerated beams and drastically improved parameters of new separators will secure the increase of the total efficiency of experiments at least by a factor of 1000. Another promising field of research is connected with the use of secondary beams of radioactive nuclei. The new fragment separator ACCULINNA-2 intended for studies in the region of light masses close to the nucleon drip lines was recently put into operation in the Flerov Laboratory. The scientific plan for forthcoming several years implies modernization of the operating accelerators aimed, particularly, at the essential increase of the energy of accelerated nuclei used for the production of radioactive beams. The technical changes combined with tried and tested experimental approaches provide luminosity of secondary beams on a physical target the level with that expected for the most advanced Radioactive Beam Factories.
</Content>
<field id="content">
The development of the experimental base of the Flerov Laboratory (JINR, Dubna) assumed for the forthcoming 7-year period includes two principal directions. The first one implies the study of physical and chemical properties of nuclei in the vicinity of the so called “Stability Island”. This activity will be developed mainly on the base of the Super Heavy Elements (SHE) Factory. The factory, comprising the high current cyclotron DC280 and a number of new facilities is expected to be launched by the end of 2017. High intensity of accelerated beams and drastically improved parameters of new separators will secure the increase of the total efficiency of experiments at least by a factor of 1000. Another promising field of research is connected with the use of secondary beams of radioactive nuclei. The new fragment separator ACCULINNA-2 intended for studies in the region of light masses close to the nucleon drip lines was recently put into operation in the Flerov Laboratory. The scientific plan for forthcoming several years implies modernization of the operating accelerators aimed, particularly, at the essential increase of the energy of accelerated nuclei used for the production of radioactive beams. The technical changes combined with tried and tested experimental approaches provide luminosity of secondary beams on a physical target the level with that expected for the most advanced Radioactive Beam Factories.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Sergey</FirstName>
<FamilyName>Sidorchuk</FamilyName>
<Email>sidorchuk@jinr.ru</Email>
<Affiliation>FLNR JINR</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Sergey</FirstName>
<FamilyName>Sidorchuk</FamilyName>
<Email>sidorchuk@jinr.ru</Email>
<Affiliation>FLNR JINR</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Research Data Infrastructures</Track>
</abstract>
<abstract>
<Id>134</Id>
<Title>Slow Control system at BM@N experiment</Title>
<Content>
Big modern physics experiments represent a collaboration of workgroups and require wide variety of different electronic equipment. Besides trigger electronics or Data acquisition system (DAQ), there is a hardware that is not time-critical, and can be run at a low priority. Slow Control system are used for setup and monitoring such hardware. Slow Control systems in a typical experiment are often used to setup and/or monitor components such as high voltage modules, temperature sensors, pressure gauges, leak detectors, RF generators, PID controllers etc. often from a large number of hardware vendors. Slow Control system also has to archive revieved data for further analysis and handling by physicists and to warn personnel about critical situations and contingency.
</Content>
<field id="content">
Big modern physics experiments represent a collaboration of workgroups and require wide variety of different electronic equipment. Besides trigger electronics or Data acquisition system (DAQ), there is a hardware that is not time-critical, and can be run at a low priority. Slow Control system are used for setup and monitoring such hardware. Slow Control systems in a typical experiment are often used to setup and/or monitor components such as high voltage modules, temperature sensors, pressure gauges, leak detectors, RF generators, PID controllers etc. often from a large number of hardware vendors. Slow Control system also has to archive revieved data for further analysis and handling by physicists and to warn personnel about critical situations and contingency.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Dmitry</FirstName>
<FamilyName>Egorov</FamilyName>
<Email>degorov@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Vitaly</FirstName>
<FamilyName>Shutov</FamilyName>
<Email>shutov@jinr.ru</Email>
<Affiliation>Borisovich</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Roman</FirstName>
<FamilyName>Nagdasev</FamilyName>
<Email>kraken.rus@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Peter</FirstName>
<FamilyName>Chumakov</FamilyName>
<Email>peter.chumakov@bk.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Dmitry</FirstName>
<FamilyName>Egorov</FamilyName>
<Email>degorov@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Triggering, Data Acquisition, Control Systems</Track>
</abstract>
<abstract>
<Id>135</Id>
<Title>
Betatron tune measurement system upgrade at Nuclotron
</Title>
<Content>
A few improvements have been made in order to enhance the resolution of the Q measurement system such as development of the additional NI FlexRIO digitizer module with two 18-Bit ADC AD7960 and TDC-GP22 for precision beam revolution frequency measurement. The new amplification system for picking up signals was developed using diode detection technique, analog filtering and real-time gain adjusting to allow carry out measurements during beam injection and acceleration.
</Content>
<field id="content">
A few improvements have been made in order to enhance the resolution of the Q measurement system such as development of the additional NI FlexRIO digitizer module with two 18-Bit ADC AD7960 and TDC-GP22 for precision beam revolution frequency measurement. The new amplification system for picking up signals was developed using diode detection technique, analog filtering and real-time gain adjusting to allow carry out measurements during beam injection and acceleration.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Dmitrii</FirstName>
<FamilyName>Monakhov</FamilyName>
<Email>cornflyer@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Evgeny</FirstName>
<FamilyName>Gorbachev</FamilyName>
<Email>gorbe@sunse.jinr.ru</Email>
<Affiliation>VBLHEP JINR</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Dmitrii</FirstName>
<FamilyName>Monakhov</FamilyName>
<Email>cornflyer@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Detector &amp; Nuclear Electronics</Track>
<Track>Triggering, Data Acquisition, Control Systems</Track>
</abstract>
<abstract>
<Id>136</Id>
<Title>
LABVIEW BASED MAGNETIC FIELD MAPPING SYSTEM FOR CYCLOTRON DC-280
</Title>
<Content>
New isochronous cyclotron DC-280 is being created at the FLNR JINR. The software application uses LabVIEW and supports pneumatic step movement, data acquisition, and magnet power supply control. A complete map of 360 degrees is obtained in approximately 14 hours, measuring 148750 field values with a spacing of 10 mm in radius every degree. This paper describes software part of the magnetic field mapping system based on LabVIEW DSC module.
</Content>
<field id="content">
New isochronous cyclotron DC-280 is being created at the FLNR JINR. The software application uses LabVIEW and supports pneumatic step movement, data acquisition, and magnet power supply control. A complete map of 360 degrees is obtained in approximately 14 hours, measuring 148750 field values with a spacing of 10 mm in radius every degree. This paper describes software part of the magnetic field mapping system based on LabVIEW DSC module.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Vitali</FirstName>
<FamilyName>Aleinikov</FamilyName>
<Email>vitalirus@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Kirill</FirstName>
<FamilyName>Sychev</FamilyName>
<Email>sychev@jinr.ru</Email>
<Affiliation>FLNR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Ivan</FirstName>
<FamilyName>Ivanenko</FamilyName>
<Email>ivan@jinr.ru</Email>
<Affiliation>FLNR</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Vitali</FirstName>
<FamilyName>Aleinikov</FamilyName>
<Email>vitalirus@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Triggering, Data Acquisition, Control Systems</Track>
</abstract>
<abstract>
<Id>137</Id>
<Title>
Readout electronics for TPC detector in the MPD/NICA project
</Title>
<Content>
The TPC barrel is placed in the middle of a Multi-Purpose Detector and provides tracing and identifying of charged particles in the pseudorapidity range │η│ &#8804; 1.2. Tracks in the TPC are registered by 24 readout chambers placed at both end-caps of the sensitive volume of the barrel. The readout system of one chamber consists of the front-end card (FEC) set and a readout control unit (RCU). FECs collects signals directly from the registration chamber pads, amplifies them, digitizes, processes and transfers it to the RCU. To ensure good reconstruction of all tracks, the 95232 electronic channels must meet strong requirements: the signal to noise ratio – 30, the equivalent noise charge &lt; 1000 e-, power consumption less than 100 mW per channel.
</Content>
<field id="content">
The TPC barrel is placed in the middle of a Multi-Purpose Detector and provides tracing and identifying of charged particles in the pseudorapidity range │η│ &#8804; 1.2. Tracks in the TPC are registered by 24 readout chambers placed at both end-caps of the sensitive volume of the barrel. The readout system of one chamber consists of the front-end card (FEC) set and a readout control unit (RCU). FECs collects signals directly from the registration chamber pads, amplifies them, digitizes, processes and transfers it to the RCU. To ensure good reconstruction of all tracks, the 95232 electronic channels must meet strong requirements: the signal to noise ratio – 30, the equivalent noise charge &lt; 1000 e-, power consumption less than 100 mW per channel.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Stepan</FirstName>
<FamilyName>Vereschagin</FamilyName>
<Email>vereschagin@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Stepan</FirstName>
<FamilyName>Vereschagin</FamilyName>
<Email>vereschagin@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Detector &amp; Nuclear Electronics</Track>
<Track>Triggering, Data Acquisition, Control Systems</Track>
</abstract>
<abstract>
<Id>138</Id>
<Title>
ESIS ions injection, holding and extraction control system
</Title>
<Content>
During the work on the creation of a new Electron String Ion Source (ESIS) for the NICA/MPD project several electronic modules were created. Modules includes pulsed HV (+3 kV) potential barriers formation modules used to hold ions, HV (+3 kV) ion extraction module and several secondary modules. Modules development process and test results are discribed.
</Content>
<field id="content">
During the work on the creation of a new Electron String Ion Source (ESIS) for the NICA/MPD project several electronic modules were created. Modules includes pulsed HV (+3 kV) potential barriers formation modules used to hold ions, HV (+3 kV) ion extraction module and several secondary modules. Modules development process and test results are discribed.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Dmitriy</FirstName>
<FamilyName>Ponkin</FamilyName>
<Email>ponkin@jinr.ru</Email>
<Affiliation>LHEP JINR</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Denis</FirstName>
<FamilyName>Donets</FamilyName>
<Email>ru3daz@yandex.ru</Email>
<Affiliation>LHEP JINR</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Nikolay</FirstName>
<FamilyName>Gorbunov</FamilyName>
<Email>nikolai_gorbunov@mail.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Dmitriy</FirstName>
<FamilyName>Ponkin</FamilyName>
<Email>ponkin@jinr.ru</Email>
<Affiliation>LHEP JINR</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Detector &amp; Nuclear Electronics</Track>
<Track>Triggering, Data Acquisition, Control Systems</Track>
</abstract>
<abstract>
<Id>139</Id>
<Title>JINR cloud computing in the NOvA experiment</Title>
<Content>
NOvA is a large-scale neutrino experiment JINR takes part in in many directions including those connected to information technologies usage. A cloud resource was provided by the JINR computing center for the NOvA experiment within which a pool of virtual machines was deployed to provide local JINR users a way of using them in interactive mode giving the users ability to use this service for neutrino events modeling, supporting experimental data acquisition and control and performing physical analysis.
</Content>
<field id="content">
NOvA is a large-scale neutrino experiment JINR takes part in in many directions including those connected to information technologies usage. A cloud resource was provided by the JINR computing center for the NOvA experiment within which a pool of virtual machines was deployed to provide local JINR users a way of using them in interactive mode giving the users ability to use this service for neutrino events modeling, supporting experimental data acquisition and control and performing physical analysis.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Nikita</FirstName>
<FamilyName>Balashov</FamilyName>
<Email>balashov.nikita@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Alexandr</FirstName>
<FamilyName>Baranov</FamilyName>
<Email>baranov@jinr.ru</Email>
<Affiliation>(JINR)</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Evgeniy</FirstName>
<FamilyName>Kuznetsov</FamilyName>
<Email>evkuz@inbox.ru</Email>
<Affiliation>LIT NINR</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Oleg</FirstName>
<FamilyName>Samoylov</FamilyName>
<Email>samoylov@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Andrey</FirstName>
<FamilyName>Sheshukov</FamilyName>
<Email>sheshuk@nusun.jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Oleg</FirstName>
<FamilyName>Samoylov</FamilyName>
<Email>samoylov@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Distributed Computing. GRID &amp; Cloud Computing</Track>
</abstract>
<abstract>
<Id>140</Id>
<Title>
Software Implementation of USB 3.0 Stack for Upgraded Data Link Interface on IBR-2 Reactor Spectrometers in FLNP
</Title>
<Content>
In this work software implementation of USB3.0 stack protocols for operating data acquisition units of the IBR-2 spectrometric system with an upgraded communication adapter, is considered. The data acquisition system on De-Li-DAQ-2D and MPD blocks developed earlier in FLNP is widely used at present on neutron spectrometers. To connect the modules to the computer, an FLINK fiber optic adapter with an USB2.0 interface was originally developed for this system. Modern trends towards increasing the number of detector channels and the volumes of the recorded and accumulated information in real time in experiments on IBR-2 spectrometers in FLNP require increasing the bandwidth and reliability of the communication channel. In addition to replacing the driver and using the FTD3XX library of the FT600 chip to provide the USB Super Speed to FIFO bridge with a new communication adapter, improvement of software for an advanced application communication protocol with DAQ blocks is also required. Upgrading of the adapter and improvement of software for a new application-layer protocol have resulted in an increase of the bandwidth and reliability of the communication channel.
</Content>
<field id="content">
In this work software implementation of USB3.0 stack protocols for operating data acquisition units of the IBR-2 spectrometric system with an upgraded communication adapter, is considered. The data acquisition system on De-Li-DAQ-2D and MPD blocks developed earlier in FLNP is widely used at present on neutron spectrometers. To connect the modules to the computer, an FLINK fiber optic adapter with an USB2.0 interface was originally developed for this system. Modern trends towards increasing the number of detector channels and the volumes of the recorded and accumulated information in real time in experiments on IBR-2 spectrometers in FLNP require increasing the bandwidth and reliability of the communication channel. In addition to replacing the driver and using the FTD3XX library of the FT600 chip to provide the USB Super Speed to FIFO bridge with a new communication adapter, improvement of software for an advanced application communication protocol with DAQ blocks is also required. Upgrading of the adapter and improvement of software for a new application-layer protocol have resulted in an increase of the bandwidth and reliability of the communication channel.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Svetlana</FirstName>
<FamilyName>Murashkevich</FamilyName>
<Email>svetlana@jinr.ru</Email>
<Affiliation>RUSSIA, JINR</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Vladimir</FirstName>
<FamilyName>Drozdov</FamilyName>
<Email>drozdov@jinr.ru</Email>
<Affiliation>JINR, FLNP</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Svetlana</FirstName>
<FamilyName>Murashkevich</FamilyName>
<Email>svetlana@jinr.ru</Email>
<Affiliation>RUSSIA, JINR</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Triggering, Data Acquisition, Control Systems</Track>
</abstract>
<abstract>
<Id>141</Id>
<Title>
High-speed Data Acquisition System Based on DRS4 Waveform Digitization
</Title>
<Content>
We present a 5GSPS high-speed data acquisition system with up to 32 channels and 1024 cells per channel, designed for fast-sampling, front-end applications. The data acquisition system consists of four waveform digitalization front-end circuits, one high-speed data transmission circuit and upper computer software. Waveform digitalization front-end circuit has one piece DRS4 Switched Capacitor Array chip developed at Paul Scherrer Institute, Switzerland. The DRS4 chip has been working in full readout mode and its eight channels are independent. High-speed data transmission circuit accept sampling data of four waveform digitization front circuits through four 2.5 Gbps serial link transceivers and send these data to upper computer real-timely. High-speed data transmission circuit with 8 lane PCI Express interface can be inserted in the PCIE x8 slot of upper computer directly and exchange data with the upper computer high-speed. Upper computer software with friendly human-computer interaction interface is responsible for system detection, sending configuration instructions, data processing and saving, etc. The first prototype of high-speed data acquisition system was designed and tested in the laboratory.
</Content>
<field id="content">
We present a 5GSPS high-speed data acquisition system with up to 32 channels and 1024 cells per channel, designed for fast-sampling, front-end applications. The data acquisition system consists of four waveform digitalization front-end circuits, one high-speed data transmission circuit and upper computer software. Waveform digitalization front-end circuit has one piece DRS4 Switched Capacitor Array chip developed at Paul Scherrer Institute, Switzerland. The DRS4 chip has been working in full readout mode and its eight channels are independent. High-speed data transmission circuit accept sampling data of four waveform digitization front circuits through four 2.5 Gbps serial link transceivers and send these data to upper computer real-timely. High-speed data transmission circuit with 8 lane PCI Express interface can be inserted in the PCIE x8 slot of upper computer directly and exchange data with the upper computer high-speed. Upper computer software with friendly human-computer interaction interface is responsible for system detection, sending configuration instructions, data processing and saving, etc. The first prototype of high-speed data acquisition system was designed and tested in the laboratory.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Jingzhe</FirstName>
<FamilyName>Zhang</FamilyName>
<Email>zhangjz@impcas.ac.cn</Email>
<Affiliation>
Institute of Modern Physics, Chinese Academy of Sciences
</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Haibo</FirstName>
<FamilyName>YANG</FamilyName>
<Email>yanghaibo@impcas.ac.cn</Email>
<Affiliation>
Institute of modern physics, Chinese academy of sciences
</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Jie</FirstName>
<FamilyName>Kong</FamilyName>
<Email>kongjie@impcas.ac.cn</Email>
<Affiliation>
Institute of Modern Physics,Chinese Academy of Sciences
</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Hongyun</FirstName>
<FamilyName>Zhao</FamilyName>
<Email>zhaohy_06@impcas.ac.cn</Email>
<Affiliation>
Institute of modern physics, Chinese academy of sciences
</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Yi</FirstName>
<FamilyName>Qian</FamilyName>
<Email>qianyi@impcas.ac.cn</Email>
<Affiliation>
Institute of modern physics, Chinese academy of sciences
</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Qianshun</FirstName>
<FamilyName>SHE</FamilyName>
<Email>sheshun322@impcas.ac.cn</Email>
<Affiliation>
Institute of Modern Physics, Chinese Academy of Sciences
</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Hong</FirstName>
<FamilyName>Su</FamilyName>
<Email>suhong@impcas.ac.cn</Email>
<Affiliation>
Institute of modern physics, Chinese academy of sciences
</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Jingzhe</FirstName>
<FamilyName>Zhang</FamilyName>
<Email>zhangjz@impcas.ac.cn</Email>
<Affiliation>
Institute of Modern Physics, Chinese Academy of Sciences
</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Detector &amp; Nuclear Electronics</Track>
<Track>Triggering, Data Acquisition, Control Systems</Track>
</abstract>
<abstract>
<Id>142</Id>
<Title>
Data acquisition systems on neutron spectrometers of the IBR-2 reactor
</Title>
<Content>
The report describes the electronics and software of data acquisition systems for thermal neutron detectors [1], which are currently used on the spectrometers of the IBR-2 reactor at JINR. The experience gained during the operation of these systems has been summarized, the results of the performance analysis of the data acquisition systems developed in the FLNP for position-sensitive neutron detectors based on multi-wire proportional chambers with the readout from delay line are given. Requirements for the throughput of electronics and software functionality of such systems are refined. [1] Kulikov S.A., Prikhodko V.I., 2016 Physics of Particles and Nuclei 47(4) 702-10
</Content>
<field id="content">
The report describes the electronics and software of data acquisition systems for thermal neutron detectors [1], which are currently used on the spectrometers of the IBR-2 reactor at JINR. The experience gained during the operation of these systems has been summarized, the results of the performance analysis of the data acquisition systems developed in the FLNP for position-sensitive neutron detectors based on multi-wire proportional chambers with the readout from delay line are given. Requirements for the throughput of electronics and software functionality of such systems are refined. [1] Kulikov S.A., Prikhodko V.I., 2016 Physics of Particles and Nuclei 47(4) 702-10
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Elena</FirstName>
<FamilyName>Litvinenko</FamilyName>
<Email>litvin@nf.jinr.ru</Email>
<Affiliation>FLNP JINR</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Andrey</FirstName>
<FamilyName>Bogdzel</FamilyName>
<Email>abogdz@nf.jinr.ru</Email>
<Affiliation>FLNP JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Andrey</FirstName>
<FamilyName>Churakov</FamilyName>
<Email>churakov@nf.jinr.ru</Email>
<Affiliation>FLNP JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Andrey</FirstName>
<FamilyName>Kirilov</FamilyName>
<Email>akirilov@nf.jinr.ru</Email>
<Affiliation>FLNP JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Vladimir</FirstName>
<FamilyName>Kruglov</FamilyName>
<Email>vkruglov@nf.jinr.ru</Email>
<Affiliation>FLNP JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Sergey</FirstName>
<FamilyName>Kulikov</FamilyName>
<Email>ksa@nf.jinr.ru</Email>
<Affiliation>FLNP JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Feodosiy</FirstName>
<FamilyName>Levchanovsky</FamilyName>
<Email>levch@nf.jinr.ru</Email>
<Affiliation>FLNP JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Svetlana</FirstName>
<FamilyName>Murashkevich</FamilyName>
<Email>svetlana@nf.jinr.ru</Email>
<Affiliation>FLNP JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Vladimir</FirstName>
<FamilyName>Drozdov</FamilyName>
<Email>drozdov@jinr.ru</Email>
<Affiliation>FLNP JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Valentin</FirstName>
<FamilyName>Prikhodko</FamilyName>
<Email>prikh@nf.jinr.ru</Email>
<Affiliation>FLNP JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Valery</FirstName>
<FamilyName>Zhuravlev</FamilyName>
<Email>zhur@nf.jinr.ru</Email>
<Affiliation>FLNP JINR</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Vladimir</FirstName>
<FamilyName>Drozdov</FamilyName>
<Email>drozdov@jinr.ru</Email>
<Affiliation>FLNP JINR</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Detector &amp; Nuclear Electronics</Track>
<Track>Triggering, Data Acquisition, Control Systems</Track>
</abstract>
<abstract>
<Id>143</Id>
<Title>
Control system of the superconducting magnets cryogenic test bench for the NICA accelerator complex
</Title>
<Content>
Control system of the superconducting magnets cryogenic test bench has been designed in Tango Controls format. It includes the Thermometry system and the Satellite refrigerators control system. The report describes hardware and software modules for data acquisition and management, archiving system, configuration system, access control system, web service and web client applications.
</Content>
<field id="content">
Control system of the superconducting magnets cryogenic test bench has been designed in Tango Controls format. It includes the Thermometry system and the Satellite refrigerators control system. The report describes hardware and software modules for data acquisition and management, archiving system, configuration system, access control system, web service and web client applications.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Georgy</FirstName>
<FamilyName>Sedykh</FamilyName>
<Email>egor@dubna.tk</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Evgeny</FirstName>
<FamilyName>Gorbachev</FamilyName>
<Email>gorbe@sunse.jinr.ru</Email>
<Affiliation>VBLHEP JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Artem</FirstName>
<FamilyName>Galimov</FamilyName>
<Email>galimov@jinr.ru</Email>
<Affiliation>Joint Institute for Nuclear Research</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Aleksandr</FirstName>
<FamilyName>Kirichenko</FamilyName>
<Email>akir20041957@mail.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Viacheslav</FirstName>
<FamilyName>Kosachev</FamilyName>
<Email>vacheslav1991@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Denis</FirstName>
<FamilyName>Neapolitanskiy</FamilyName>
<Email>neapolit@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Dmitriy</FirstName>
<FamilyName>Nikiforov</FamilyName>
<Email>dmit-nikiforov@yandex.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Roman</FirstName>
<FamilyName>Pivin</FamilyName>
<Email>pivin@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Valery</FirstName>
<FamilyName>Volkov</FamilyName>
<Email>vvolkov@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Georgy</FirstName>
<FamilyName>Sedykh</FamilyName>
<Email>egor@dubna.tk</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Triggering, Data Acquisition, Control Systems</Track>
</abstract>
<abstract>
<Id>144</Id>
<Title>
Equal cost multi pathing in high power systems with TRILL
</Title>
<Content>
The work is devoted to the result of the creating a first module of the 1-st phase of the data processing center at the Joint Institute for nuclear research for modeling and processing experiments carried out on the test installations of the Large Hadron Collider. The issues related to handling the enormous data flow from the LHC experimental installations and troubles of distributed storages are considered. The article presents a hierarchical diagram of the network farm and a basic model of the network architecture levels. The project documentation of the network based on the Brocade equipment is considered. Protocols for disposal full mesh network topologies are considered. The newest modern data transfer protocol Transparent Interconnection of Lots of Links (TRILL) is presented. Its advantages are analyzed in comparison with the other possible protocols that may be used in the full-mesh topology, like a Spanning tree protocol. Empirical calculations of data routing based on a Dijkstra's algorithm and a patent formula of the TRILL protocol are given. Two monitoring systems of the network segment and download of the data channels are described. The former is a typical packet software; the latter is a newly designed software with an application to graph drawing. The data are presented which were obtained experimentally from 40G interfaces through by each monitoring systems, their behavior is analyzed. The data accuracy in different systems is proved. The main result is that the discrepancy of experimental data with theoretical predictions to be equal to the weight balancing of the traffic when transmitting the batch information over the equivalent edges of the graph. It is shown that the distribution of the traffic over such routes is of arbitrary and inconsistent with the patent formula character. The conclusion analyzes the issues of the traffic behavior under extreme conditions. There are two main questions to be answered. Which way does the distribution of batch data transfer over four equivalent routes occur? What happens if overload takes place? An assumption is made of the need to compare the traffic behavior in various data centers with the help of the traffic generators.
</Content>
<field id="content">
The work is devoted to the result of the creating a first module of the 1-st phase of the data processing center at the Joint Institute for nuclear research for modeling and processing experiments carried out on the test installations of the Large Hadron Collider. The issues related to handling the enormous data flow from the LHC experimental installations and troubles of distributed storages are considered. The article presents a hierarchical diagram of the network farm and a basic model of the network architecture levels. The project documentation of the network based on the Brocade equipment is considered. Protocols for disposal full mesh network topologies are considered. The newest modern data transfer protocol Transparent Interconnection of Lots of Links (TRILL) is presented. Its advantages are analyzed in comparison with the other possible protocols that may be used in the full-mesh topology, like a Spanning tree protocol. Empirical calculations of data routing based on a Dijkstra's algorithm and a patent formula of the TRILL protocol are given. Two monitoring systems of the network segment and download of the data channels are described. The former is a typical packet software; the latter is a newly designed software with an application to graph drawing. The data are presented which were obtained experimentally from 40G interfaces through by each monitoring systems, their behavior is analyzed. The data accuracy in different systems is proved. The main result is that the discrepancy of experimental data with theoretical predictions to be equal to the weight balancing of the traffic when transmitting the batch information over the equivalent edges of the graph. It is shown that the distribution of the traffic over such routes is of arbitrary and inconsistent with the patent formula character. The conclusion analyzes the issues of the traffic behavior under extreme conditions. There are two main questions to be answered. Which way does the distribution of batch data transfer over four equivalent routes occur? What happens if overload takes place? An assumption is made of the need to compare the traffic behavior in various data centers with the help of the traffic generators.
</field>
<field id="summary">
As said before, the network architecture of the TIER 1 data center in JINR of the first module is built on hardware supplied by Brocade company using the modern multichannel data transfer protocol TRILL. The so obtained experimental data direct our activities for the further research of the nature of traffic distribution in redundant topologies. Nowadays, we need to answer several obvious questions. How the distribution is done while transmitting packed data by four peer paths, provided that the conditions of the patent claims are not met? What will happen if the traffic via one of the communication channels achieves peak values? Works in this field are continued and we are developing a test bench to carry out a similar experiment using a traffic generator. It is possible that, in order to confirm the collected data, it will be necessary to build, as in all physical experiments, a similar network fabrics running via TRILL protocol on another's manufacturer's equipment. The proposal for construction of the second data center module is received from Huawei. At present, its implementation possibility is being analyzed. In case of completing the harmonization of this matter, we shall carry out a comparative functional analysis of the TRILL protocol in various data center modules by various manufacturers. This work has immense importance, because the collected data will be used for constructing the Data Processing Center within the NICA (Nuclotron-based Ion Collider fAcility) megaproject.
</field>
<PrimaryAuthor>
<FirstName>Andrey</FirstName>
<FamilyName>Baginyan</FamilyName>
<Email>bagish@mail.ru</Email>
<Affiliation>ccnp</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Vladimir</FirstName>
<FamilyName>Korenkov</FamilyName>
<Email>korenkov@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Andrey</FirstName>
<FamilyName>Dolbilov</FamilyName>
<Email>dolbilov@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Ivan</FirstName>
<FamilyName>Kashunin</FamilyName>
<Email>miramir@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Andrey</FirstName>
<FamilyName>Baginyan</FamilyName>
<Email>bagish@mail.ru</Email>
<Affiliation>ccnp</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Distributed Computing. GRID &amp; Cloud Computing</Track>
<Track>
Computing for Large Scale Facilities (LHC, FAIR, NICA, SKA, PIC, XFEL, ELI, etc.)
</Track>
</abstract>
<abstract>
<Id>145</Id>
<Title>Development of the Booster injection prototype</Title>
<Content>
The report describes requirements for the Booster injection system, its operation algorithm and realization details. The control system is based on National Instruments CompactRIO equipment and realizes injection devices control, synchronization and monitoring. The results of high voltage tests are presented.
</Content>
<field id="content">
The report describes requirements for the Booster injection system, its operation algorithm and realization details. The control system is based on National Instruments CompactRIO equipment and realizes injection devices control, synchronization and monitoring. The results of high voltage tests are presented.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Hristo</FirstName>
<FamilyName>Nazlev</FamilyName>
<Email>hristei@mail.bg</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Anatoly</FirstName>
<FamilyName>Fateev</FamilyName>
<Email>fateev@sunse.jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Vladimir</FirstName>
<FamilyName>Tarasov</FamilyName>
<Email>vtarasov51@mail.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Evgeny</FirstName>
<FamilyName>Gorbachev</FamilyName>
<Email>egorbe@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Hristo</FirstName>
<FamilyName>Nazlev</FamilyName>
<Email>hristei@mail.bg</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Triggering, Data Acquisition, Control Systems</Track>
</abstract>
<abstract>
<Id>146</Id>
<Title>
Development of the CBM RICH readout electronics and DAQ
</Title>
<Content>
The ring imaging Cherenkov detector (RICH) is an integral component of the future Compressed Baryonic Matter (CBM) experiment at FAIR (Darmstadt, Germany). The readout and data acquisition system of the CBM RICH has been studied in detail. Detailed analysis of the readout and DAQ prototype has been conducted using the data gathered during the tests of the CBM RICH prototype in the beam conditions at CERN and using the results of the laboratory measurements performed by means of the specifically developed test stand. One readout and DAQ module prototype consists of a Hamamatsu H12700 MAPMT, the PADIWA preamplifier-discriminator boards, and the TDC‑HUB board TRBv3. Calibration techniques have been developed and implemented along with the DAQ and analysis code in the CbmRoot framework. Optimization of the readout module components has been performed, allowing to achieve best timing characteristics in the high beam rate conditions, expected at CBM. The obtained sub-nanosecond time precision allows also to directly measure the time profile of the additional wavelength-shifting films on top of the MAPMT windows and investigate their effect on timing of the full CBM RICH readout chain. Recent results will be presented, focusing on the comparison of two different types of readout chains, amplitude-sensitive using n-XYTER chip and time-sensitive using the CBM RICH electronics.
</Content>
<field id="content">
The ring imaging Cherenkov detector (RICH) is an integral component of the future Compressed Baryonic Matter (CBM) experiment at FAIR (Darmstadt, Germany). The readout and data acquisition system of the CBM RICH has been studied in detail. Detailed analysis of the readout and DAQ prototype has been conducted using the data gathered during the tests of the CBM RICH prototype in the beam conditions at CERN and using the results of the laboratory measurements performed by means of the specifically developed test stand. One readout and DAQ module prototype consists of a Hamamatsu H12700 MAPMT, the PADIWA preamplifier-discriminator boards, and the TDC‑HUB board TRBv3. Calibration techniques have been developed and implemented along with the DAQ and analysis code in the CbmRoot framework. Optimization of the readout module components has been performed, allowing to achieve best timing characteristics in the high beam rate conditions, expected at CBM. The obtained sub-nanosecond time precision allows also to directly measure the time profile of the additional wavelength-shifting films on top of the MAPMT windows and investigate their effect on timing of the full CBM RICH readout chain. Recent results will be presented, focusing on the comparison of two different types of readout chains, amplitude-sensitive using n-XYTER chip and time-sensitive using the CBM RICH electronics.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Egor</FirstName>
<FamilyName>Ovcharenko</FamilyName>
<Email>evovch@gmail.com</Email>
<Affiliation>JINR, ITEP</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Sergey</FirstName>
<FamilyName>Belogurov</FamilyName>
<Email>belogurov@jinr.ru</Email>
<Affiliation>FLNR JINR</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Egor</FirstName>
<FamilyName>Ovcharenko</FamilyName>
<Email>evovch@gmail.com</Email>
<Affiliation>JINR, ITEP</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Detector &amp; Nuclear Electronics</Track>
<Track>Triggering, Data Acquisition, Control Systems</Track>
<Track>
Computing for Large Scale Facilities (LHC, FAIR, NICA, SKA, PIC, XFEL, ELI, etc.)
</Track>
</abstract>
<abstract>
<Id>147</Id>
<Title>
Optimizing new components of PanDA for ATLAS production on HPC resources
</Title>
<Content>
The Production and Distributed Analysis system (PanDA) has been used for workload management in the ATLAS Experiment for over a decade. It uses pilots to retrieve jobs from the PanDA server and execute them on worker nodes. While PanDA has been mostly used on Worldwide LHC Computing Grid (WLCG) resources for production operations, R&amp;D work has been ongoing on cloud and HPC resources for many years. These efforts have led to the significant usage of large scale HPC resources in the past couple of years. In this talk we will describe the changes to the pilot which enabled the use of HPC sites by PanDA, specifically the Titan supercomputer at Oakridge National Laboratory. Furthermore, it was decided in 2016 to start a fresh redesign of the Pilot with a more modern approach to better serve present and future needs from ATLAS and other collaborations that are interested in using the PanDA System. Another new project for development of a resource oriented service, PanDA Harvester, was also launched in 2016. The main goal of the Harvester is flexible distribution of payloads for opportunistic resources like HPC and clouds. Both applications are now in full development after a year of studying use cases, trying different designs and deciding on the shared components model. This talk will give an overview of the evolution of the HPC pilot into Pilot 2 and Harvester projects for better utilization of HPC resources.
</Content>
<field id="content">
The Production and Distributed Analysis system (PanDA) has been used for workload management in the ATLAS Experiment for over a decade. It uses pilots to retrieve jobs from the PanDA server and execute them on worker nodes. While PanDA has been mostly used on Worldwide LHC Computing Grid (WLCG) resources for production operations, R&amp;D work has been ongoing on cloud and HPC resources for many years. These efforts have led to the significant usage of large scale HPC resources in the past couple of years. In this talk we will describe the changes to the pilot which enabled the use of HPC sites by PanDA, specifically the Titan supercomputer at Oakridge National Laboratory. Furthermore, it was decided in 2016 to start a fresh redesign of the Pilot with a more modern approach to better serve present and future needs from ATLAS and other collaborations that are interested in using the PanDA System. Another new project for development of a resource oriented service, PanDA Harvester, was also launched in 2016. The main goal of the Harvester is flexible distribution of payloads for opportunistic resources like HPC and clouds. Both applications are now in full development after a year of studying use cases, trying different designs and deciding on the shared components model. This talk will give an overview of the evolution of the HPC pilot into Pilot 2 and Harvester projects for better utilization of HPC resources.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Danila</FirstName>
<FamilyName>Oleynik</FamilyName>
<Email>danila@jinr.ru</Email>
<Affiliation>JINR LIT</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Danila</FirstName>
<FamilyName>Oleynik</FamilyName>
<Email>danila@jinr.ru</Email>
<Affiliation>JINR LIT</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Distributed Computing. GRID &amp; Cloud Computing</Track>
</abstract>
<abstract>
<Id>148</Id>
<Title>
NEUTRON GENERATORS AND DAQ SYSTEMS FOR TAGGED NEUTRON TECHNOLOGY
</Title>
<Content>
At the T(d,n)He4 reaction each 14 MeV neutron is accompanied by a 3.5 MeV alpha- particle emitted in the opposite direction. A position- and time-sensitive alpha-detector measures time and coordinates of the associated alpha particle which allows determining time and direction (tags) of neutron escape. The tagged neutron technology is based on a time and spatial selection of events that occur when a tagged neutron moves through the object. The ING-27 neutron generators produced by VNIIA provide high intensity of tagged neutrons in a wide cone angle, the high spatial and time resolution of tagged the neutrons is provided by the pixelated alpha-detector. The requirements to DAQ system for various tagged neutron devices are reported. The architecture and parameters of DAQ system based on preliminary online selection of signals by analog front-end electronics and transmission of only useful events for subsequent computer processing are considered. The examples of tagged neutron devices for various applications are considered.
</Content>
<field id="content">
At the T(d,n)He4 reaction each 14 MeV neutron is accompanied by a 3.5 MeV alpha- particle emitted in the opposite direction. A position- and time-sensitive alpha-detector measures time and coordinates of the associated alpha particle which allows determining time and direction (tags) of neutron escape. The tagged neutron technology is based on a time and spatial selection of events that occur when a tagged neutron moves through the object. The ING-27 neutron generators produced by VNIIA provide high intensity of tagged neutrons in a wide cone angle, the high spatial and time resolution of tagged the neutrons is provided by the pixelated alpha-detector. The requirements to DAQ system for various tagged neutron devices are reported. The architecture and parameters of DAQ system based on preliminary online selection of signals by analog front-end electronics and transmission of only useful events for subsequent computer processing are considered. The examples of tagged neutron devices for various applications are considered.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Maxim</FirstName>
<FamilyName>Karetnikov</FamilyName>
<Email>mcsym01@gmail.com</Email>
<Affiliation>VNIIA</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Maxim</FirstName>
<FamilyName>Karetnikov</FamilyName>
<Email>mcsym01@gmail.com</Email>
<Affiliation>VNIIA</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Detector &amp; Nuclear Electronics</Track>
</abstract>
<abstract>
<Id>149</Id>
<Title>Service monitoring system for Tier-1 in JINR</Title>
<Content>
The computing center at JINR is one of the thirteen Tier-1 Grid-centers which provide critical services for processing, storage and analysis of all the data from the LHC. It is important to keep track of all indicators and take action fast in case of events and processes which lead to system malfunction. In most cases modern monitoring systems do not satisfy all the requirements coming from administrators, so they need to keep an eye on different sources of information about the functioning of a computing center. A monitoring system was developed for the purposes of JINR Tier-1 center to improve maintenance and provide an overview of its performance. The monitoring system consists of two parts: the core and the monitoring modules. The monitoring modules themselves consist of a data collection part which is written in Python and a data visualization part which is written in HTML and JavaScript. This monitoring system is supposed to be a single point for providing all necessary information about Tier-1 services and systems.
</Content>
<field id="content">
The computing center at JINR is one of the thirteen Tier-1 Grid-centers which provide critical services for processing, storage and analysis of all the data from the LHC. It is important to keep track of all indicators and take action fast in case of events and processes which lead to system malfunction. In most cases modern monitoring systems do not satisfy all the requirements coming from administrators, so they need to keep an eye on different sources of information about the functioning of a computing center. A monitoring system was developed for the purposes of JINR Tier-1 center to improve maintenance and provide an overview of its performance. The monitoring system consists of two parts: the core and the monitoring modules. The monitoring modules themselves consist of a data collection part which is written in Python and a data visualization part which is written in HTML and JavaScript. This monitoring system is supposed to be a single point for providing all necessary information about Tier-1 services and systems.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Ivan</FirstName>
<FamilyName>Kadochnikov</FamilyName>
<Email>kadivas@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Vladimir</FirstName>
<FamilyName>Korenkov</FamilyName>
<Email>korenkov@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Valery</FirstName>
<FamilyName>Mitsyn</FamilyName>
<Email>vvm@mammoth.jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Igor</FirstName>
<FamilyName>Pelevanyuk</FamilyName>
<Email>gavelock@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Tatiana</FirstName>
<FamilyName>Strizh</FamilyName>
<Email>strizh@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Igor</FirstName>
<FamilyName>Pelevanyuk</FamilyName>
<Email>gavelock@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Distributed Computing. GRID &amp; Cloud Computing</Track>
</abstract>
<abstract>
<Id>150</Id>
<Title>
The Trigger Readout Electronics for the Phase-1 Upgrade of the ATLAS Liquid-Argon Calorimeters
</Title>
<Content>
The upgrade of the Large Hadron Collider (LHC) scheduled for the shut-down period of 2018-2019 (Phase-I upgrade), will increase the instantaneous luminosity to about three times the design value. Since the current ATLAS trigger system does not allow a corresponding increase of the trigger rate, an improvement of the trigger system is required. The new trigger signals from the ATLAS Liquid Argon Calorimeter will be arranged in 34000 so-called Super Cells which achieve 5-10 times better granularity than the current system; this improves the background rejection capabilities through more precise energy measurements, and the use of shower shapes to discriminate electrons and photons from jets. The new system will process the signal of the Super Cells at every LHC bunch-crossing at 12-bit precision and a frequency of 40 MHz. The data will be transmitted to the back-end using a custom serializer and optical converter with 5.12 Gb/s. To verify the full functionality, a demonstrator set-up has been installed on the ATLAS detector and operated during the LHC Run-2 of the LHC. The talk will give a status on hardware developments towards the final design readout system, including the performance of the newly developed ASICs. Their radiation tolerance, the performance of the prototype boards, results of the high-speed link test with the prototypes and the performance of the demonstrator with collision data will be also reported.
</Content>
<field id="content">
The upgrade of the Large Hadron Collider (LHC) scheduled for the shut-down period of 2018-2019 (Phase-I upgrade), will increase the instantaneous luminosity to about three times the design value. Since the current ATLAS trigger system does not allow a corresponding increase of the trigger rate, an improvement of the trigger system is required. The new trigger signals from the ATLAS Liquid Argon Calorimeter will be arranged in 34000 so-called Super Cells which achieve 5-10 times better granularity than the current system; this improves the background rejection capabilities through more precise energy measurements, and the use of shower shapes to discriminate electrons and photons from jets. The new system will process the signal of the Super Cells at every LHC bunch-crossing at 12-bit precision and a frequency of 40 MHz. The data will be transmitted to the back-end using a custom serializer and optical converter with 5.12 Gb/s. To verify the full functionality, a demonstrator set-up has been installed on the ATLAS detector and operated during the LHC Run-2 of the LHC. The talk will give a status on hardware developments towards the final design readout system, including the performance of the newly developed ASICs. Their radiation tolerance, the performance of the prototype boards, results of the high-speed link test with the prototypes and the performance of the demonstrator with collision data will be also reported.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Yuji</FirstName>
<FamilyName>Enari</FamilyName>
<Email>yuji.enari@cern.ch</Email>
<Affiliation>ICEPP, University of Tokyo</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Robert</FirstName>
<FamilyName>Wolff</FamilyName>
<Email>wolff@cppm.in2p3.fr</Email>
<Affiliation>CPPM, Aix-Marseille Université, CNRS/IN2P3 (FR)</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Robert</FirstName>
<FamilyName>Wolff</FamilyName>
<Email>wolff@cppm.in2p3.fr</Email>
<Affiliation>CPPM, Aix-Marseille Université, CNRS/IN2P3 (FR)</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Triggering, Data Acquisition, Control Systems</Track>
<Track>
Advanced Technologies for the High-Intensity Domains of Science and Business Applications
</Track>
</abstract>
<abstract>
<Id>151</Id>
<Title>The Phase-II upgrade of ATLAS Calorimeter</Title>
<Content>
This presentation will show the status of the upgrade projects of the ATLAS calorimeter system for the high luminosity phase of the LHC (HL-LHC). For the HL-LHC, the instantaneous luminosity is expected to increase up to L ≃ 7.5 × 10^{34} cm^{−2} s^{−1} and the average pile-up up to 200 interactions per bunch crossing. The Liquid Argon (LAr) calorimeter electronics will need to be replaced to cope with these challenging conditions: the expected radiation doses will indeed exceed the qualification range of the current readout system, and the upgraded trigger system will require much longer data storage in the electronics (up to 60µs), that the current system cannot sustain. The status on the R&amp;D of the low-power ASICs (pre-amplifier, shaper, ADC, selializer and transmitters) and readout electronics design will be discussed. Moreover, a High Granularity Timing Detector (HGTD) is proposed to be added on front of the LAr calorimeters in the end-cap region (2.4 &lt;|eta|&lt; 4.2) for pile-up mitigation at Level-0 trigger level and offline reconstruction. The HGTD will correlate the energy deposits in the calorimeter to different proton-proton collision vertices by using time of flight information with high timing resolution (30 pico-second per readout cell) based on the Silicon sensor technologies. The current test beam results will be presented as well as performance expectation of the new detector.
</Content>
<field id="content">
This presentation will show the status of the upgrade projects of the ATLAS calorimeter system for the high luminosity phase of the LHC (HL-LHC). For the HL-LHC, the instantaneous luminosity is expected to increase up to L ≃ 7.5 × 10^{34} cm^{−2} s^{−1} and the average pile-up up to 200 interactions per bunch crossing. The Liquid Argon (LAr) calorimeter electronics will need to be replaced to cope with these challenging conditions: the expected radiation doses will indeed exceed the qualification range of the current readout system, and the upgraded trigger system will require much longer data storage in the electronics (up to 60µs), that the current system cannot sustain. The status on the R&amp;D of the low-power ASICs (pre-amplifier, shaper, ADC, selializer and transmitters) and readout electronics design will be discussed. Moreover, a High Granularity Timing Detector (HGTD) is proposed to be added on front of the LAr calorimeters in the end-cap region (2.4 &lt;|eta|&lt; 4.2) for pile-up mitigation at Level-0 trigger level and offline reconstruction. The HGTD will correlate the energy deposits in the calorimeter to different proton-proton collision vertices by using time of flight information with high timing resolution (30 pico-second per readout cell) based on the Silicon sensor technologies. The current test beam results will be presented as well as performance expectation of the new detector.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Yuji</FirstName>
<FamilyName>Enari</FamilyName>
<Email>yuji.enari@cern.ch</Email>
<Affiliation>ICEPP, University of Tokyo</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Francesco</FirstName>
<FamilyName>Tartarelli</FamilyName>
<Email>francesco.tartarelli@mi.infn.it</Email>
<Affiliation>Università degli Studi e INFN Milano</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Francesco</FirstName>
<FamilyName>Tartarelli</FamilyName>
<Email>francesco.tartarelli@mi.infn.it</Email>
<Affiliation>Università degli Studi e INFN Milano</Affiliation>
</Speaker>
<ContributionType>Plenary</ContributionType>
<Track>Detector &amp; Nuclear Electronics</Track>
<Track>Triggering, Data Acquisition, Control Systems</Track>
<Track>
Advanced Technologies for the High-Intensity Domains of Science and Business Applications
</Track>
</abstract>
<abstract>
<Id>152</Id>
<Title>The ATLAS Production System Evolution</Title>
<Content>
The second generation of the ATLAS Production System called ProdSys2 is a distributed workload manager that runs daily hundreds of thousands of jobs, from dozens of different ATLAS-specific workflows, across more than a hundred heterogeneous sites. It achieves high utilization by combining dynamic job definition based upon many criteria, such as input and output size, memory requirements and CPU consumption, with manageable scheduling policies and by supporting different kinds of computational resources, such as GRID, clouds, supercomputers and volunteer computers. The system dynamically assigns a group of jobs (task) to a group of geographically distributed computing resources. Dynamic assignment and resource utilization is one of the major features of the system. The Production System has a sophisticated job fault recovery mechanism, which efficiently allows running multi-terabyte tasks without human intervention. We have implemented new features which allow automatic task submission and chaining of different types of production. We present recent improvements of the ATLAS Production System and its major components: task definition and web user interface. We also report the performance of the designed system and how various workflows, such as data (re)processing, Monte Carlo and physics group production, and user analysis, are scheduled and executed within one production system on heterogeneous computing resources.
</Content>
<field id="content">
The second generation of the ATLAS Production System called ProdSys2 is a distributed workload manager that runs daily hundreds of thousands of jobs, from dozens of different ATLAS-specific workflows, across more than a hundred heterogeneous sites. It achieves high utilization by combining dynamic job definition based upon many criteria, such as input and output size, memory requirements and CPU consumption, with manageable scheduling policies and by supporting different kinds of computational resources, such as GRID, clouds, supercomputers and volunteer computers. The system dynamically assigns a group of jobs (task) to a group of geographically distributed computing resources. Dynamic assignment and resource utilization is one of the major features of the system. The Production System has a sophisticated job fault recovery mechanism, which efficiently allows running multi-terabyte tasks without human intervention. We have implemented new features which allow automatic task submission and chaining of different types of production. We present recent improvements of the ATLAS Production System and its major components: task definition and web user interface. We also report the performance of the designed system and how various workflows, such as data (re)processing, Monte Carlo and physics group production, and user analysis, are scheduled and executed within one production system on heterogeneous computing resources.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Mikhail</FirstName>
<FamilyName>Borodin</FamilyName>
<Email>mborodin@cern.ch</Email>
<Affiliation>The University of Iowa (US)</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Alexei</FirstName>
<FamilyName>Klimentov</FamilyName>
<Email>alexei.klimentov@cern.ch</Email>
<Affiliation>Brookhaven National Lab</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Mikhail</FirstName>
<FamilyName>Borodin</FamilyName>
<Email>mborodin@cern.ch</Email>
<Affiliation>The University of Iowa (US)</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Distributed Computing. GRID &amp; Cloud Computing</Track>
<Track>
Computing for Large Scale Facilities (LHC, FAIR, NICA, SKA, PIC, XFEL, ELI, etc.)
</Track>
<Track>
Computations with Hybrid Systems (CPU, GPU, coprocessors)
</Track>
</abstract>
<abstract>
<Id>153</Id>
<Title>
Software for magnetic measurements test bench for NICA and FAIR superconducting magnets
</Title>
<Content>
The software, which has been used for the magnetic measurements test bench for superconducting magnets of NICA and FAIR projects is described. Main measurement program that is used in order to collect measured data, and is responsible for sensor position as well as software for processing measured data are presented. Filtering and smoothing algorithm based on wavelets and splines that were used before data processing are also described.
</Content>
<field id="content">
The software, which has been used for the magnetic measurements test bench for superconducting magnets of NICA and FAIR projects is described. Main measurement program that is used in order to collect measured data, and is responsible for sensor position as well as software for processing measured data are presented. Filtering and smoothing algorithm based on wavelets and splines that were used before data processing are also described.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Alexander</FirstName>
<FamilyName>Bychkov</FamilyName>
<Email>abychkov@jinr.ru</Email>
<Affiliation>LHEP</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Alexander</FirstName>
<FamilyName>Bychkov</FamilyName>
<Email>abychkov@jinr.ru</Email>
<Affiliation>LHEP</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Triggering, Data Acquisition, Control Systems</Track>
<Track>
Computing for Large Scale Facilities (LHC, FAIR, NICA, SKA, PIC, XFEL, ELI, etc.)
</Track>
</abstract>
<abstract>
<Id>154</Id>
<Title>
Application of NVIDIA CUDA technology to calculation of ground states of few-body nuclei
</Title>
<Content>
The modern parallel computing solutions were used to speed up the calculations by Feynman’s continual integrals method. The algorithm was implemented in C++ programming language. Calculations using NVIDIA CUDA technology were performed on the NVIDIA Tesla K40 accelerator installed within the heterogeneous cluster of the Laboratory of Information Technologies, Joint Institute for Nuclear Research, Dubna. The results for energies of the ground states of several few-body nuclei demonstrate overall good agreement with experimental data. The obtained square modulus of the wave function of the ground states provided the possibility of investigating the spatial structure of the studied nuclei. The use of general-purpose computing on graphics processing units significantly (two orders of magnitude) increases the speed of calculations. This approach may be useful for investigation of any few-body system including few-quark systems and may serve as an addition to other well-known methods, e.g., Gaussian expansion method and hyperspherical-harmonics technique.
</Content>
<field id="content">
The modern parallel computing solutions were used to speed up the calculations by Feynman’s continual integrals method. The algorithm was implemented in C++ programming language. Calculations using NVIDIA CUDA technology were performed on the NVIDIA Tesla K40 accelerator installed within the heterogeneous cluster of the Laboratory of Information Technologies, Joint Institute for Nuclear Research, Dubna. The results for energies of the ground states of several few-body nuclei demonstrate overall good agreement with experimental data. The obtained square modulus of the wave function of the ground states provided the possibility of investigating the spatial structure of the studied nuclei. The use of general-purpose computing on graphics processing units significantly (two orders of magnitude) increases the speed of calculations. This approach may be useful for investigation of any few-body system including few-quark systems and may serve as an addition to other well-known methods, e.g., Gaussian expansion method and hyperspherical-harmonics technique.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Viacheslav</FirstName>
<FamilyName>Samarin</FamilyName>
<Email>samarin@jinr.ru</Email>
<Affiliation>
Joint Institute for Nuclear Research, Flerov Laboratory of Nuclear Reactions
</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Mikhail</FirstName>
<FamilyName>Naumenko</FamilyName>
<Email>anaumenko@jinr.ru</Email>
<Affiliation>Joint Institute for Nuclear Research</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Viacheslav</FirstName>
<FamilyName>Samarin</FamilyName>
<Email>samarin@jinr.ru</Email>
<Affiliation>
Joint Institute for Nuclear Research, Flerov Laboratory of Nuclear Reactions
</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>
Computations with Hybrid Systems (CPU, GPU, coprocessors)
</Track>
</abstract>
<abstract>
<Id>155</Id>
<Title>Online monitoring system for the BM@N experiment</Title>
<Content>
The BM@N experiment is the crucial stage in the technical development of the NICA project. In order to effectively maintain experiment it is extremely important to have uniform for all detectors, fast and convenient tool to monitor experimental facility. The system implements decoding of the incoming raw data on the fly, preprocessing and visualization on the webpage. Users can monitor any detector subsystem, select specific detector's plane/station, time or strip profile histograms in 1/2/3D view. The system is developed as a part of the BmnRoot package with the use of the CERN jsROOT library. The lighttpd webserver is used.
</Content>
<field id="content">
The BM@N experiment is the crucial stage in the technical development of the NICA project. In order to effectively maintain experiment it is extremely important to have uniform for all detectors, fast and convenient tool to monitor experimental facility. The system implements decoding of the incoming raw data on the fly, preprocessing and visualization on the webpage. Users can monitor any detector subsystem, select specific detector's plane/station, time or strip profile histograms in 1/2/3D view. The system is developed as a part of the BmnRoot package with the use of the CERN jsROOT library. The lighttpd webserver is used.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Ilnur</FirstName>
<FamilyName>Gabdrakhmanov</FamilyName>
<Email>ilnur@jinr.ru</Email>
<Affiliation>VBLHEP</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>S</FirstName>
<FamilyName>Merts</FamilyName>
<Email>sergey.merts@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Ilnur</FirstName>
<FamilyName>Gabdrakhmanov</FamilyName>
<Email>ilnur@jinr.ru</Email>
<Affiliation>VBLHEP</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Triggering, Data Acquisition, Control Systems</Track>
</abstract>
<abstract>
<Id>156</Id>
<Title>
Collecting virtual machine performance metrics for cloud management
</Title>
<Content>
The cloud infrastructure at JINR has different use-cases with different resource requirement. To optimize resource utilization under irregular load it is necessary to gather preformance data about every virtual machine in the cloud. Existing general-purpose monitoring tools were used to minimize development effort. After a practical comparison of popular systems Icinga2 was selected for this purpose. The setup for collecting performance data is very straightforward. A SNMP extension runs on the cloud hosts and provides the required VM load metrics. They are polled over SNMP by an Icinga2 plugin. To make use of this information, it needs to be kept for analysis over a long period of time. Icinga2 normally puts historical data in NoSQL time-series databases Graphite and InfluxDB. Both are used to store collected data and show good performance and usability. As more data gets collected, it will be possible and necessary to choose the better of the two database backends. The performance information can be used for simple analysis and visualization with Grafana, as well as for cloud management automation research.
</Content>
<field id="content">
The cloud infrastructure at JINR has different use-cases with different resource requirement. To optimize resource utilization under irregular load it is necessary to gather preformance data about every virtual machine in the cloud. Existing general-purpose monitoring tools were used to minimize development effort. After a practical comparison of popular systems Icinga2 was selected for this purpose. The setup for collecting performance data is very straightforward. A SNMP extension runs on the cloud hosts and provides the required VM load metrics. They are polled over SNMP by an Icinga2 plugin. To make use of this information, it needs to be kept for analysis over a long period of time. Icinga2 normally puts historical data in NoSQL time-series databases Graphite and InfluxDB. Both are used to store collected data and show good performance and usability. As more data gets collected, it will be possible and necessary to choose the better of the two database backends. The performance information can be used for simple analysis and visualization with Grafana, as well as for cloud management automation research.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Alexandr</FirstName>
<FamilyName>Baranov</FamilyName>
<Email>baranov@jinr.ru</Email>
<Affiliation>(JINR)</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Ivan</FirstName>
<FamilyName>Kadochnikov</FamilyName>
<Email>kadivas@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Nikita</FirstName>
<FamilyName>Balashov</FamilyName>
<Email>balashov.nikita@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Sergey</FirstName>
<FamilyName>Belov</FamilyName>
<Email>belov@jinr.ru</Email>
<Affiliation>Joint Institute for Nuclear Research</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Andrey</FirstName>
<FamilyName>Nechaevskiy</FamilyName>
<Email>nechav@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Vladimir</FirstName>
<FamilyName>Korenkov</FamilyName>
<Email>korenkov@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Nikolay</FirstName>
<FamilyName>Kutovskiy</FamilyName>
<Email>kut@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Igor</FirstName>
<FamilyName>Pelevanyuk</FamilyName>
<Email>gavelock@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Ivan</FirstName>
<FamilyName>Kadochnikov</FamilyName>
<Email>kadivas@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Distributed Computing. GRID &amp; Cloud Computing</Track>
<Track>
Non-relational Databases and Heterogeneous Repositories
</Track>
</abstract>
<abstract>
<Id>157</Id>
<Title>
The ATLAS Trigger system upgrade and performance in Run 2
</Title>
<Content>
The ATLAS trigger has been used very successfully for the online event selection during the first part of the second LHC run (Run-2) in 2015/16 at a centre-of-mass energy of 13 TeV. The trigger system is composed of a hardware Level-1 trigger and a software-based high-level trigger; it reduces the event rate from the bunch-crossing rate of 40 MHz to an average recording rate of about 1 kHz. The excellent performance of the ATLAS trigger has been vital for the ATLAS physics program of Run-2, selecting interesting collision events for wide variety of physics signatures with high efficiency. The trigger selection capabilities of ATLAS during Run-2 have been significantly improved compared to Run-1, in order to cope with the higher event rates and pile-up which are the result of the almost doubling of the center-of-mass collision energy and the increase in the instantaneous luminosity of the LHC. At the Level-1 trigger the undertaken improvements resulted in more pile-up robust selection efficiencies and event rates and in a reduction of fake candidate particles. A new hardware system, designed to analyse event-topologies, supports a more refined event selection at the Level-1. A hardware-based high-rate track reconstruction, currently being commissioned, enables the software trigger to make use of tracking information at the full input rate. Together with a re-design of the high-level trigger to deploy more offline-like reconstruction techniques, these changes improve the performance of the trigger selection turn-on and efficiency to nearly that of the offline reconstruction. In order to prepare for the anticipated further luminosity increase of the LHC in 2017/18, improving the trigger performance remains an ongoing endeavour. Thereby coping with the large number of pile-up events is one of the most prominent challenges. This presentation gives a short review the ATLAS trigger system and its performance in 2015/16 before describing the significant improvements in selection sensitivity and pile-up robustness, which we implemented in preparation for the expected highest ever luminosities of the 2017/18 LHC.
</Content>
<field id="content">
The ATLAS trigger has been used very successfully for the online event selection during the first part of the second LHC run (Run-2) in 2015/16 at a centre-of-mass energy of 13 TeV. The trigger system is composed of a hardware Level-1 trigger and a software-based high-level trigger; it reduces the event rate from the bunch-crossing rate of 40 MHz to an average recording rate of about 1 kHz. The excellent performance of the ATLAS trigger has been vital for the ATLAS physics program of Run-2, selecting interesting collision events for wide variety of physics signatures with high efficiency. The trigger selection capabilities of ATLAS during Run-2 have been significantly improved compared to Run-1, in order to cope with the higher event rates and pile-up which are the result of the almost doubling of the center-of-mass collision energy and the increase in the instantaneous luminosity of the LHC. At the Level-1 trigger the undertaken improvements resulted in more pile-up robust selection efficiencies and event rates and in a reduction of fake candidate particles. A new hardware system, designed to analyse event-topologies, supports a more refined event selection at the Level-1. A hardware-based high-rate track reconstruction, currently being commissioned, enables the software trigger to make use of tracking information at the full input rate. Together with a re-design of the high-level trigger to deploy more offline-like reconstruction techniques, these changes improve the performance of the trigger selection turn-on and efficiency to nearly that of the offline reconstruction. In order to prepare for the anticipated further luminosity increase of the LHC in 2017/18, improving the trigger performance remains an ongoing endeavour. Thereby coping with the large number of pile-up events is one of the most prominent challenges. This presentation gives a short review the ATLAS trigger system and its performance in 2015/16 before describing the significant improvements in selection sensitivity and pile-up robustness, which we implemented in preparation for the expected highest ever luminosities of the 2017/18 LHC.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Jiri</FirstName>
<FamilyName>Masik</FamilyName>
<Email>jiri.masik@manchester.ac.uk</Email>
<Affiliation>on behalf of the ATLAS TDAQ speakers committee</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Jiri</FirstName>
<FamilyName>Masik</FamilyName>
<Email>jiri.masik@manchester.ac.uk</Email>
<Affiliation>on behalf of the ATLAS TDAQ speakers committee</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Triggering, Data Acquisition, Control Systems</Track>
</abstract>
<abstract>
<Id>158</Id>
<Title>
Data management and processing system for Baikal-GVD
</Title>
<Content>
Baikal-GVD is a gigaton volume underwater neutrino detector located in Lake Baikal. Compared to NT-200+, a previous iteration of the Baikal neurino observatory, Baikal-GVD represents a leap in complexity and raw data output. Therefore, a new, comprehensive data management infrastructure for transfer and analysis of the experimental data has been established. It includes a two-tier data storage, data quality control and online processing facilities. Experimental data is analyzed with BARS (Baikal Analysis and Reconstruction Software) - a framework, designed specifically for Baikal-GVD, that provides both low level interface for data analysis and a set of high level utilities for common tasks.
</Content>
<field id="content">
Baikal-GVD is a gigaton volume underwater neutrino detector located in Lake Baikal. Compared to NT-200+, a previous iteration of the Baikal neurino observatory, Baikal-GVD represents a leap in complexity and raw data output. Therefore, a new, comprehensive data management infrastructure for transfer and analysis of the experimental data has been established. It includes a two-tier data storage, data quality control and online processing facilities. Experimental data is analyzed with BARS (Baikal Analysis and Reconstruction Software) - a framework, designed specifically for Baikal-GVD, that provides both low level interface for data analysis and a set of high level utilities for common tasks.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Bair</FirstName>
<FamilyName>Shaybonov</FamilyName>
<Email>bairsh@yandex.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Alexander</FirstName>
<FamilyName>Avrorin</FamilyName>
<Email>a.d.avrorin@gmail.com</Email>
<Affiliation>INR RAS</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Alexander</FirstName>
<FamilyName>Avrorin</FamilyName>
<Email>a.d.avrorin@gmail.com</Email>
<Affiliation>INR RAS</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Triggering, Data Acquisition, Control Systems</Track>
</abstract>
<abstract>
<Id>159</Id>
<Title>
The ATLAS Trigger Menu design for higher luminosities in Run 2
</Title>
<Content>
The ATLAS experiment aims at recording about 1 kHz of physics collisions, starting with an LHC design bunch crossing rate of 40 MHz. To reduce the large background rate while maintaining a high selection efficiency for rare physics events (such as beyond the Standard Model physics), a two-level trigger system is used. Events are selected based on physics signatures such as the presence of energetic leptons, photons, jets or large missing energy. The trigger system exploits topological information, as well as multivariate methods to carry out the necessary physics filtering for the many analyses that are pursued by the ATLAS community. In total, the ATLAS online selection consists of nearly two thousand individual triggers. A Trigger Menu is the compilation of these triggers, it specifies the physics selection algorithms to be used during data taking and the rate and bandwidth a given trigger is allocated. Trigger menus must reflect the physics goals of the collaboration for a given run, but also take into consideration the instantaneous luminosity of the LHC and limitations from the ATLAS detector readout and offline processing farm. For the 2017 run, the ATLAS trigger has been enhanced to be able to handle higher instantaneous luminosities (up to 2.0x10^{34}cm^{-2}s^{-1}) and to ensure the selection robustness against higher average multiple interactions per bunch crossing. In this presentation we describe the design criteria for the trigger menu for Run 2. We discuss several aspects of the process of planning the trigger menu, starting from how ATLAS physics goals and the need for detector performance measurements enter the menu design, and how rate, bandwidth, and CPU constraints are folded in during the compilation of the menu. We present the tools that allow us to predict and optimize the trigger rates and CPU consumption for the anticipated LHC luminosities. We outline the online system that we implemented to monitor deviations from the individual trigger target rates and to quickly react to changing LHC conditions and data taking scenarios. Finally we give a glimpse of the 2017 Trigger Menu, allowing the listener to get a taste of the vast physics program that the trigger is supporting.
</Content>
<field id="content">
The ATLAS experiment aims at recording about 1 kHz of physics collisions, starting with an LHC design bunch crossing rate of 40 MHz. To reduce the large background rate while maintaining a high selection efficiency for rare physics events (such as beyond the Standard Model physics), a two-level trigger system is used. Events are selected based on physics signatures such as the presence of energetic leptons, photons, jets or large missing energy. The trigger system exploits topological information, as well as multivariate methods to carry out the necessary physics filtering for the many analyses that are pursued by the ATLAS community. In total, the ATLAS online selection consists of nearly two thousand individual triggers. A Trigger Menu is the compilation of these triggers, it specifies the physics selection algorithms to be used during data taking and the rate and bandwidth a given trigger is allocated. Trigger menus must reflect the physics goals of the collaboration for a given run, but also take into consideration the instantaneous luminosity of the LHC and limitations from the ATLAS detector readout and offline processing farm. For the 2017 run, the ATLAS trigger has been enhanced to be able to handle higher instantaneous luminosities (up to 2.0x10^{34}cm^{-2}s^{-1}) and to ensure the selection robustness against higher average multiple interactions per bunch crossing. In this presentation we describe the design criteria for the trigger menu for Run 2. We discuss several aspects of the process of planning the trigger menu, starting from how ATLAS physics goals and the need for detector performance measurements enter the menu design, and how rate, bandwidth, and CPU constraints are folded in during the compilation of the menu. We present the tools that allow us to predict and optimize the trigger rates and CPU consumption for the anticipated LHC luminosities. We outline the online system that we implemented to monitor deviations from the individual trigger target rates and to quickly react to changing LHC conditions and data taking scenarios. Finally we give a glimpse of the 2017 Trigger Menu, allowing the listener to get a taste of the vast physics program that the trigger is supporting.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Jiri</FirstName>
<FamilyName>Masik</FamilyName>
<Email>jiri.masik@manchester.ac.uk</Email>
<Affiliation>on behalf of the ATLAS TDAQ speakers committee</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Jiri</FirstName>
<FamilyName>Masik</FamilyName>
<Email>jiri.masik@manchester.ac.uk</Email>
<Affiliation>on behalf of the ATLAS TDAQ speakers committee</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Triggering, Data Acquisition, Control Systems</Track>
</abstract>
<abstract>
<Id>160</Id>
<Title>Performance of the ATLAS Muon Trigger in Run 2</Title>
<Content>
Events containing muons in the final state are an important signature for many analyses being carried out at the Large Hadron Collider (LHC), including both standard model measurements and searches for new physics. To be able to study such events, it is required to have an efficient and well-understood muon trigger. The ATLAS muon trigger consists of a hardware based system (Level 1), as well as a software based reconstruction (High Level Trigger). Due to high luminosity and pile up conditions in Run 2, several improvements have been implemented to keep the trigger rate low while still maintaining a high efficiency. Some examples of recent improvements include requiring coincidence hits between different layers of the muon spectrometer, improvements for handling overlapping muons, and optimised muon isolation. We will present an overview of how we trigger on muons, recent improvements, and the performance of the muon trigger in Run 2 data.
</Content>
<field id="content">
Events containing muons in the final state are an important signature for many analyses being carried out at the Large Hadron Collider (LHC), including both standard model measurements and searches for new physics. To be able to study such events, it is required to have an efficient and well-understood muon trigger. The ATLAS muon trigger consists of a hardware based system (Level 1), as well as a software based reconstruction (High Level Trigger). Due to high luminosity and pile up conditions in Run 2, several improvements have been implemented to keep the trigger rate low while still maintaining a high efficiency. Some examples of recent improvements include requiring coincidence hits between different layers of the muon spectrometer, improvements for handling overlapping muons, and optimised muon isolation. We will present an overview of how we trigger on muons, recent improvements, and the performance of the muon trigger in Run 2 data.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Jiri</FirstName>
<FamilyName>Masik</FamilyName>
<Email>jiri.masik@manchester.ac.uk</Email>
<Affiliation>on behalf of the ATLAS TDAQ speakers committee</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Jiri</FirstName>
<FamilyName>Masik</FamilyName>
<Email>jiri.masik@manchester.ac.uk</Email>
<Affiliation>on behalf of the ATLAS TDAQ speakers committee</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Triggering, Data Acquisition, Control Systems</Track>
</abstract>
<abstract>
<Id>161</Id>
<Title>
New particle position determination modules for Double Side Silicon Strip Detector at DGFRS
</Title>
<Content>
New particle position determination modules for Double side silicon strip detector were designed that allow to simplify existing multi-channel measurement system in search for the rare events of super heavy elements formation at DGFRS. The main principle is to search position correlated sequences of implanted SHE and followed alpha-particles/or SF events above predefined threshold Energy level in real-time for all 128 back strips. The resulting information is about providing the address of active strip and the coincidence sign. The newly developed system trigger passed the prototyping stage and is about to use in next experiment. This system will reduce the overall system dead time. This talk is about to description in deep of the CD32-5M coder units and the PKK-05 preregister unit briefly introduced by this abstract.
</Content>
<field id="content">
New particle position determination modules for Double side silicon strip detector were designed that allow to simplify existing multi-channel measurement system in search for the rare events of super heavy elements formation at DGFRS. The main principle is to search position correlated sequences of implanted SHE and followed alpha-particles/or SF events above predefined threshold Energy level in real-time for all 128 back strips. The resulting information is about providing the address of active strip and the coincidence sign. The newly developed system trigger passed the prototyping stage and is about to use in next experiment. This system will reduce the overall system dead time. This talk is about to description in deep of the CD32-5M coder units and the PKK-05 preregister unit briefly introduced by this abstract.
</field>
<field id="summary">
A new series of experiments aimed on the synthesis and study of decay properties of the most neutron-deficient isotopes of element Flerovium (Z = 114) and of the heaviest isotopes of 118 element was performed on the Dubna Gas-filled recoil separator. An appropriate registering system is to be implemented to transfer spectrometric data from double-sided silicon strip detector. New measuring and automatization modules were designed that allow to simplify existing multi-channel measurement system in search for the rare events of super heavy elements formation. The newly developed position determination system trigger passed the prototyping stage and is about to use in next experiment.
</field>
<PrimaryAuthor>
<FirstName>Leo</FirstName>
<FamilyName>Schlattauer</FamilyName>
<Email>schlattauer@jinr.ru</Email>
<Affiliation>
Palacky University Olomouc, Czech Republic, JINR Dubna
</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Alexey</FirstName>
<FamilyName>Voinov</FamilyName>
<Email>voinov_2000@mail.ru</Email>
<Affiliation>FLNR JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>V. G.</FirstName>
<FamilyName>Subbotin</FamilyName>
<Email>subbotin@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Yury</FirstName>
<FamilyName>Tsyganov</FamilyName>
<Email>tyra@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Leo</FirstName>
<FamilyName>Schlattauer</FamilyName>
<Email>schlattauer@jinr.ru</Email>
<Affiliation>
Palacky University Olomouc, Czech Republic, JINR Dubna
</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Detector &amp; Nuclear Electronics</Track>
<Track>Triggering, Data Acquisition, Control Systems</Track>
</abstract>
<abstract>
<Id>162</Id>
<Title>
The design and performance of the ATLAS Inner Detector trigger in high pileup collisions at 13 TeV at the Large Hadron Collider
</Title>
<Content>
The design and performance of the ATLAS Inner Detector (ID) trigger algorithms running online on the high level trigger (HLT) processor farm for 13 TeV LHC collision data with high pileup are discussed. The HLT ID tracking is a vital component in all physics signatures in the ATLAS Trigger for the precise selection of the rare or interesting events necessary for physics analysis without overwhelming the offline data storage in terms of both size and rate. To cope with the high expected interaction rates in the 13 TeV LHC collisions the ID trigger was redesigned during the 2013-15 long shutdown. The performance of the ID Trigger in the 2016 data from 13 TeV LHC collisions has been excellent and exceeded expectations as the interaction multiplicity increased throughout the year. The detailed efficiencies and resolutions of the trigger in a wide range of physics signatures are presented, to demonstrate how the trigger responded well under the extreme pileup conditions. The performance of the ID Trigger algorithms in the first data from the even higher interaction multiplicity collisions from 2017 are presented, and illustrates how the ID tracking continues to enable the ATLAS physics program currently, and will continue to do so in the future.
</Content>
<field id="content">
The design and performance of the ATLAS Inner Detector (ID) trigger algorithms running online on the high level trigger (HLT) processor farm for 13 TeV LHC collision data with high pileup are discussed. The HLT ID tracking is a vital component in all physics signatures in the ATLAS Trigger for the precise selection of the rare or interesting events necessary for physics analysis without overwhelming the offline data storage in terms of both size and rate. To cope with the high expected interaction rates in the 13 TeV LHC collisions the ID trigger was redesigned during the 2013-15 long shutdown. The performance of the ID Trigger in the 2016 data from 13 TeV LHC collisions has been excellent and exceeded expectations as the interaction multiplicity increased throughout the year. The detailed efficiencies and resolutions of the trigger in a wide range of physics signatures are presented, to demonstrate how the trigger responded well under the extreme pileup conditions. The performance of the ID Trigger algorithms in the first data from the even higher interaction multiplicity collisions from 2017 are presented, and illustrates how the ID tracking continues to enable the ATLAS physics program currently, and will continue to do so in the future.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Jiri</FirstName>
<FamilyName>Masik</FamilyName>
<Email>jiri.masik@manchester.ac.uk</Email>
<Affiliation>on behalf of the ATLAS TDAQ speakers committee</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Jiri</FirstName>
<FamilyName>Masik</FamilyName>
<Email>jiri.masik@manchester.ac.uk</Email>
<Affiliation>on behalf of the ATLAS TDAQ speakers committee</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Triggering, Data Acquisition, Control Systems</Track>
</abstract>
<abstract>
<Id>163</Id>
<Title>The ATLAS Data Acquisition System in LHC Run 2</Title>
<Content>
The LHC has been providing proton-proton collisions with record intensity and energy since the start of Run 2 in 2015. In the ATLAS experiment the Data Acquisition is responsible for the transport and storage of the more complex event data at higher rates that the new collision environment implies. Data from events selected by the first level hardware trigger are subject to further filtration from software running on a commodity load balanced processing farm of some 2000 servers. During this time the data transferred from detector electronics across 1900 optical links to custom buffer hardware hosted across 100 commodity server PCs, and transferred across the system for processing by high bandwidth network at an average throughput of 30 GB/s. Accepted events are then transported to a data logging system for final packaging and transfer to permanent storage, with a final average output bandwidth of 1.5 GB/s. The whole system is actively monitored to maximise efficiency and minimise downtime. Due to the scale of the system and the challenging collision environment the ATLAS DAQ system is a prime example of the effective use of many modern technologies and standards in a high energy physics data taking environment, with state of the art networking, data transport and real time monitoring applications. This presentation will cover overall design of the system, focusing on the novel technology elements in use, before demonstrating its performance so far in LHC Run 2.
</Content>
<field id="content">
The LHC has been providing proton-proton collisions with record intensity and energy since the start of Run 2 in 2015. In the ATLAS experiment the Data Acquisition is responsible for the transport and storage of the more complex event data at higher rates that the new collision environment implies. Data from events selected by the first level hardware trigger are subject to further filtration from software running on a commodity load balanced processing farm of some 2000 servers. During this time the data transferred from detector electronics across 1900 optical links to custom buffer hardware hosted across 100 commodity server PCs, and transferred across the system for processing by high bandwidth network at an average throughput of 30 GB/s. Accepted events are then transported to a data logging system for final packaging and transfer to permanent storage, with a final average output bandwidth of 1.5 GB/s. The whole system is actively monitored to maximise efficiency and minimise downtime. Due to the scale of the system and the challenging collision environment the ATLAS DAQ system is a prime example of the effective use of many modern technologies and standards in a high energy physics data taking environment, with state of the art networking, data transport and real time monitoring applications. This presentation will cover overall design of the system, focusing on the novel technology elements in use, before demonstrating its performance so far in LHC Run 2.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Jiri</FirstName>
<FamilyName>Masik</FamilyName>
<Email>jiri.masik@manchester.ac.uk</Email>
<Affiliation>on behalf of the ATLAS TDAQ speakers committee</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Jiri</FirstName>
<FamilyName>Masik</FamilyName>
<Email>jiri.masik@manchester.ac.uk</Email>
<Affiliation>on behalf of the ATLAS TDAQ speakers committee</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Triggering, Data Acquisition, Control Systems</Track>
</abstract>
<abstract>
<Id>164</Id>
<Title>
ATLAS Trigger and Data Acquisition Upgrade Plans for High Luminosity LHC
</Title>
<Content>
By 2026 the High Luminosity LHC will be able to deliver to its experiments at CERN 14 TeV proton-proton collisions with an order of magnitude higher instantaneous luminosity than the original design, at the expected value of 7.5 × 10^34 cm−2s−1. The ATLAS experiment is planning a series of upgrades to prepare for this new and challenging environment, which will produce much higher data rates and larger and more complex events than the current experiment was designed to handle. A broad physics program prepared for this fourth LHC run is driving the full upgrade plan, which will involve major changes in the detectors as well as in the trigger and data acquisition system. The detector upgrades themselves present new requirements but also new opportunities for radical changes in the trigger and data acquisition architecture. This presentation will describe the baseline architectures established for different luminosity scenarios, while also detailing ongoing studies into new system components and their interconnections. The overall challenge here is to meet low latency and high data throughput requirements within the limits given by technological evolution. One key aspect driving the design is the need for rate reduction, which will be based on easily identifiable high momentum electrons and muons. However, hadronic final states are also becoming important for investigations of the full phase space of the Standard Model and beyond. This is motivating the inclusion of both higher resolution first-level trigger information and a new hardware tracking system. The high throughput data acquisition system and the commodity hardware and software-based data handling and event filtering are also key ingredients to ensure maximum efficiency in recording data and processing. A discussion on the physics motivations and the expected performance based on simulation studies will be presented, together with the open issues and plans.
</Content>
<field id="content">
By 2026 the High Luminosity LHC will be able to deliver to its experiments at CERN 14 TeV proton-proton collisions with an order of magnitude higher instantaneous luminosity than the original design, at the expected value of 7.5 × 10^34 cm−2s−1. The ATLAS experiment is planning a series of upgrades to prepare for this new and challenging environment, which will produce much higher data rates and larger and more complex events than the current experiment was designed to handle. A broad physics program prepared for this fourth LHC run is driving the full upgrade plan, which will involve major changes in the detectors as well as in the trigger and data acquisition system. The detector upgrades themselves present new requirements but also new opportunities for radical changes in the trigger and data acquisition architecture. This presentation will describe the baseline architectures established for different luminosity scenarios, while also detailing ongoing studies into new system components and their interconnections. The overall challenge here is to meet low latency and high data throughput requirements within the limits given by technological evolution. One key aspect driving the design is the need for rate reduction, which will be based on easily identifiable high momentum electrons and muons. However, hadronic final states are also becoming important for investigations of the full phase space of the Standard Model and beyond. This is motivating the inclusion of both higher resolution first-level trigger information and a new hardware tracking system. The high throughput data acquisition system and the commodity hardware and software-based data handling and event filtering are also key ingredients to ensure maximum efficiency in recording data and processing. A discussion on the physics motivations and the expected performance based on simulation studies will be presented, together with the open issues and plans.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Jiri</FirstName>
<FamilyName>Masik</FamilyName>
<Email>jiri.masik@manchester.ac.uk</Email>
<Affiliation>on behalf of the ATLAS TDAQ speakers committee</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Jiri</FirstName>
<FamilyName>Masik</FamilyName>
<Email>jiri.masik@manchester.ac.uk</Email>
<Affiliation>on behalf of the ATLAS TDAQ speakers committee</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Triggering, Data Acquisition, Control Systems</Track>
</abstract>
<abstract>
<Id>165</Id>
<Title>
Videoconference support for large multi-sectional meetings at VBLHEP
</Title>
<Content>
This report talks about the use of modern technologies that allow the conference to be taken out of the audience and make it international. The report is based on the completed task of organizing videoconference support for large multi-sectional meetings at VBLHEP. The software for individual communication on the Internet is actively developing, but the issues of organization of collective videoconferencing are still open. Not every seminar or conference is broadcast on the Internet or has the possibility of two-way communication. In this regard, the report highlights the most urgent tasks of organizing a videoconference of various levels. Questions on the use of software and hardware, as well as the optimal choice of equipment and technologies are considered. Detailed description of the technical solutions that were used for VBLHEP.
</Content>
<field id="content">
This report talks about the use of modern technologies that allow the conference to be taken out of the audience and make it international. The report is based on the completed task of organizing videoconference support for large multi-sectional meetings at VBLHEP. The software for individual communication on the Internet is actively developing, but the issues of organization of collective videoconferencing are still open. Not every seminar or conference is broadcast on the Internet or has the possibility of two-way communication. In this regard, the report highlights the most urgent tasks of organizing a videoconference of various levels. Questions on the use of software and hardware, as well as the optimal choice of equipment and technologies are considered. Detailed description of the technical solutions that were used for VBLHEP.
</field>
<field id="summary">
This report tells about the experience accumulated in the solution of urgent problems on the organization of videoconference support for large international and multi-sectional meetings on the example of the work done at VBLHEP.
</field>
<PrimaryAuthor>
<FirstName>Ivan</FirstName>
<FamilyName>Slepov</FamilyName>
<Email>slepov@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Iouri</FirstName>
<FamilyName>Potrebenikov</FamilyName>
<Email>iouri.potrebenikov@cern.ch</Email>
<Affiliation>сo-author</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Boris</FirstName>
<FamilyName>Shchinov</FamilyName>
<Email>sbg@jinr.ru</Email>
<Affiliation>co-author</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Oleg</FirstName>
<FamilyName>Fedoseev</FamilyName>
<Email>fosssss@list.ru</Email>
<Affiliation>co-author</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Andrey</FirstName>
<FamilyName>Starikov</FamilyName>
<Email>astarikov@jinr.ru</Email>
<Affiliation>co-author</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Ivan</FirstName>
<FamilyName>Slepov</FamilyName>
<Email>slepov@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Innovative IT Education</Track>
<Track>
Advanced Technologies for the High-Intensity Domains of Science and Business Applications
</Track>
</abstract>
<abstract>
<Id>166</Id>
<Title>
High performance computing system in the framework of the Higgs boson studies
</Title>
<Content>
The Higgs boson physics is one of the most important and promising fields of study in the modern high energy physics. It is important to notice, that GRID computing resources become strictly limited due to increasing amount of statistics, required for physics analyses and unprecedented LHC performance. One of the possibilities to address the shortfall of computing resources is the usage of computer institutes' clusters, commercial computing resources and supercomputers. To perform precision measurements of the Higgs boson properties in these realities, it is also highly required to have effective instruments to simulate kinematic distributions of signal events. In this talk we give a brief description of the modern distribution reconstruction method called Morphing and perform few efficiency tests to demonstrate its potential. These studies have been performed on the WLCG and Kurchatov Institute’s Data Processing Center, including Tier-1 GRID site and supercomputer as well. We also analyze the CPU efficiency during these studies for different computing facilities. Reviewed approach demonstrates high efficiency and stability of the Kurchatov Institute's Data Processing Center in the field of the Higgs boson physics studies.
</Content>
<field id="content">
The Higgs boson physics is one of the most important and promising fields of study in the modern high energy physics. It is important to notice, that GRID computing resources become strictly limited due to increasing amount of statistics, required for physics analyses and unprecedented LHC performance. One of the possibilities to address the shortfall of computing resources is the usage of computer institutes' clusters, commercial computing resources and supercomputers. To perform precision measurements of the Higgs boson properties in these realities, it is also highly required to have effective instruments to simulate kinematic distributions of signal events. In this talk we give a brief description of the modern distribution reconstruction method called Morphing and perform few efficiency tests to demonstrate its potential. These studies have been performed on the WLCG and Kurchatov Institute’s Data Processing Center, including Tier-1 GRID site and supercomputer as well. We also analyze the CPU efficiency during these studies for different computing facilities. Reviewed approach demonstrates high efficiency and stability of the Kurchatov Institute's Data Processing Center in the field of the Higgs boson physics studies.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Nikita</FirstName>
<FamilyName>Belyaev</FamilyName>
<Email>nbelyaev@cern.ch</Email>
<Affiliation>NRC "Kurchatov Institute"</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Dimitrii</FirstName>
<FamilyName>Krasnopevtsev</FamilyName>
<Email>dimitriy.krasnopevtsev@cern.ch</Email>
<Affiliation>National Research Nuclear University MEPhI (RU)</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>vasily</FirstName>
<FamilyName>velikhov</FamilyName>
<Email>velikhovve@kiae.ru</Email>
<Affiliation>NRC "Kurchatov Institute"</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Eygene</FirstName>
<FamilyName>Ryabinkin</FamilyName>
<Email>rea@grid.kiae.ru</Email>
<Affiliation>NRC "Kurchatov Institute"</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Alexei</FirstName>
<FamilyName>Klimentov</FamilyName>
<Email>alexei.klimentov@cern.ch</Email>
<Affiliation>Brookhaven National Lab</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Rostislav</FirstName>
<FamilyName>Konoplich</FamilyName>
<Email>rk60@nyu.edu</Email>
<Affiliation>NYU</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Kirill</FirstName>
<FamilyName>Prokofiev</FamilyName>
<Email>kprok@cern.ch</Email>
<Affiliation>HKUST</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Nikita</FirstName>
<FamilyName>Belyaev</FamilyName>
<Email>nbelyaev@cern.ch</Email>
<Affiliation>NRC "Kurchatov Institute"</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Distributed Computing. GRID &amp; Cloud Computing</Track>
</abstract>
<abstract>
<Id>167</Id>
<Title>INDIGO-DataCloud: Quality of Service in storage</Title>
<Content>
When preparing the Data Management Plan for larger scientific endeavours, PI’s have to balance between the most appropriate qualities of storage space along the line of the planned data lifecycle, it’s price and the available funding. Storage properties can be the media type, implicitly determining access latency and durability of stored data, the number and locality of replicas, as well as available access protocols or authentication mechanisms. Negotiations between the scientific community and the responsible infrastructures generally happen upfront, where the amount of storage space, media types, like: disk, tape and SSD and the foreseeable data life-cycles are negotiated. With the introduction of cloud management platforms, both in computing and storage, resources can be brokered to achieve the best price per unit of a given quality. However, in order to allow the platform orchestrators to programatically negotiate the most appropriate resources, a standard vocabulary for different properties of resources and a commonly agreed protocol to communicate those, has to be available. In order to agree on a basic vocabulary for storage space properties, the storage infrastructure group in INDIGO-DataCloud together with INDIGO-associated and external scientific groups, created a working group under the umbrella of the “Research Data Alliance (RDA)”. As communication protocol, to query and negotiate storage qualities, the “Cloud Data Management Interface (CDMI)” has been selected. Necessary extensions to CDMI are defined in regular meetings between INDIGO and the “Storage Network Industry Association (SNIA)”. Furthermore, INDIGO is contributing to the SNIA CDMI reference implementation as the basis for interfacing the various storage systems in INDIGO to the agreed protocol and to provide an official OpenSource skeleton for systems not being maintained by INDIGO partners. In a first step, INDIGO will equip its supported storage systems, like dCache, StoRM, IBM GPFS and HPSS and possibly public cloud systems, with the developed interface to enable the INDIGO platform layer to programatically auto-detect the available storage properties and select the most appropriate endpoints based on its own policies. In a second step INDIGO will provide means to change the quality of storage, mainly to support data life cycle, but as well to make data available for low latency media for demanding HPC application before the requesting jobs are launched, which maps to the ‘bring online’ command in current HEP frameworks. Our presentation will elaborate on the planned common agreements between the involved scientific communities and the supporting infrastructures, the available software stack, the integration into the general INDIGO framework. Furthermore we’ll be able to demonstrate a first prototype of an example Web Service, providing a collective view into the storage space offerings of INDIGO partners across Europe, together with their different service qualities, entire based on the developed CDMI protocol extensions.
</Content>
<field id="content">
When preparing the Data Management Plan for larger scientific endeavours, PI’s have to balance between the most appropriate qualities of storage space along the line of the planned data lifecycle, it’s price and the available funding. Storage properties can be the media type, implicitly determining access latency and durability of stored data, the number and locality of replicas, as well as available access protocols or authentication mechanisms. Negotiations between the scientific community and the responsible infrastructures generally happen upfront, where the amount of storage space, media types, like: disk, tape and SSD and the foreseeable data life-cycles are negotiated. With the introduction of cloud management platforms, both in computing and storage, resources can be brokered to achieve the best price per unit of a given quality. However, in order to allow the platform orchestrators to programatically negotiate the most appropriate resources, a standard vocabulary for different properties of resources and a commonly agreed protocol to communicate those, has to be available. In order to agree on a basic vocabulary for storage space properties, the storage infrastructure group in INDIGO-DataCloud together with INDIGO-associated and external scientific groups, created a working group under the umbrella of the “Research Data Alliance (RDA)”. As communication protocol, to query and negotiate storage qualities, the “Cloud Data Management Interface (CDMI)” has been selected. Necessary extensions to CDMI are defined in regular meetings between INDIGO and the “Storage Network Industry Association (SNIA)”. Furthermore, INDIGO is contributing to the SNIA CDMI reference implementation as the basis for interfacing the various storage systems in INDIGO to the agreed protocol and to provide an official OpenSource skeleton for systems not being maintained by INDIGO partners. In a first step, INDIGO will equip its supported storage systems, like dCache, StoRM, IBM GPFS and HPSS and possibly public cloud systems, with the developed interface to enable the INDIGO platform layer to programatically auto-detect the available storage properties and select the most appropriate endpoints based on its own policies. In a second step INDIGO will provide means to change the quality of storage, mainly to support data life cycle, but as well to make data available for low latency media for demanding HPC application before the requesting jobs are launched, which maps to the ‘bring online’ command in current HEP frameworks. Our presentation will elaborate on the planned common agreements between the involved scientific communities and the supporting infrastructures, the available software stack, the integration into the general INDIGO framework. Furthermore we’ll be able to demonstrate a first prototype of an example Web Service, providing a collective view into the storage space offerings of INDIGO partners across Europe, together with their different service qualities, entire based on the developed CDMI protocol extensions.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Patrick</FirstName>
<FamilyName>Fuhrmann</FamilyName>
<Email>patrick.fuhrmann@desy.de</Email>
<Affiliation>DESY</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Paul</FirstName>
<FamilyName>Millar</FamilyName>
<Email>paul.millar@desy.de</Email>
<Affiliation>DESY</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Marcus</FirstName>
<FamilyName>Hardt</FamilyName>
<Email>hardt@kit.edu</Email>
<Affiliation>KIT</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Benjamin</FirstName>
<FamilyName>Ertl</FamilyName>
<Email>benjamin.ertl@kit.edu</Email>
<Affiliation>KIT</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Maciej</FirstName>
<FamilyName>Brzeźniak</FamilyName>
<Email>maciekb@man.poznan.pl</Email>
<Affiliation>poznan</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Patrick</FirstName>
<FamilyName>Fuhrmann</FamilyName>
<Email>patrick.fuhrmann@desy.de</Email>
<Affiliation>DESY</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Distributed Computing. GRID &amp; Cloud Computing</Track>
</abstract>
<abstract>
<Id>168</Id>
<Title>Event reconstruction chain for the BM@N experiment</Title>
<Content>
The BM@N experiment is the first stage experiment on the NICA accelerator complex. Software for simulations, reconstruction and analysis of particle physics data is an essential part of any high-energy physics experiment. In current talk a chain for experimental data reconstruction procedure are described. The chain contains experimental RAW-data digitization, spatial hits reconstrucion and tracks finding procedure.
</Content>
<field id="content">
The BM@N experiment is the first stage experiment on the NICA accelerator complex. Software for simulations, reconstruction and analysis of particle physics data is an essential part of any high-energy physics experiment. In current talk a chain for experimental data reconstruction procedure are described. The chain contains experimental RAW-data digitization, spatial hits reconstrucion and tracks finding procedure.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>S</FirstName>
<FamilyName>Merts</FamilyName>
<Email>sergey.merts@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>S</FirstName>
<FamilyName>Merts</FamilyName>
<Email>sergey.merts@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>
Computing for Large Scale Facilities (LHC, FAIR, NICA, SKA, PIC, XFEL, ELI, etc.)
</Track>
</abstract>
<abstract>
<Id>169</Id>
<Title>
Metadata curation and integration in High Energy and Nuclear Physics
</Title>
<Content>
Modern High Energy and Nuclear Physics experiments generate vast volumes of scientific data and metadata, describing scientific goals, the data provenance, conditions of the research environment, and other experiment-specific information. Data Knowledge Base (DKB) R&amp;D project has been initially started in 2016 as a joint project of National Research Center “Kurchatov Institute” and Tomsk Polytechnic University. And later the interest from the ATLAS experiment at LHC guided it to the new area of studies. Within the project we studied metadata sources in ATLAS. There are many sources of metadata, such as physics topics metadata, papers and conference notes, supporting documents, Twiki pages, google documents and spreadsheets, data sample catalogs, conditions and production analysis system databases. It has been noticed that information between sources is loosely coupled. Therefore, to provide a holistic view on physics topics, including integrated representation of all ATLAS documents and corresponding data samples, scientists need to obtain cross relations among metadata by themselves. DKB is designed to provide metadata integration and is considered to look for cross references among the metadata from various data sources. For the end user DKB frontend will be implemented as a graphical user interface, providing convenient integrated metadata representation, navigation, and efficient search - upwards to common metadata (production campaigns, projects, physics groups) and downwards from the specific, fine-grained metadata objects (detector geometry version, software release, conditions tags). Currently, the data scheme of ATLAS integrated metadata is organized as an ontological model. The backend of the DKB is the OpenLink Virtuoso RDF storage. It is populated with the information from ATLAS publications, supporting documents and underlying data samples. Metadata from unstructured texts were extracted by the PDFAnalyzer utility, developed by the research team. The integration dataflow execution is automated by Apache Kafka Streams. We observed that Twiki pages are very popular in physics community and they contain the metadata, corresponding to physics topics and production campaigns in semi-structured form. It is natural to expand the DKB functionality by adding the analysis of the Twiki pages. This will allow to have more complete and accurate integrational data model. Implementation of the DKB is closely related to the data samples curation and discovery. To choose the most suitable method, providing a performant look up for data samples by the various combinations of parameters, we should evaluate different technologies, such as graph databases (Neo4j, OrientDB), ElasticSearch, Virtuoso, Oracle JSONs search. In our report we will summarize the current state of our project, technology evaluation results and the recent prototype of the DKB architecture.
</Content>
<field id="content">
Modern High Energy and Nuclear Physics experiments generate vast volumes of scientific data and metadata, describing scientific goals, the data provenance, conditions of the research environment, and other experiment-specific information. Data Knowledge Base (DKB) R&amp;D project has been initially started in 2016 as a joint project of National Research Center “Kurchatov Institute” and Tomsk Polytechnic University. And later the interest from the ATLAS experiment at LHC guided it to the new area of studies. Within the project we studied metadata sources in ATLAS. There are many sources of metadata, such as physics topics metadata, papers and conference notes, supporting documents, Twiki pages, google documents and spreadsheets, data sample catalogs, conditions and production analysis system databases. It has been noticed that information between sources is loosely coupled. Therefore, to provide a holistic view on physics topics, including integrated representation of all ATLAS documents and corresponding data samples, scientists need to obtain cross relations among metadata by themselves. DKB is designed to provide metadata integration and is considered to look for cross references among the metadata from various data sources. For the end user DKB frontend will be implemented as a graphical user interface, providing convenient integrated metadata representation, navigation, and efficient search - upwards to common metadata (production campaigns, projects, physics groups) and downwards from the specific, fine-grained metadata objects (detector geometry version, software release, conditions tags). Currently, the data scheme of ATLAS integrated metadata is organized as an ontological model. The backend of the DKB is the OpenLink Virtuoso RDF storage. It is populated with the information from ATLAS publications, supporting documents and underlying data samples. Metadata from unstructured texts were extracted by the PDFAnalyzer utility, developed by the research team. The integration dataflow execution is automated by Apache Kafka Streams. We observed that Twiki pages are very popular in physics community and they contain the metadata, corresponding to physics topics and production campaigns in semi-structured form. It is natural to expand the DKB functionality by adding the analysis of the Twiki pages. This will allow to have more complete and accurate integrational data model. Implementation of the DKB is closely related to the data samples curation and discovery. To choose the most suitable method, providing a performant look up for data samples by the various combinations of parameters, we should evaluate different technologies, such as graph databases (Neo4j, OrientDB), ElasticSearch, Virtuoso, Oracle JSONs search. In our report we will summarize the current state of our project, technology evaluation results and the recent prototype of the DKB architecture.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Grigorieva</FirstName>
<FamilyName>Maria</FamilyName>
<Email>maria.grigorieva@cern.ch</Email>
<Affiliation>NRC KI</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Marina</FirstName>
<FamilyName>Golosova</FamilyName>
<Email>golosova.marina@gmail.com</Email>
<Affiliation>National Research Center "Kurchatov Institute"</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Alexei</FirstName>
<FamilyName>Klimentov</FamilyName>
<Email>alexei.klimentov@cern.ch</Email>
<Affiliation>Brookhaven National Lab</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Maksim</FirstName>
<FamilyName>Gubin</FamilyName>
<Email>gubin.m.u@gmail.com</Email>
<Affiliation>Tomsk Polytechnic University</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Vasiliy</FirstName>
<FamilyName>Aulov</FamilyName>
<Email>vasiliyaulov@gmail.com</Email>
<Affiliation>NRC Kurchatov Institute</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Torre</FirstName>
<FamilyName>Wenaus</FamilyName>
<Email>wenaus@gmail.com</Email>
<Affiliation>Brookhaven National Laboratory</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Grigorieva</FirstName>
<FamilyName>Maria</FamilyName>
<Email>maria.grigorieva@cern.ch</Email>
<Affiliation>NRC KI</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>
Non-relational Databases and Heterogeneous Repositories
</Track>
</abstract>
<abstract>
<Id>170</Id>
<Title>
Simulation of the MPI calculations running in the cloud environment
</Title>
<Content>
The cloud-based parallel computations running in the Laboratory of Information Technologies of the Joint Institute for Nuclear Research (JINR) would presumably allow to improve the efficiency of numerical calculations and to speed up acquisition of new physically meaningful results due to more efficient use of computing resources. To optimize a parallel computation scheme, especially if it is to be realized on some cloud mean, it is necessary to explore it for different combinations of setup parameters (number of CPU cores, amount of RAM per core, level of paralleling, network capacity, etc). The most rational way to perform this study is simulating as parallel scheme, as its cloud environment. For the better understanding and concreteness the simulation is done on the quite actual example of parallel calculations based on the MPI technology for long Josephson junctions (LJJ) accomplished in recent years at the JINR computing center. The simulation results of the LJJ calculations on the cloud infrastructure are presented.
</Content>
<field id="content">
The cloud-based parallel computations running in the Laboratory of Information Technologies of the Joint Institute for Nuclear Research (JINR) would presumably allow to improve the efficiency of numerical calculations and to speed up acquisition of new physically meaningful results due to more efficient use of computing resources. To optimize a parallel computation scheme, especially if it is to be realized on some cloud mean, it is necessary to explore it for different combinations of setup parameters (number of CPU cores, amount of RAM per core, level of paralleling, network capacity, etc). The most rational way to perform this study is simulating as parallel scheme, as its cloud environment. For the better understanding and concreteness the simulation is done on the quite actual example of parallel calculations based on the MPI technology for long Josephson junctions (LJJ) accomplished in recent years at the JINR computing center. The simulation results of the LJJ calculations on the cloud infrastructure are presented.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Gennady</FirstName>
<FamilyName>Ososkov</FamilyName>
<Email>ososkov@jinr.ru</Email>
<Affiliation>Joint Institute for Nuclear Research</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Nikolay</FirstName>
<FamilyName>Kutovskiy</FamilyName>
<Email>kut@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Vladimir</FirstName>
<FamilyName>Trofimov</FamilyName>
<Email>tvv@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Andrey</FirstName>
<FamilyName>Nechaevskiy</FamilyName>
<Email>nechav@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Дарья</FirstName>
<FamilyName>Пряхина</FamilyName>
<Email>pryahinad@jinr.ru</Email>
<Affiliation>ЛИТ</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Дарья</FirstName>
<FamilyName>Пряхина</FamilyName>
<Email>pryahinad@jinr.ru</Email>
<Affiliation>ЛИТ</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>
Computations with Hybrid Systems (CPU, GPU, coprocessors)
</Track>
</abstract>
<abstract>
<Id>171</Id>
<Title>Optimization of the JINR Cloud’s Efficiency</Title>
<Content>
Clouds built on Infrastructure-as-a-Service (IaaS) model (such as JINR Cloud) gave us new universal and flexible tools and ways to use computing resources. These new tools may help scientists speed up their research work but at the cost of a significant drop (compared to the more traditional systems in science such as grid) of the overall utilization efficiency of an underlying infrastructure. The talk covers Smart Cloud Scheduler project aimed at optimizing the performance of the IaaS-based clouds, including its architecture, development status and plans. The project includes development of a software framework that would allow one to implement custom schemes of dynamic reallocation and consolidation of virtual machines. The resulting system will give a possibility to dynamically rebalance the cloud workload in an automated fashion in order to increase the overall infrastructure utilization efficiency.
</Content>
<field id="content">
Clouds built on Infrastructure-as-a-Service (IaaS) model (such as JINR Cloud) gave us new universal and flexible tools and ways to use computing resources. These new tools may help scientists speed up their research work but at the cost of a significant drop (compared to the more traditional systems in science such as grid) of the overall utilization efficiency of an underlying infrastructure. The talk covers Smart Cloud Scheduler project aimed at optimizing the performance of the IaaS-based clouds, including its architecture, development status and plans. The project includes development of a software framework that would allow one to implement custom schemes of dynamic reallocation and consolidation of virtual machines. The resulting system will give a possibility to dynamically rebalance the cloud workload in an automated fashion in order to increase the overall infrastructure utilization efficiency.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Nikita</FirstName>
<FamilyName>Balashov</FamilyName>
<Email>balashov.nikita@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Alexandr</FirstName>
<FamilyName>Baranov</FamilyName>
<Email>baranov@jinr.ru</Email>
<Affiliation>(JINR)</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Ivan</FirstName>
<FamilyName>Kadochnikov</FamilyName>
<Email>kadivas@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Igor</FirstName>
<FamilyName>Pelevanyuk</FamilyName>
<Email>gavelock@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Sergey</FirstName>
<FamilyName>Belov</FamilyName>
<Email>belov@jinr.ru</Email>
<Affiliation>Joint Institute for Nuclear Research</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Vladimir</FirstName>
<FamilyName>Korenkov</FamilyName>
<Email>korenkov@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Andrey</FirstName>
<FamilyName>Nechaevskiy</FamilyName>
<Email>nechav@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Nikolay</FirstName>
<FamilyName>Kutovskiy</FamilyName>
<Email>kut@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Nikita</FirstName>
<FamilyName>Balashov</FamilyName>
<Email>balashov.nikita@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Distributed Computing. GRID &amp; Cloud Computing</Track>
</abstract>
<abstract>
<Id>172</Id>
<Title>
Development of the autocalibration system for the DGFRS spectrometer based on the double-sided silicon strip detectors
</Title>
<Content>
The detection system of the Dubna gas-filled recoil separator (DGFRS) aimed at the studying of the SHE nuclei and their decay properties has been modernized during last few years. The new set of multistrips double-sided silicon detectors (DSSD) in focal plane of DGFRS is applied now instead of the old array of 12-strips position-sensitive Si detectors. The total amount of measuring spectroscopic channels of the registering system has increased also up to 224 channels. It leads to more precise measuring of the energy and coordinate of the implanted nuclei of the SHE into the focal detectors and of their decay products. It is important to test the registering system and perform energy calibration before carrying out of such unique experiments on the synthesis of new nuclei from the “Island of stability”. This work is devoted to describe the designed method and produced specific digital module which allows to perform energy calibration for all 224 individual spectroscopic channels independently. This device provides automatic bypassing of the all individual channels one after another, and thus imitating charge particles incoming to the detectors. Energy of the imitating signal can be chosen from the range of 1 MeV up to 250 MeV with good amplitude linearity and stability.
</Content>
<field id="content">
The detection system of the Dubna gas-filled recoil separator (DGFRS) aimed at the studying of the SHE nuclei and their decay properties has been modernized during last few years. The new set of multistrips double-sided silicon detectors (DSSD) in focal plane of DGFRS is applied now instead of the old array of 12-strips position-sensitive Si detectors. The total amount of measuring spectroscopic channels of the registering system has increased also up to 224 channels. It leads to more precise measuring of the energy and coordinate of the implanted nuclei of the SHE into the focal detectors and of their decay products. It is important to test the registering system and perform energy calibration before carrying out of such unique experiments on the synthesis of new nuclei from the “Island of stability”. This work is devoted to describe the designed method and produced specific digital module which allows to perform energy calibration for all 224 individual spectroscopic channels independently. This device provides automatic bypassing of the all individual channels one after another, and thus imitating charge particles incoming to the detectors. Energy of the imitating signal can be chosen from the range of 1 MeV up to 250 MeV with good amplitude linearity and stability.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Alexey</FirstName>
<FamilyName>Voinov</FamilyName>
<Email>voinov_2000@mail.ru</Email>
<Affiliation>FLNR JINR</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Vladimir</FirstName>
<FamilyName>Subbotin</FamilyName>
<Email>subbotin@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Alla</FirstName>
<FamilyName>Zubareva</FamilyName>
<Email>amzubareva@mail.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Leo</FirstName>
<FamilyName>Schlattauer</FamilyName>
<Email>leospv@gmail.com</Email>
<Affiliation>Palacky University Olomouc, Czech Republic</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Alexey</FirstName>
<FamilyName>Voinov</FamilyName>
<Email>voinov_2000@mail.ru</Email>
<Affiliation>FLNR JINR</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Detector &amp; Nuclear Electronics</Track>
<Track>Triggering, Data Acquisition, Control Systems</Track>
</abstract>
<abstract>
<Id>173</Id>
<Title>
JINR cloud: changes in architecture, getting distributed, smarter and more powerful
</Title>
<Content>
New JINR cloud hardware allows to make changes in its architecture and shift from running virtual machines' (VMs) and containers' (CTs) disks from local HDDs to software-defined storage. That speeds up deployment of VMs and CTs, live migration between cloud nodes as well as increases reliability and availability of virtual instances. Ongoing work on smart cloud scheduler will help to sufficiently increase efficiency of hardware resources utilization. To join resources for solving common tasks as well as to distribute peak loads across capacities provided by partner organizations from JINR Member states the JINR cloud became a core which all these resources are integrated with.
</Content>
<field id="content">
New JINR cloud hardware allows to make changes in its architecture and shift from running virtual machines' (VMs) and containers' (CTs) disks from local HDDs to software-defined storage. That speeds up deployment of VMs and CTs, live migration between cloud nodes as well as increases reliability and availability of virtual instances. Ongoing work on smart cloud scheduler will help to sufficiently increase efficiency of hardware resources utilization. To join resources for solving common tasks as well as to distribute peak loads across capacities provided by partner organizations from JINR Member states the JINR cloud became a core which all these resources are integrated with.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Nikita</FirstName>
<FamilyName>Balashov</FamilyName>
<Email>balashov.nikita@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Alexandr</FirstName>
<FamilyName>Baranov</FamilyName>
<Email>baranov@jinr.ru</Email>
<Affiliation>(JINR)</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Nikolay</FirstName>
<FamilyName>Kutovskiy</FamilyName>
<Email>kut@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Yelena</FirstName>
<FamilyName>Mazhitova</FamilyName>
<Email>emazhitova@jinr.ru</Email>
<Affiliation>Joint Institute for Nuclear Research</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Roman</FirstName>
<FamilyName>Semenov</FamilyName>
<Email>roman@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Nikita</FirstName>
<FamilyName>Balashov</FamilyName>
<Email>balashov.nikita@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Distributed Computing. GRID &amp; Cloud Computing</Track>
</abstract>
<abstract>
<Id>174</Id>
<Title>
Charged particles reconstruction in high energy physics experiments
</Title>
<Content>
Tracking detector systems in modern experiments allow to decide complicated tasks of charged particles reconstruction. Algorithms of pattern recognition, track fitting and practical application of statistics in data analysis on the example of several experiments are presented.
</Content>
<field id="content">
Tracking detector systems in modern experiments allow to decide complicated tasks of charged particles reconstruction. Algorithms of pattern recognition, track fitting and practical application of statistics in data analysis on the example of several experiments are presented.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Vladimir</FirstName>
<FamilyName>Palichik</FamilyName>
<Email>vladimir.palchik@cern.ch</Email>
<Affiliation>JINR Dubna</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Vladimir</FirstName>
<FamilyName>Palichik</FamilyName>
<Email>vladimir.palchik@cern.ch</Email>
<Affiliation>JINR Dubna</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>
Computing for Large Scale Facilities (LHC, FAIR, NICA, SKA, PIC, XFEL, ELI, etc.)
</Track>
</abstract>
<abstract>
<Id>175</Id>
<Title>
Tango software development at JINR. Tango module for WebSocket connection.
</Title>
<Content>
The report describes Tango module for WebSocket connection. WebSocket is a computer communications protocol, providing full-duplex communication channels over a single TCP connection. This module allows to carry out both monitoring and management of tango devices. The module also has several modes of operation. Depending on the selected mode, you can control both one and any number of required tango devices. The exchange of messages between the client and the server is in json format.
</Content>
<field id="content">
The report describes Tango module for WebSocket connection. WebSocket is a computer communications protocol, providing full-duplex communication channels over a single TCP connection. This module allows to carry out both monitoring and management of tango devices. The module also has several modes of operation. Depending on the selected mode, you can control both one and any number of required tango devices. The exchange of messages between the client and the server is in json format.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Vladimir</FirstName>
<FamilyName>Elkin</FamilyName>
<Email>elkin@jinr.ru</Email>
<Affiliation>Gennadyevich</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Evgeny</FirstName>
<FamilyName>Gorbachev</FamilyName>
<Email>gorbe@sunse.jinr.ru</Email>
<Affiliation>VBLHEP JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Georgy</FirstName>
<FamilyName>Sedykh</FamilyName>
<Email>egor@dubna.tk</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Vladimir</FirstName>
<FamilyName>Elkin</FamilyName>
<Email>elkin@jinr.ru</Email>
<Affiliation>Gennadyevich</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Triggering, Data Acquisition, Control Systems</Track>
</abstract>
<abstract>
<Id>176</Id>
<Title>
Application of Semantic Integration Methods for Cross-agency Information Sharing
</Title>
<Content>
Information sharing has become the key enabler for cross-agency interaction. Heterogeneous environment is inherent for the public sector and requires the application of integration methods, which will guarantee the achievement of unambiguous meaningful interpretation of data. The paper gives a brief review of international approaches to the achievement of semantic interoperability: built on XML-based models implementation (USA, NIEM) and the use of semantic integration methods (EU, SEMIC). The authors compare three methods: classic interaction using web clients; the application of a unified data model that requires the adjustment of adapters and the implementation of a domain data model, common to all the participants of information exchange in distributed systems. The comparison was made on the example developed within the project of Plekhanov Russian University of Economics named the “Center of semantic integration”.
</Content>
<field id="content">
Information sharing has become the key enabler for cross-agency interaction. Heterogeneous environment is inherent for the public sector and requires the application of integration methods, which will guarantee the achievement of unambiguous meaningful interpretation of data. The paper gives a brief review of international approaches to the achievement of semantic interoperability: built on XML-based models implementation (USA, NIEM) and the use of semantic integration methods (EU, SEMIC). The authors compare three methods: classic interaction using web clients; the application of a unified data model that requires the adjustment of adapters and the implementation of a domain data model, common to all the participants of information exchange in distributed systems. The comparison was made on the example developed within the project of Plekhanov Russian University of Economics named the “Center of semantic integration”.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Yury</FirstName>
<FamilyName>Akatkin</FamilyName>
<Email>uakatkin@yandex.ru</Email>
<Affiliation>Plekhanov Russian University of Economics</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Elena</FirstName>
<FamilyName>Yasinovskaya</FamilyName>
<Email>elena@semanticpro.org</Email>
<Affiliation>Plekhanov Russian University of Economics</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Michail</FirstName>
<FamilyName>Bich</FamilyName>
<Email>misha@semanticpro.org</Email>
<Affiliation>Plekhanov Russian University of Economics</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Yury</FirstName>
<FamilyName>Akatkin</FamilyName>
<Email>uakatkin@yandex.ru</Email>
<Affiliation>Plekhanov Russian University of Economics</Affiliation>
</Speaker>
<Speaker>
<FirstName>Elena</FirstName>
<FamilyName>Yasinovskaya</FamilyName>
<Email>elena@semanticpro.org</Email>
<Affiliation>Plekhanov Russian University of Economics</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Triggering, Data Acquisition, Control Systems</Track>
<Track>
Advanced Technologies for the High-Intensity Domains of Science and Business Applications
</Track>
</abstract>
<abstract>
<Id>177</Id>
<Title>
The FabrIc for Frontier Experiments Project at Fermilab: Computing for Experiments
</Title>
<Content>
The FabrIc for Frontier Experiments (FIFE) project is a major initiative within the Fermilab Scientific Computing Division designed to steer the computing model for non-LHC experiments at Fermilab. The FIFE project enables close collaboration between experimenters and computing professionals to serve high-energy physics experiments of differing scope and physics area of study. The project also tracks and provides feedback on the development of common tools for job submission, identity management, software and data distribution, job monitoring, and databases for project tracking. The computing needs of the experiments under the FIFE umbrella continue to increase, and present a complex list of requirements to their service providers. To meet these requirements, recent advances in the FIFE toolset include a new identity management infrastructure, significantly upgraded job monitoring tools, and a workflow management system. We have also upgraded existing tools to access remote computing resources such as GPU clusters and sites outside the United States. We will present these recent advances, highlight the nature of collaboration between the diverse set of experimenters and service providers, and discuss the project's future directions.
</Content>
<field id="content">
The FabrIc for Frontier Experiments (FIFE) project is a major initiative within the Fermilab Scientific Computing Division designed to steer the computing model for non-LHC experiments at Fermilab. The FIFE project enables close collaboration between experimenters and computing professionals to serve high-energy physics experiments of differing scope and physics area of study. The project also tracks and provides feedback on the development of common tools for job submission, identity management, software and data distribution, job monitoring, and databases for project tracking. The computing needs of the experiments under the FIFE umbrella continue to increase, and present a complex list of requirements to their service providers. To meet these requirements, recent advances in the FIFE toolset include a new identity management infrastructure, significantly upgraded job monitoring tools, and a workflow management system. We have also upgraded existing tools to access remote computing resources such as GPU clusters and sites outside the United States. We will present these recent advances, highlight the nature of collaboration between the diverse set of experimenters and service providers, and discuss the project's future directions.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Kenneth</FirstName>
<FamilyName>Herner</FamilyName>
<Email>kherner@fnal.gov</Email>
<Affiliation>Fermi National Accelerator Laboratory</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Michael</FirstName>
<FamilyName>Kirby</FamilyName>
<Email>kirby@fnal.gov</Email>
<Affiliation>Fermi National Accelerator Laboratory</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Kenneth</FirstName>
<FamilyName>Herner</FamilyName>
<Email>kherner@fnal.gov</Email>
<Affiliation>Fermi National Accelerator Laboratory</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Distributed Computing. GRID &amp; Cloud Computing</Track>
<Track>
Computing for Large Scale Facilities (LHC, FAIR, NICA, SKA, PIC, XFEL, ELI, etc.)
</Track>
</abstract>
<abstract>
<Id>178</Id>
<Title>
Federated data storage system prototype for LHC experiments and data intensive science
</Title>
<Content>
Rapid increase of data volume from the experiments running at the Large Hadron Collider (LHC) prompted physics computing community to evaluate new data handling and processing solutions. Russian grid sites and universities’ clusters scattered over a large area aim at the task of uniting their resources for future productive work, at the same time giving an opportunity to support large physics collaborations. In our talk we will cover deployment and testing of federated data storage prototype for WLCG centers of different levels and university clusters within one Russian National Cloud. The prototype is based on computing resources located in Moscow, Dubna, Saint Petersburg, Gatchina and Geneva. This project intends to implement a federated distributed storage for scientific applications with access from Grid centers, university clusters, supercomputers, academic and commercial clouds. The efficiency and performance of the system are demonstrated using synthetic and experiment-specific tests including real data processing and analysis workflows from ATLAS and ALICE experiments. We will present topology and architecture of the designed system and show how it can be achieved using different software solutions such as EOS and dCache. We will also describe how sharing data on a widely distributed storage system can lead to a new computing model and reformations of classic computing style.
</Content>
<field id="content">
Rapid increase of data volume from the experiments running at the Large Hadron Collider (LHC) prompted physics computing community to evaluate new data handling and processing solutions. Russian grid sites and universities’ clusters scattered over a large area aim at the task of uniting their resources for future productive work, at the same time giving an opportunity to support large physics collaborations. In our talk we will cover deployment and testing of federated data storage prototype for WLCG centers of different levels and university clusters within one Russian National Cloud. The prototype is based on computing resources located in Moscow, Dubna, Saint Petersburg, Gatchina and Geneva. This project intends to implement a federated distributed storage for scientific applications with access from Grid centers, university clusters, supercomputers, academic and commercial clouds. The efficiency and performance of the system are demonstrated using synthetic and experiment-specific tests including real data processing and analysis workflows from ATLAS and ALICE experiments. We will present topology and architecture of the designed system and show how it can be achieved using different software solutions such as EOS and dCache. We will also describe how sharing data on a widely distributed storage system can lead to a new computing model and reformations of classic computing style.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Andrey</FirstName>
<FamilyName>Kiryanov</FamilyName>
<Email>globus@pnpi.nw.ru</Email>
<Affiliation>PNPI</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Andrey</FirstName>
<FamilyName>Zarochentsev</FamilyName>
<Email>andrey.zar@gmail.com</Email>
<Affiliation>SPbSU</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Alexei</FirstName>
<FamilyName>Klimentov</FamilyName>
<Email>alexei.klimentov@cern.ch</Email>
<Affiliation>Brookhaven National Lab</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Artem</FirstName>
<FamilyName>Petrosyan</FamilyName>
<Email>artem.petrosyan@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Andrey</FirstName>
<FamilyName>Kiryanov</FamilyName>
<Email>globus@pnpi.nw.ru</Email>
<Affiliation>PNPI</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Research Data Infrastructures</Track>
<Track>Distributed Computing. GRID &amp; Cloud Computing</Track>
<Track>
Computing for Large Scale Facilities (LHC, FAIR, NICA, SKA, PIC, XFEL, ELI, etc.)
</Track>
</abstract>
<abstract>
<Id>179</Id>
<Title>
Data management in heterogeneous metadata storage and access infrastructures
</Title>
<Content>
In modern times many large projects, sooner or later, have to face the problem of how to store, manage and access huge volumes of semi-structured and loosely connected data, namely project metadata -- information, required for monitoring and management of the project itself and its internal processes. The structure of the metadata evolves all the time to meet the needs of the monitoring tasks and user requirements. And as the structure and volume of the metadata grow, it becomes impractical to store everything in a single central storage -- with time such a storage becomes less flexible in structure, and query processing slows down. To provide structure flexibility and to keep metadata access time short enough for comfort interaction with monitoring systems, next step is to replace the single central storage with a number of task-specific storages -- one for active metadata, another for the archive, yet another to store aggregated information (as a cache storage), etc. In a broad sense the combination of these storages can be described as a single hybrid (or heterogeneous) metadata storage and access infrastructure. The main goal of this infrastructure is to provide information about the project and its internal processes in a human readable and searchable way. Among the possible components of this infrastructure can be text documents, wiki pages, databases, search interfaces to storage systems, etc. To keep all these components synchronized even in case of any -- software, hardware or network -- failure, there is a need of some supervising tool (or a set of tools), which is aware of the infrastructure and takes care of data consistency within it. The usual way is to create such a supervising tool individually for each case, meaning that each part of the infrastructure takes care of itself, synchronizing data only with the direct neighbors, namely the information sources for this part. And for each case one must solve same issues of reliability, throughput, scalability and fault tolerance. To avoid solving same issues individually for every new system operating with metadata, we started to design a unified way to develop and implement such a supervising tool. It would allow developers in every particular case implement only the case-specific modules, and rest the responsibility for common issues upon the common and ready-to-use tools. The first premise for this work appeared in 2014-2015, when we were working on the Metadata Hybrid Storage R&amp;D project for PanDA, the workflow management system of ATLAS experiment on LHC, in NRC “Kurchatov Institute”. In this report we will explain the motivation of the problem, describe the principal architecture designed to address it and tell about the prototype system, developed and implemented for ATLAS Data Knowledge Base, the joint R&amp;D project of NRC KI and Tomsk Polytechnic Institute, started in 2016. Also we will discuss our technology choice for the prototype, provide the performance and scalability test results and present our plans for the future.
</Content>
<field id="content">
In modern times many large projects, sooner or later, have to face the problem of how to store, manage and access huge volumes of semi-structured and loosely connected data, namely project metadata -- information, required for monitoring and management of the project itself and its internal processes. The structure of the metadata evolves all the time to meet the needs of the monitoring tasks and user requirements. And as the structure and volume of the metadata grow, it becomes impractical to store everything in a single central storage -- with time such a storage becomes less flexible in structure, and query processing slows down. To provide structure flexibility and to keep metadata access time short enough for comfort interaction with monitoring systems, next step is to replace the single central storage with a number of task-specific storages -- one for active metadata, another for the archive, yet another to store aggregated information (as a cache storage), etc. In a broad sense the combination of these storages can be described as a single hybrid (or heterogeneous) metadata storage and access infrastructure. The main goal of this infrastructure is to provide information about the project and its internal processes in a human readable and searchable way. Among the possible components of this infrastructure can be text documents, wiki pages, databases, search interfaces to storage systems, etc. To keep all these components synchronized even in case of any -- software, hardware or network -- failure, there is a need of some supervising tool (or a set of tools), which is aware of the infrastructure and takes care of data consistency within it. The usual way is to create such a supervising tool individually for each case, meaning that each part of the infrastructure takes care of itself, synchronizing data only with the direct neighbors, namely the information sources for this part. And for each case one must solve same issues of reliability, throughput, scalability and fault tolerance. To avoid solving same issues individually for every new system operating with metadata, we started to design a unified way to develop and implement such a supervising tool. It would allow developers in every particular case implement only the case-specific modules, and rest the responsibility for common issues upon the common and ready-to-use tools. The first premise for this work appeared in 2014-2015, when we were working on the Metadata Hybrid Storage R&amp;D project for PanDA, the workflow management system of ATLAS experiment on LHC, in NRC “Kurchatov Institute”. In this report we will explain the motivation of the problem, describe the principal architecture designed to address it and tell about the prototype system, developed and implemented for ATLAS Data Knowledge Base, the joint R&amp;D project of NRC KI and Tomsk Polytechnic Institute, started in 2016. Also we will discuss our technology choice for the prototype, provide the performance and scalability test results and present our plans for the future.
</field>
<field id="summary">
In modern times many large projects, sooner or later, have to face the problem of how to store, manage and access huge volumes of semi-structured and loosely connected data, namely project metadata. To provide structure flexibility and to keep metadata access time short a number of task-specific storages -- so-called heterogeneous metadata storage and access infrastructure -- appears. To avoid solving same issues of reliability, throughput, scalability and fault tolerance for every single subsystem in this infrastructure, we started to design a unified way to develop and implement a supervising tools for the subsystems. In the report we will present the first results and the prototype system, developed for ATLAS Data Knowledge Base.
</field>
<PrimaryAuthor>
<FirstName>Marina</FirstName>
<FamilyName>Golosova</FamilyName>
<Email>golosova.marina@gmail.com</Email>
<Affiliation>National Research Center "Kurchatov Institute"</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Eygene</FirstName>
<FamilyName>Ryabinkin</FamilyName>
<Email>rea@grid.kiae.ru</Email>
<Affiliation>NRC "Kurchatov Institute"</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Marina</FirstName>
<FamilyName>Golosova</FamilyName>
<Email>golosova.marina@gmail.com</Email>
<Affiliation>National Research Center "Kurchatov Institute"</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>
Non-relational Databases and Heterogeneous Repositories
</Track>
</abstract>
<abstract>
<Id>180</Id>
<Title>
Application of the HEP Computing Tools for Brain studies
</Title>
<Content>
The Production and Distributed Analysis (PanDA) system was designed to meet the requirements for a workload management system (WMS) capable to operate at the LHC data processing scale. It has been used in the ATLAS experiment since 2005 and is now being part of the BigPanDA project expanding into a meta application, providing transparency of the data processing and workflow management for High Energy Physics (HEP) and other data-intensive sciences. BigPanDA was one of the first WMS systems expanded beyond the Grid to support the High Performance Clusters, Supercomputers, Clouds and Leadership Computing Facilities (LFC). This could not fail to attract the attention to the system from other compute intensive sciences such as brain studies. In 2017, the pilot project was started between teams of the BigPanDA and the Blue Brain Project (BBP) of the Ecole Polytechnique Federal de Lausanne (EPFL) located in Lausanne, Switzerland. This proof of concept project is aimed at demonstrating the efficient application of the BigPanDA system to support the complex scientific workflow of the BBP which relies on using a mix of desktop, cluster and supercomputers to reconstruct and simulate accurate models of brain tissue. During the first phase of the collaboration we demonstrated the execution of the computational jobs on a variety of BBP distributed computing systems powered. The targeted systems for demonstration included: Intel x86-NVIDIA GPU based BBP clusters located in Geneva (47 TFlops) and Lugano (81 TFlops), BBP IBM BlueGene/Q supercomputer ( 0.78 PFLops and 65 TB of DRAM memory) located in Lugano, the Titan Supercomputer with peak theoretical performance 27 PFlops operated by the Oak Ridge Leadership Computing Facility (OLCF), and Cloud based resources such as Amazon Cloud. To hide execution complexity and simplify manual tasks by end-users, we developed a web interface to submit, control and monitor user tasks and seamlessly integrated it with the PanDA WMS system. The project demonstrating that the software tools and methods for processing large volumes of experimental data, which have been developed initially for experiments at the LHC accelerator, can be successfully applied to other scientific fields.
</Content>
<field id="content">
The Production and Distributed Analysis (PanDA) system was designed to meet the requirements for a workload management system (WMS) capable to operate at the LHC data processing scale. It has been used in the ATLAS experiment since 2005 and is now being part of the BigPanDA project expanding into a meta application, providing transparency of the data processing and workflow management for High Energy Physics (HEP) and other data-intensive sciences. BigPanDA was one of the first WMS systems expanded beyond the Grid to support the High Performance Clusters, Supercomputers, Clouds and Leadership Computing Facilities (LFC). This could not fail to attract the attention to the system from other compute intensive sciences such as brain studies. In 2017, the pilot project was started between teams of the BigPanDA and the Blue Brain Project (BBP) of the Ecole Polytechnique Federal de Lausanne (EPFL) located in Lausanne, Switzerland. This proof of concept project is aimed at demonstrating the efficient application of the BigPanDA system to support the complex scientific workflow of the BBP which relies on using a mix of desktop, cluster and supercomputers to reconstruct and simulate accurate models of brain tissue. During the first phase of the collaboration we demonstrated the execution of the computational jobs on a variety of BBP distributed computing systems powered. The targeted systems for demonstration included: Intel x86-NVIDIA GPU based BBP clusters located in Geneva (47 TFlops) and Lugano (81 TFlops), BBP IBM BlueGene/Q supercomputer ( 0.78 PFLops and 65 TB of DRAM memory) located in Lugano, the Titan Supercomputer with peak theoretical performance 27 PFlops operated by the Oak Ridge Leadership Computing Facility (OLCF), and Cloud based resources such as Amazon Cloud. To hide execution complexity and simplify manual tasks by end-users, we developed a web interface to submit, control and monitor user tasks and seamlessly integrated it with the PanDA WMS system. The project demonstrating that the software tools and methods for processing large volumes of experimental data, which have been developed initially for experiments at the LHC accelerator, can be successfully applied to other scientific fields.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Ruslan</FirstName>
<FamilyName>Mashinistov</FamilyName>
<Email>rmashinistov@gmail.com</Email>
<Affiliation>NRC KI</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Kaushik</FirstName>
<FamilyName>De</FamilyName>
<Email>kaushik@uta.edu</Email>
<Affiliation>UTA</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Alexandre</FirstName>
<FamilyName>Beche</FamilyName>
<Email>alexandre.beche@epfl.ch</Email>
<Affiliation>EPFL</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Fabien</FirstName>
<FamilyName>Delalondre</FamilyName>
<Email>fabien.delalondre@epfl.ch</Email>
<Affiliation>EPFL</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Felix</FirstName>
<FamilyName>Schuermann</FamilyName>
<Email>felix.schuermann@epfl.ch</Email>
<Affiliation>EPFL</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Ivan</FirstName>
<FamilyName>Tertychnyy</FamilyName>
<Email>ivantertychnyi@gmail.com</Email>
<Affiliation>NRC KI</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Alexei</FirstName>
<FamilyName>Klimentov</FamilyName>
<Email>alexei.klimentov@cern.ch</Email>
<Affiliation>Brookhaven National Lab</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Ruslan</FirstName>
<FamilyName>Mashinistov</FamilyName>
<Email>rmashinistov@gmail.com</Email>
<Affiliation>NRC KI</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Distributed Computing. GRID &amp; Cloud Computing</Track>
</abstract>
<abstract>
<Id>181</Id>
<Title>Signal synchronization system</Title>
<Content>
The paper deals with creation of the concept and the first prototype of the new signal synchronization system of the Nuclotron accelerator complex. The text describes a new scheme for the collection and distribution of signals needed to synchronize many devices of the accelerator complex. Much attention is given to problems of the development of electronic equipment. The following modules are considered in the text: Optics to i/o converter, pulse-forming block, interface submodules. It should be stressed that the signal synchronization system in association with white rabbit technology will be the basis for creating the global timing system of the NICA.
</Content>
<field id="content">
The paper deals with creation of the concept and the first prototype of the new signal synchronization system of the Nuclotron accelerator complex. The text describes a new scheme for the collection and distribution of signals needed to synchronize many devices of the accelerator complex. Much attention is given to problems of the development of electronic equipment. The following modules are considered in the text: Optics to i/o converter, pulse-forming block, interface submodules. It should be stressed that the signal synchronization system in association with white rabbit technology will be the basis for creating the global timing system of the NICA.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Ilya</FirstName>
<FamilyName>Shirikov</FamilyName>
<Email>shirikov@bk.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Ilya</FirstName>
<FamilyName>Shirikov</FamilyName>
<Email>shirikov@bk.ru</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Triggering, Data Acquisition, Control Systems</Track>
</abstract>
<abstract>
<Id>182</Id>
<Title>
The methodology of testing of distributed storages for Big Data.
</Title>
<Content>
During the work on federated storage prototype for data-intensive scientific projects, such as high-energy physics experiments, the authors have faced the challenge of a proper testing of various distributed storage systems, each of which has its own characteristics and properties. The goal was to find common testing methods and software applicable to all such systems and determine the major measurable quantities by which the these systems can be compared. The testing method and software shown in this paper was tried with EOS-based and dCache-based distributes storages using xrootd access protocol. The paper also shows the comparative test results and describes the problems encountered by the authors.
</Content>
<field id="content">
During the work on federated storage prototype for data-intensive scientific projects, such as high-energy physics experiments, the authors have faced the challenge of a proper testing of various distributed storage systems, each of which has its own characteristics and properties. The goal was to find common testing methods and software applicable to all such systems and determine the major measurable quantities by which the these systems can be compared. The testing method and software shown in this paper was tried with EOS-based and dCache-based distributes storages using xrootd access protocol. The paper also shows the comparative test results and describes the problems encountered by the authors.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Andrey</FirstName>
<FamilyName>Zarochentsev</FamilyName>
<Email>andrey.zar@gmail.com</Email>
<Affiliation>SPbSU</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Andrey</FirstName>
<FamilyName>Kiryanov</FamilyName>
<Email>globus@pnpi.nw.ru</Email>
<Affiliation>PNPI</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Andrey</FirstName>
<FamilyName>Zarochentsev</FamilyName>
<Email>andrey.zar@gmail.com</Email>
<Affiliation>SPbSU</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Research Data Infrastructures</Track>
</abstract>
<abstract>
<Id>184</Id>
<Title>
Approaches to building of Cloud base scientific computing infrastructure
</Title>
<Content>
In the paper presented results of works focused on building of heterogeneous Cloud base scientific computing infrastructure. Main purpose of infrastructure is to provide for researchers a possibility to access ”on demand” a wide range of different types of resources, that can be physically located in local, federated and GEANT offering clouds. These resources include pure and customized Virtual Machines with preinstalled and configured software, GRID and HPC facilities on the base of virtualization paradigm within integrated Cloud infrastructure. Considered creation of “Centre of Excellence” where researcher can start with parallel clusters’ systems study, development and debugging of initial versions of parallel applications for further scaling them to resources that are more powerful. The aim of proposed infrastructure is to provide to researchers access to multi-cloud platform with horizontal and vertical scaling, self-healing (the ability of a system to recover from failures) and with different SLA levels, depending on time of researcher experiments. To ensure operation of federated mechanism to access distributed computing resources were investigated approaches and finalized works to realize solutions that allow providing unified access to cloud infrastructures and be integrated in the Research &amp; Educational identity management federations operated within eduGAIN inter-federation authorization &amp; authentication mechanism (AAI) Perspectives of utilization of virtualization technologies for integration of Grid and HPC clusters in heterogeneous computer infrastructures that are offering effective computing resources and end-user interfaces are considered. Keywords: distributed computing technology, Cloud computing, High Performance Computing, computational clusters, Federated Cloud on-demand Services
</Content>
<field id="content">
In the paper presented results of works focused on building of heterogeneous Cloud base scientific computing infrastructure. Main purpose of infrastructure is to provide for researchers a possibility to access ”on demand” a wide range of different types of resources, that can be physically located in local, federated and GEANT offering clouds. These resources include pure and customized Virtual Machines with preinstalled and configured software, GRID and HPC facilities on the base of virtualization paradigm within integrated Cloud infrastructure. Considered creation of “Centre of Excellence” where researcher can start with parallel clusters’ systems study, development and debugging of initial versions of parallel applications for further scaling them to resources that are more powerful. The aim of proposed infrastructure is to provide to researchers access to multi-cloud platform with horizontal and vertical scaling, self-healing (the ability of a system to recover from failures) and with different SLA levels, depending on time of researcher experiments. To ensure operation of federated mechanism to access distributed computing resources were investigated approaches and finalized works to realize solutions that allow providing unified access to cloud infrastructures and be integrated in the Research &amp; Educational identity management federations operated within eduGAIN inter-federation authorization &amp; authentication mechanism (AAI) Perspectives of utilization of virtualization technologies for integration of Grid and HPC clusters in heterogeneous computer infrastructures that are offering effective computing resources and end-user interfaces are considered. Keywords: distributed computing technology, Cloud computing, High Performance Computing, computational clusters, Federated Cloud on-demand Services
</field>
<field id="summary">
Keywords: distributed computing technology, Cloud computing, High Performance Computing, computational clusters, Federated Cloud on-demand Services
</field>
<PrimaryAuthor>
<FirstName>Peter</FirstName>
<FamilyName>Bogatencov</FamilyName>
<Email>bogatencov@renam.md</Email>
<Affiliation>RENAM, Moldova</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Nichita</FirstName>
<FamilyName>Degteariov</FamilyName>
<Email>ndegteariov@renam.md</Email>
<Affiliation>RENAM</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Grigore</FirstName>
<FamilyName>Secrieru</FamilyName>
<Email>secrieru@renam.md</Email>
<Affiliation>Vasile</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Nicolai</FirstName>
<FamilyName>Iliuha</FamilyName>
<Email>nicolai.iliuha@renam.md</Email>
<Affiliation>RENAM</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Grigorii</FirstName>
<FamilyName>Horos</FamilyName>
<Email>grigorii.horos@math.md</Email>
<Affiliation>IMI ASM</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Peter</FirstName>
<FamilyName>Bogatencov</FamilyName>
<Email>bogatencov@renam.md</Email>
<Affiliation>RENAM, Moldova</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Distributed Computing. GRID &amp; Cloud Computing</Track>
</abstract>
<abstract>
<Id>185</Id>
<Title>NFV Cloud Infrastructure for Researchers</Title>
<Content>
Modern research cloud infrastructures purposed to help researcher to prepare virtual environment that satisfy various specific requirements. The focus could be set on a network topology and providing different network functions (NAT, Firewall, IDS, vSwitch etc.) in order to provide testbed for network research, or a network device testing. Another focus could be set on compute resources providing researcher the computational cluster, for example Hadoop-cluster. Regardless of purposes the researcher uses the cloud infrastructure we need the unified system that manages and orchestrates all types of cloud infrastructure resources. Network Function Virtualization (NFV) techniques separates network function logic from the hardware, which executes the first. There are several basic use cases for NFV. The network function (NF) lifecycle is stage process, that means that each NF passed through deployed stage, initialization stage, configuration stage, execution stage and undeploying stage. In our Demo the NF lifecycle managing and orchestration in DC will be demonstration. We would like to present a cloud platform architecture that adheres to the reference implementation ETSI NFV MANO model. As we will show the platform architecture and design successfully meet the requirements of researcher cloud VNFs. We call our platform Cloud Conductor or C2. The C2 platform provides the full VNF life-cycle support: initialization, configuration, execution and uninitialization. The C2 Platform provides virtual network services (VNS) to researchers by CPE virtualization. Virtual customer premise equipment (vCPE) is a way to deliver network services such as routing, firewall security and access to computational resources by using software rather than dedicated hardware devices. By virtualizing the CPE, the C2 Platform can simplify and accelerate service delivery, remotely configuring and managing devices and allowing researchers to order new services or adjust existing ones on demand. Especially will be discussed the CloudGW module solution, that is aimed to provide an entry border point for traffic from researcher to a virtual cloud infrastructure for further processing, depending on required virtual infrastructure (DBaaS, Hadoop, classic IaaS and etc.). The main value of a general service-oriented platform is the diversity of the predefined VNFs that can be deployed on this platform and the quality of service this cloud platform guarantees to researchers. We believe it is fundamentally wrong to create dedicated VNFs for a specific cloud platform. All VNF descriptions should be done based on a standard domain-specific language (DSL) to provide isolation between a cloud platform API and a VFN implementation. For the purposes of VNF specification we use TOSCA (Topology and Orchestration Specification for Cloud Applications). This description, called TOSCA-template, is everything that is needed to describe a VNF. It covers the structure of a cloud application, its management policies, specifies an OS image and scripts, which can start, stop and configure the application that implements the VNF. The C2 platform assumes that a cloud manager should provide the TOSCA-template as a zip or tar archive with predetermined structure. We’d like to share the results of over NFV MANO platform overheads analysis and discuss our future plans.
</Content>
<field id="content">
Modern research cloud infrastructures purposed to help researcher to prepare virtual environment that satisfy various specific requirements. The focus could be set on a network topology and providing different network functions (NAT, Firewall, IDS, vSwitch etc.) in order to provide testbed for network research, or a network device testing. Another focus could be set on compute resources providing researcher the computational cluster, for example Hadoop-cluster. Regardless of purposes the researcher uses the cloud infrastructure we need the unified system that manages and orchestrates all types of cloud infrastructure resources. Network Function Virtualization (NFV) techniques separates network function logic from the hardware, which executes the first. There are several basic use cases for NFV. The network function (NF) lifecycle is stage process, that means that each NF passed through deployed stage, initialization stage, configuration stage, execution stage and undeploying stage. In our Demo the NF lifecycle managing and orchestration in DC will be demonstration. We would like to present a cloud platform architecture that adheres to the reference implementation ETSI NFV MANO model. As we will show the platform architecture and design successfully meet the requirements of researcher cloud VNFs. We call our platform Cloud Conductor or C2. The C2 platform provides the full VNF life-cycle support: initialization, configuration, execution and uninitialization. The C2 Platform provides virtual network services (VNS) to researchers by CPE virtualization. Virtual customer premise equipment (vCPE) is a way to deliver network services such as routing, firewall security and access to computational resources by using software rather than dedicated hardware devices. By virtualizing the CPE, the C2 Platform can simplify and accelerate service delivery, remotely configuring and managing devices and allowing researchers to order new services or adjust existing ones on demand. Especially will be discussed the CloudGW module solution, that is aimed to provide an entry border point for traffic from researcher to a virtual cloud infrastructure for further processing, depending on required virtual infrastructure (DBaaS, Hadoop, classic IaaS and etc.). The main value of a general service-oriented platform is the diversity of the predefined VNFs that can be deployed on this platform and the quality of service this cloud platform guarantees to researchers. We believe it is fundamentally wrong to create dedicated VNFs for a specific cloud platform. All VNF descriptions should be done based on a standard domain-specific language (DSL) to provide isolation between a cloud platform API and a VFN implementation. For the purposes of VNF specification we use TOSCA (Topology and Orchestration Specification for Cloud Applications). This description, called TOSCA-template, is everything that is needed to describe a VNF. It covers the structure of a cloud application, its management policies, specifies an OS image and scripts, which can start, stop and configure the application that implements the VNF. The C2 platform assumes that a cloud manager should provide the TOSCA-template as a zip or tar archive with predetermined structure. We’d like to share the results of over NFV MANO platform overheads analysis and discuss our future plans.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Vitaly</FirstName>
<FamilyName>Antonenko</FamilyName>
<Email>vantonenko@arccn.ru</Email>
<Affiliation>ARCCN</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Ruslan</FirstName>
<FamilyName>Smeliansky</FamilyName>
<Email>rsmeliansky@arccn.ru</Email>
<Affiliation>ARCCN</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Vitaly</FirstName>
<FamilyName>Antonenko</FamilyName>
<Email>vantonenko@arccn.ru</Email>
<Affiliation>ARCCN</Affiliation>
</Speaker>
<Speaker>
<FirstName>Ruslan</FirstName>
<FamilyName>Smeliansky</FamilyName>
<Email>rsmeliansky@arccn.ru</Email>
<Affiliation>ARCCN</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Research Data Infrastructures</Track>
<Track>Distributed Computing. GRID &amp; Cloud Computing</Track>
<Track>
Advanced Technologies for the High-Intensity Domains of Science and Business Applications
</Track>
</abstract>
<abstract>
<Id>186</Id>
<Title>BY-NCPHEP site: cloud and grid</Title>
<Content>
Status of the INP BSU Tier 3 site BY-NCPHEP presented. The experience of operation, efficience, flexibility, reliability and versatility of the new cloud based structure is discussed.
</Content>
<field id="content">
Status of the INP BSU Tier 3 site BY-NCPHEP presented. The experience of operation, efficience, flexibility, reliability and versatility of the new cloud based structure is discussed.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Vladimir</FirstName>
<FamilyName>Mossolov</FamilyName>
<Email>mos@hep.by</Email>
<Affiliation>INP BSU</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Vladimir</FirstName>
<FamilyName>Mossolov</FamilyName>
<Email>mos@hep.by</Email>
<Affiliation>INP BSU</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Distributed Computing. GRID &amp; Cloud Computing</Track>
</abstract>
<abstract>
<Id>187</Id>
<Title>
Activities Related to Scientific Visualization at National Research Nuclear University "MEPhI", Moscow, Russia: Research, Education and Publications
</Title>
<Content>
In this paper we present the results of scientific visualization research as a joint project of the NRNU MEPhI (Moscow, Russia) and the National Centre for Computer Animation, Bournemouth University (Bournemouth, United Kingdom). We consider scientific visualization as a modern computer-based method of data analysis. The essence of this method is to establish the correspondence between the analyzed initial data and their static or dynamic graphical interpretation analyzed visually. The results of the analysis of graphical data are interpreted in terms of both the initial data and the reality behind it. The method of scientific visualization as a method of scientific data analysis is a method of spatial modeling of those data. It allows for utilizing enormous potential abilities of a researcher and their spatial imaginary thinking in the process of data analysis. In the general case, the process of data analysis using the method of scientific visualization can be quite complex, iterative and interactive. This article is devoted to results of research and educational projects. These projects are quite different and multidisciplinary. The goals of the data analysis can be different as well. As a result, in most cases significantly different graphical representations can be involved.
</Content>
<field id="content">
In this paper we present the results of scientific visualization research as a joint project of the NRNU MEPhI (Moscow, Russia) and the National Centre for Computer Animation, Bournemouth University (Bournemouth, United Kingdom). We consider scientific visualization as a modern computer-based method of data analysis. The essence of this method is to establish the correspondence between the analyzed initial data and their static or dynamic graphical interpretation analyzed visually. The results of the analysis of graphical data are interpreted in terms of both the initial data and the reality behind it. The method of scientific visualization as a method of scientific data analysis is a method of spatial modeling of those data. It allows for utilizing enormous potential abilities of a researcher and their spatial imaginary thinking in the process of data analysis. In the general case, the process of data analysis using the method of scientific visualization can be quite complex, iterative and interactive. This article is devoted to results of research and educational projects. These projects are quite different and multidisciplinary. The goals of the data analysis can be different as well. As a result, in most cases significantly different graphical representations can be involved.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Victor</FirstName>
<FamilyName>Pilyugin</FamilyName>
<Email>vvpilyugin@mephi.ru</Email>
<Affiliation>
National Research Nuclear University "MEPhI", Moscow, Russian Federation
</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Evgeniya</FirstName>
<FamilyName>Malikova</FamilyName>
<Email>e_mal@inbox.ru</Email>
<Affiliation>LLC SMEDX, Samara, Russian Federation</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Valery</FirstName>
<FamilyName>Adzhiev</FamilyName>
<Email>valchess@gmail.com</Email>
<Affiliation>
National Centre for Computer Animation, Bournemouth University, Bournemouth, United Kingdom
</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Alexander</FirstName>
<FamilyName>Pasko</FamilyName>
<Email>apasko@bournemouth.ac.uk</Email>
<Affiliation>
National Centre for Computer Animation, Bournemouth University, Bournemouth, United Kingdom
</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Galina</FirstName>
<FamilyName>Pasko</FamilyName>
<Email>gip@pasko.org</Email>
<Affiliation>Uformia AS, Furuflaten, Norway</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Igal</FirstName>
<FamilyName>Milman</FamilyName>
<Email>igalush@gmail.com</Email>
<Affiliation>
National Research Nuclear University "MEPhI", Moscow, Russian Federation
</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Dmitry</FirstName>
<FamilyName>Popov</FamilyName>
<Email>ddpopov@mephi.ru</Email>
<Affiliation>Student NRNU MEPhI</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Victor</FirstName>
<FamilyName>Pilyugin</FamilyName>
<Email>vvpilyugin@mephi.ru</Email>
<Affiliation>
National Research Nuclear University "MEPhI", Moscow, Russian Federation
</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Innovative IT Education</Track>
<Track>Research Data Infrastructures</Track>
<Track>
Advanced Technologies for the High-Intensity Domains of Science and Business Applications
</Track>
</abstract>
<abstract>
<Id>188</Id>
<Title>
Service Reliability with the Cloud of Data Centers under Openstack
</Title>
<Content>
University ITMO (ifmo.ru) is developing the cloud of geographically distributed data centers under Openstack. The term “geographically distributed” in our proposal means data centers (DC) located in different places far from each other by hundred or thousand kilometers. Authors follow the conception of “dark” DC, i.e the DC has to perform normal operation without permanent maintainers even with minor problems (single machine or a number of disk drives went down). Periodically staff might visit the DC and fix the problems. It helps to reduce the overall maintenance cost for cloud of DC if reliable operation schema would be implemented. The proposal includes many aspects. In between the topics: the network architecture and features, main storage engine, …, service reliability. The last one is probably most important. Authors plan to describe thoughts and experiments with service reliability for geographically distributed data centers under Openstack. In particular it is planned to discuss Openstack deployment configurations and failover recovery tools.
</Content>
<field id="content">
University ITMO (ifmo.ru) is developing the cloud of geographically distributed data centers under Openstack. The term “geographically distributed” in our proposal means data centers (DC) located in different places far from each other by hundred or thousand kilometers. Authors follow the conception of “dark” DC, i.e the DC has to perform normal operation without permanent maintainers even with minor problems (single machine or a number of disk drives went down). Periodically staff might visit the DC and fix the problems. It helps to reduce the overall maintenance cost for cloud of DC if reliable operation schema would be implemented. The proposal includes many aspects. In between the topics: the network architecture and features, main storage engine, …, service reliability. The last one is probably most important. Authors plan to describe thoughts and experiments with service reliability for geographically distributed data centers under Openstack. In particular it is planned to discuss Openstack deployment configurations and failover recovery tools.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>andrey</FirstName>
<FamilyName>shevel</FamilyName>
<Email>andrey.shevel@pnpi.spb.ru</Email>
<Affiliation>PNPI, ITMO</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Sergey</FirstName>
<FamilyName>Khoruzhnikov</FamilyName>
<Email>xse@vuztc.ru</Email>
<Affiliation>ITMO</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Petr</FirstName>
<FamilyName>Fedchenkov</FamilyName>
<Email>giggsoff@gmail.com</Email>
<Affiliation>ITMO</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Nikita</FirstName>
<FamilyName>Samochin</FamilyName>
<Email>samon@corp.ifmo.ru</Email>
<Affiliation>ITMO</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Oleg</FirstName>
<FamilyName>Lazo</FamilyName>
<Email>oll@vuztc.ru</Email>
<Affiliation>ITMO</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Oleg</FirstName>
<FamilyName>Sadov</FamilyName>
<Email>oleg.sadov@gmail.com</Email>
<Affiliation>ITMO</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Anatoly</FirstName>
<FamilyName>Oreshkin</FamilyName>
<Email>anatoly.oreshkin@gmail.com</Email>
<Affiliation>PNPI</Affiliation>
</Co-Author>
<Speaker>
<FirstName>andrey</FirstName>
<FamilyName>shevel</FamilyName>
<Email>andrey.shevel@pnpi.spb.ru</Email>
<Affiliation>PNPI, ITMO</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Distributed Computing. GRID &amp; Cloud Computing</Track>
</abstract>
<abstract>
<Id>189</Id>
<Title>
Specialists - Electronic Training for «Nica» Program
</Title>
<Content>
The report given investigates the influence of Bologna Process on Russian System of Higher Professional Education. Changes are presented of both the format and content of Higher Education, made due to the European Educational format. Analysis of bachalors and masters students training is done at the State Dubna University for «Nica» Program. The Key-point of educational training Program is made on individualized teaching approach, located at the Base Department Laboratory of Physics High Energy (LPHE). The perspective of Electronic Training following individualized educational Routes is made, combined with sound Professional training.
</Content>
<field id="content">
The report given investigates the influence of Bologna Process on Russian System of Higher Professional Education. Changes are presented of both the format and content of Higher Education, made due to the European Educational format. Analysis of bachalors and masters students training is done at the State Dubna University for «Nica» Program. The Key-point of educational training Program is made on individualized teaching approach, located at the Base Department Laboratory of Physics High Energy (LPHE). The perspective of Electronic Training following individualized educational Routes is made, combined with sound Professional training.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Iurii</FirstName>
<FamilyName>Sakharov</FamilyName>
<Email>sakharovu@yandex.ru</Email>
<Affiliation>
Dubna International University for Nature,Society and Man
</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Iurii</FirstName>
<FamilyName>Sakharov</FamilyName>
<Email>sakharovu@yandex.ru</Email>
<Affiliation>
Dubna International University for Nature,Society and Man
</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Innovative IT Education</Track>
</abstract>
<abstract>
<Id>190</Id>
<Title>
First Results of the Radiation Monitoring of the GEM Muon Detectors at CMS
</Title>
<Content>
The higher energy and luminosity of future (HL) LHC imposed the development and testing of new type high-rate detectors as GEM (Gas Electron Multiplier). A monitoring system designed for measurement of the dose absorbed by the GEM detectors during the tests has been recently described [1]. The system uses a basic detector unit, called RADMON. There are in each unit two types of sensors: RadFETs, measuring the total absorbed dose of all radiations and p-i-n diodes – for particle (proton and neutron) radiation. The system has a modular structure, permitting to increase easily the number of controlled RADMONs – one module controls up to 12 RADMONs. For the first test, a group of 3 GEM chambers called supermodule was installed at the inner CMS endcap at March this years. One RADMON was installed at this supermodule for the dose monitoring. Through the dosimetric system controller, the measured data are transferred to the experiment data acquisition system. The real dose data are registering and will be processed and presented to the NEC 2017.
</Content>
<field id="content">
The higher energy and luminosity of future (HL) LHC imposed the development and testing of new type high-rate detectors as GEM (Gas Electron Multiplier). A monitoring system designed for measurement of the dose absorbed by the GEM detectors during the tests has been recently described [1]. The system uses a basic detector unit, called RADMON. There are in each unit two types of sensors: RadFETs, measuring the total absorbed dose of all radiations and p-i-n diodes – for particle (proton and neutron) radiation. The system has a modular structure, permitting to increase easily the number of controlled RADMONs – one module controls up to 12 RADMONs. For the first test, a group of 3 GEM chambers called supermodule was installed at the inner CMS endcap at March this years. One RADMON was installed at this supermodule for the dose monitoring. Through the dosimetric system controller, the measured data are transferred to the experiment data acquisition system. The real dose data are registering and will be processed and presented to the NEC 2017.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Lubomir</FirstName>
<FamilyName>Dimitrov</FamilyName>
<Email>lubomir.dimitrov@cern.ch</Email>
<Affiliation>INRNE - BAS</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Plamen</FirstName>
<FamilyName>Iaydjiev</FamilyName>
<Email>plamen.iaydjiev@cern.ch</Email>
<Affiliation>INRNE - BAS</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Ivan</FirstName>
<FamilyName>Vankov</FamilyName>
<Email>ivan.vankov@cern.ch</Email>
<Affiliation>INRNE - BAS</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Andrey</FirstName>
<FamilyName>Marinov</FamilyName>
<Email>andrey.marinov@cern.ch</Email>
<Affiliation>INRNE - CERN</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Georgi</FirstName>
<FamilyName>Mitev</FamilyName>
<Email>georgi.mitev@cern.ch</Email>
<Affiliation>INRNE - BAS</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Lubomir</FirstName>
<FamilyName>Dimitrov</FamilyName>
<Email>lubomir.dimitrov@cern.ch</Email>
<Affiliation>INRNE - BAS</Affiliation>
</Speaker>
<Speaker>
<FirstName>Ivan</FirstName>
<FamilyName>Vankov</FamilyName>
<Email>ivan.vankov@cern.ch</Email>
<Affiliation>INRNE - BAS</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Detector &amp; Nuclear Electronics</Track>
</abstract>
<abstract>
<Id>191</Id>
<Title>
Training of skilled personnel adaptive strategy for digital economy goals in the Dubna State University
</Title>
<Content>
The report focuses on new trends in education in conditions of transition to the digital economy. The program of development of digital economy in Russia requires new approaches to training and the use of modern digital technologies. The training strategy in modern conditions will be presented on the example of the State University University «Dubna».
</Content>
<field id="content">
The report focuses on new trends in education in conditions of transition to the digital economy. The program of development of digital economy in Russia requires new approaches to training and the use of modern digital technologies. The training strategy in modern conditions will be presented on the example of the State University University «Dubna».
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Evgenia</FirstName>
<FamilyName>Cheremisina</FamilyName>
<Email>kirpicheva77@gmail.com</Email>
<Affiliation>Черемисина Евгения</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Oksana</FirstName>
<FamilyName>Kreider</FamilyName>
<Email>okrei@mail.ru</Email>
<Affiliation>Крейдер Оксана</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Elena</FirstName>
<FamilyName>Kirpicheva</FamilyName>
<Email>kirphel@mail.ru</Email>
<Affiliation>Кирпичёва Елена</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Evgenia</FirstName>
<FamilyName>Cheremisina</FamilyName>
<Email>kirpicheva77@gmail.com</Email>
<Affiliation>Черемисина Евгения</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Innovative IT Education</Track>
</abstract>
<abstract>
<Id>192</Id>
<Title>On the Way to Open Education</Title>
<Content>
The report focuses on new trends in education. The system of open education is the path to a single world educational space, which offers unique opportunities not only for new educational initiatives on a global scale, but also for modernization of existing educational institutions.
</Content>
<field id="content">
The report focuses on new trends in education. The system of open education is the path to a single world educational space, which offers unique opportunities not only for new educational initiatives on a global scale, but also for modernization of existing educational institutions.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Oksana</FirstName>
<FamilyName>Kreider</FamilyName>
<Email>okrei@mail.ru</Email>
<Affiliation>Крейдер Оксана</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Elena</FirstName>
<FamilyName>Kirpicheva</FamilyName>
<Email>kirphel@mail.ru</Email>
<Affiliation>Кирпичёва Елена</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Oksana</FirstName>
<FamilyName>Kreider</FamilyName>
<Email>okrei@mail.ru</Email>
<Affiliation>Крейдер Оксана</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Innovative IT Education</Track>
</abstract>
<abstract>
<Id>193</Id>
<Title>Educational Support of NICA Project</Title>
<Content>
Educational support of the megaproject NICA is aimed at attracting public attention (school and university students and generally interested audience) to the scientific achievements of JINR and also training specialists to work at the accelerating complex NICA in the mid-term and long-term perspective. It is also necessary to include scientific and applied results obtained at NICA in the educational programs of undergraduate and postgraduate education. The expected scientific results obtained at the NICA collider will undoubtedly broaden the horizons of the world’s knowledge about the structure and evolution of matter at the early stage of the Universe evolution and, in the light of experimental data, will allow one to answer the actual questions of the modern science, for example about nucleon spin nature and spin structure of the lightest nucleus – deuterium – at small distances. Such scientific findings and technological solutions should be accompanied by educational, popular-science and outreach projects intended for a wider audience, including school students. In the future, it will allow us to overcome a serious social problem – decline in young people's interest in scientific research and engineering professions. As a first step there has been made the open lesson for school students on the theme: “NICA. Universe in the Lab”. In this video, Academician Grigory Vladimirovich Trubnikov speaks about the research that scientists from different countries will carry out at the NICA accelerator complex. Creation of a modern educational environment of continuous learning and training of highly qualified personnel in the framework of the mega-project “NICA complex” requires development of online courses within the NICA project subject-matter, for example, about basics of accelerator equipment, experimental methods of nuclear physics, introduction to the physics of relativistic nuclear collisions, electronics for physics experiment etc. The special attention will be paid to the development and promotion of the specialized site of the NICA project which will include as the actual project information as educational materials within the subject-matter of the NICA project for students and young scientists. Using modern technologies of 3D-modeling and scientific data visualization will enable the development of educational resources of NICA at the level of the world's leading research centers. The interactive map of the NICA complex was created and will be updated during the development of the complex. Interactive map allows you to learn the setups of the collider. Complete modules of the complex shot on video, in order to demonstrate the current construction process. The modules, which are at the stage of development now demonstrated as a 3-D graphics that reveal the device itself and explain it's working principle. For each node of the complex we are expecting to make both video and graphic materials. Building brand awareness of JINR and NICA for a wider audience is one of the most important tasks. A good solution to this problem is creation of multimedia exhibits associated with the JINR research topics and participation in a variety of Russian and international exhibitions, days of science, museum exhibitions.
</Content>
<field id="content">
Educational support of the megaproject NICA is aimed at attracting public attention (school and university students and generally interested audience) to the scientific achievements of JINR and also training specialists to work at the accelerating complex NICA in the mid-term and long-term perspective. It is also necessary to include scientific and applied results obtained at NICA in the educational programs of undergraduate and postgraduate education. The expected scientific results obtained at the NICA collider will undoubtedly broaden the horizons of the world’s knowledge about the structure and evolution of matter at the early stage of the Universe evolution and, in the light of experimental data, will allow one to answer the actual questions of the modern science, for example about nucleon spin nature and spin structure of the lightest nucleus – deuterium – at small distances. Such scientific findings and technological solutions should be accompanied by educational, popular-science and outreach projects intended for a wider audience, including school students. In the future, it will allow us to overcome a serious social problem – decline in young people's interest in scientific research and engineering professions. As a first step there has been made the open lesson for school students on the theme: “NICA. Universe in the Lab”. In this video, Academician Grigory Vladimirovich Trubnikov speaks about the research that scientists from different countries will carry out at the NICA accelerator complex. Creation of a modern educational environment of continuous learning and training of highly qualified personnel in the framework of the mega-project “NICA complex” requires development of online courses within the NICA project subject-matter, for example, about basics of accelerator equipment, experimental methods of nuclear physics, introduction to the physics of relativistic nuclear collisions, electronics for physics experiment etc. The special attention will be paid to the development and promotion of the specialized site of the NICA project which will include as the actual project information as educational materials within the subject-matter of the NICA project for students and young scientists. Using modern technologies of 3D-modeling and scientific data visualization will enable the development of educational resources of NICA at the level of the world's leading research centers. The interactive map of the NICA complex was created and will be updated during the development of the complex. Interactive map allows you to learn the setups of the collider. Complete modules of the complex shot on video, in order to demonstrate the current construction process. The modules, which are at the stage of development now demonstrated as a 3-D graphics that reveal the device itself and explain it's working principle. For each node of the complex we are expecting to make both video and graphic materials. Building brand awareness of JINR and NICA for a wider audience is one of the most important tasks. A good solution to this problem is creation of multimedia exhibits associated with the JINR research topics and participation in a variety of Russian and international exhibitions, days of science, museum exhibitions.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Yury</FirstName>
<FamilyName>Panebrattsev</FamilyName>
<Email>yuri@intergraphics.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Victoria</FirstName>
<FamilyName>Belaga</FamilyName>
<Email>belaga@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Pavel</FirstName>
<FamilyName>Kochnev</FamilyName>
<Email>kochnevpav@mail.ru</Email>
<Affiliation>
Dubna International University of Nature, Society and Man
</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Evgeny</FirstName>
<FamilyName>Dolgy</FamilyName>
<Email>anegin13@yandex.ru</Email>
<Affiliation>University of Dubna</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>V.D.</FirstName>
<FamilyName>Kekelidze</FamilyName>
<Email>kekelidze@jinr.ru</Email>
<Affiliation>JINR, VBLHEP</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Ksenia</FirstName>
<FamilyName>Klygina</FamilyName>
<Email>wildksu@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Yury</FirstName>
<FamilyName>Panebrattsev</FamilyName>
<Email>yuri@intergraphics.ru</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Innovative IT Education</Track>
</abstract>
<abstract>
<Id>194</Id>
<Title>
Online courses and new educational programs to support research priorities within the subject-matter of JINR projects on the basis of modern educational platforms
</Title>
<Content>
Rapid development of information and communication technologies and widespread use of the Internet have led to a qualitative change in the educational technologies used around the world. The most popular form of training nowadays is blended learning, when a full-time educational process is complemented with computer-learning tools: online courses, interactive practicums and laboratory works, computer modelling tools and simulators. Solving the problem of integration of education and science presupposes establishing an efficient and sustainable interaction of universities with research centers and institutes. The main mission of the JINR laboratories is generating new knowledge. To develop successfully, the laboratories need to attract talented young people and highly qualified professionals to work at JINR. Moreover, there are highly skilled specialists currently working at JINR, who could share their knowledge through online courses intended for students from various universities of Russia and the Member States. On the other hand, the universities are interested in training highly qualified specialists to work on scientific projects. For this purpose, it is necessary to solve the problem of cooperation between universities and research centers aimed at coordinating the relevant educational programs, as well as to integrate the results of modern experiments in the educational process in the form of special courses and electives, or as independent course units of the basic disciplines. In the past four years, the most popular new educational technologies frequently used in the further educational process are massive open online courses (MOOCs). The world’s leading American and European universities – Harvard University, Massachusetts Institute of Technology, Stanford University, etc. – have adopted these technologies. Today, the most extensive MOOC platforms are being developed by the US universities: more than 10 million users in edX, more than 4 million – in Udacity, and more than 23 million – in Coursera. In our country there are also several platforms offering open educational resources: "Open Education", "Universarium", "Lectorium". However, when it comes to training specialists to work in the projects on the top-priority research fields and MEGASCIENCE projects, it is worth listing a number of problems that cannot be solved by the existing MOOC model: •	Survey of employers’ (research centers’) opinion in order to generate a list of positions for which young specialists are to be engaged and an appropriate set of skills that the applicants should master •	Generation of a list of courses to be studied and educational content, taking into account the requirements agreed both by teachers and employers – scientists and engineers •	Development of educational materials based on the knowledge of experts working directly in the field of interest and not teaching on a regular basis •	Development of training courses based on the results of individual research groups and experiments •	Prompt adaptation of teaching materials and practical tasks as a response to the rapidly changing technologies •	Search for subjects for scientific and engineering study and potential scientific supervisors as early as at the stage of doing relevant online-courses •	Formation of the employer's opinion based on the results of student's online-learning for continuation of their career in the research center (in the experiment). To solve the above-mentioned problems, it is proposed to develop the open educational environment to support research priorities within the subject-matter of JINR projects in cooperation with NRNU MEPhI, Dubna University, Kazan Federal University, St. Petersburg University, the universities of the Member States and associated members, and others. The JINR University Center (UC) can act as an integrator of such cooperation, as the UC includes JINR-based departments of the leading Russian universities. On the basis of the UC, together with JINR scientists and engineers, as well as specialists from the universities participating in the project, the open educational environment can be created. The use of blended learning, when a full-time educational process is complemented with e learning tools, solves several important problems: •	Lack of teaching specialists at universities; •	Need for allocation of greater financial resources to provide transportation and social infrastructure for many students from the Member States, which is required for their long-term training at the JINR basic facilities •	The developed online courses will allow forming a network of educational programs for joint training of master students, with the participation of JINR Member States universities. The courses will be developed in the MOOC (Massive Open Online Courses) format and made available on the corresponding open-source platforms. Specialists from JINR in collaboration with have experience in creating online courses for Coursera and edX. Nowadays students are enrolled in courses: “Elements of nuclear and atomic physics”, “Heavy ions physics” etc. The entire course development process was mastered, including the stage of pedagogical design, the stage of preparation of educational content, the stage of placement of the course on the educational platform and the stage of support. It is suggested to start the development with a training course for 1-year master students. The testing of the system can be carried out, for example, on the basis of master's programs of ISAM: 27.04.03-2 "System analysis of design and technological solutions". Currently, the following online work courses is proceeding: •	"Modern problems in system analysis and management" (prof. E.N. Cheremisina) •	"Distributed and cloud computing" (prof. V.V. Korenkov) •	"Big data analytics" (prof. P.V. Zrelov) In the development of courses modern technologies of dynamic interactive 2D and 3D web-graphics is used. The insurance of individual components compatibility of the open educational environment and creation of opportunities for their multiple usages will be provided by conforming to the international standards defining the requirements for educational content. In developing check and reference materials an LTI (Learning Tools Interoperability) specification will be used. It contains recommendations for the structure and rules of development of external educational applications for their further integration with a variety of learning management systems (LMS).
</Content>
<field id="content">
Rapid development of information and communication technologies and widespread use of the Internet have led to a qualitative change in the educational technologies used around the world. The most popular form of training nowadays is blended learning, when a full-time educational process is complemented with computer-learning tools: online courses, interactive practicums and laboratory works, computer modelling tools and simulators. Solving the problem of integration of education and science presupposes establishing an efficient and sustainable interaction of universities with research centers and institutes. The main mission of the JINR laboratories is generating new knowledge. To develop successfully, the laboratories need to attract talented young people and highly qualified professionals to work at JINR. Moreover, there are highly skilled specialists currently working at JINR, who could share their knowledge through online courses intended for students from various universities of Russia and the Member States. On the other hand, the universities are interested in training highly qualified specialists to work on scientific projects. For this purpose, it is necessary to solve the problem of cooperation between universities and research centers aimed at coordinating the relevant educational programs, as well as to integrate the results of modern experiments in the educational process in the form of special courses and electives, or as independent course units of the basic disciplines. In the past four years, the most popular new educational technologies frequently used in the further educational process are massive open online courses (MOOCs). The world’s leading American and European universities – Harvard University, Massachusetts Institute of Technology, Stanford University, etc. – have adopted these technologies. Today, the most extensive MOOC platforms are being developed by the US universities: more than 10 million users in edX, more than 4 million – in Udacity, and more than 23 million – in Coursera. In our country there are also several platforms offering open educational resources: "Open Education", "Universarium", "Lectorium". However, when it comes to training specialists to work in the projects on the top-priority research fields and MEGASCIENCE projects, it is worth listing a number of problems that cannot be solved by the existing MOOC model: •	Survey of employers’ (research centers’) opinion in order to generate a list of positions for which young specialists are to be engaged and an appropriate set of skills that the applicants should master •	Generation of a list of courses to be studied and educational content, taking into account the requirements agreed both by teachers and employers – scientists and engineers •	Development of educational materials based on the knowledge of experts working directly in the field of interest and not teaching on a regular basis •	Development of training courses based on the results of individual research groups and experiments •	Prompt adaptation of teaching materials and practical tasks as a response to the rapidly changing technologies •	Search for subjects for scientific and engineering study and potential scientific supervisors as early as at the stage of doing relevant online-courses •	Formation of the employer's opinion based on the results of student's online-learning for continuation of their career in the research center (in the experiment). To solve the above-mentioned problems, it is proposed to develop the open educational environment to support research priorities within the subject-matter of JINR projects in cooperation with NRNU MEPhI, Dubna University, Kazan Federal University, St. Petersburg University, the universities of the Member States and associated members, and others. The JINR University Center (UC) can act as an integrator of such cooperation, as the UC includes JINR-based departments of the leading Russian universities. On the basis of the UC, together with JINR scientists and engineers, as well as specialists from the universities participating in the project, the open educational environment can be created. The use of blended learning, when a full-time educational process is complemented with e learning tools, solves several important problems: •	Lack of teaching specialists at universities; •	Need for allocation of greater financial resources to provide transportation and social infrastructure for many students from the Member States, which is required for their long-term training at the JINR basic facilities •	The developed online courses will allow forming a network of educational programs for joint training of master students, with the participation of JINR Member States universities. The courses will be developed in the MOOC (Massive Open Online Courses) format and made available on the corresponding open-source platforms. Specialists from JINR in collaboration with have experience in creating online courses for Coursera and edX. Nowadays students are enrolled in courses: “Elements of nuclear and atomic physics”, “Heavy ions physics” etc. The entire course development process was mastered, including the stage of pedagogical design, the stage of preparation of educational content, the stage of placement of the course on the educational platform and the stage of support. It is suggested to start the development with a training course for 1-year master students. The testing of the system can be carried out, for example, on the basis of master's programs of ISAM: 27.04.03-2 "System analysis of design and technological solutions". Currently, the following online work courses is proceeding: •	"Modern problems in system analysis and management" (prof. E.N. Cheremisina) •	"Distributed and cloud computing" (prof. V.V. Korenkov) •	"Big data analytics" (prof. P.V. Zrelov) In the development of courses modern technologies of dynamic interactive 2D and 3D web-graphics is used. The insurance of individual components compatibility of the open educational environment and creation of opportunities for their multiple usages will be provided by conforming to the international standards defining the requirements for educational content. In developing check and reference materials an LTI (Learning Tools Interoperability) specification will be used. It contains recommendations for the structure and rules of development of external educational applications for their further integration with a variety of learning management systems (LMS).
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Victoria</FirstName>
<FamilyName>Belaga</FamilyName>
<Email>belaga@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Yury</FirstName>
<FamilyName>Panebrattsev</FamilyName>
<Email>yuri@intergraphics.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Pavel</FirstName>
<FamilyName>Kochnev</FamilyName>
<Email>kochnevpav@mail.ru</Email>
<Affiliation>
Dubna International University of Nature, Society and Man
</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Ksenia</FirstName>
<FamilyName>Klygina</FamilyName>
<Email>wildksu@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Victoria</FirstName>
<FamilyName>Belaga</FamilyName>
<Email>belaga@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Innovative IT Education</Track>
</abstract>
<abstract>
<Id>195</Id>
<Title>
"Interactive Platform of Nuclear Experiment Modeling” as a multidisciplinary tool in the training of specialists in the fields of ICT and experimental nuclear physics.
</Title>
<Content>
In this report, we would like to present a software and hardware complex used to training of university students for their further work in real physical experiments. Our educational tool “Virtual Laboratory of Nuclear Fission” consists of several complementary components: A) General view: – Key ideas in nuclear physics and nuclear structure, – Basic theoretical models of nuclei, – Introduction into instruments and methods for the study of radioactive decays, – Virtual practicum and real measurements. B) Specific tasks: – Physics of spontaneous fission, – Experimental studies of spontaneous fission, – Light Ions Spectrometer (LIS) spectrometer, – Measurements, – Data analysis. Use of this tool during the summer student practice will enable the professionals to be able to prepare experiments in a relatively short time and perform measurements simultaneously with data analysis. We plan to integrate this educational tool into the traditional educational process applying the blended learning model. Virtual labs based on real experimental data for development of skills and competences in nuclear physics experimental techniques. One of the main trends of the modern university education is the inclusion of experimental data and research methods into the educational process. It is crucial to ensure that university graduates are able to engage in research in modern scientific laboratories with relative ease. This project proposes to develop the educational model on the basis of the modern physical setup — Light Ion Spectrometer (LIS).This model will be developed in collaboration with the Flerov Laboratory of Nuclear Reactions. At this model students will be required to study the nuclear physics phenomenon such as spontaneous fission, which forms the basis for the studies of multi-body decay modes. A distinctive feature of this model is its relative “simplicity”, while it uses the most advanced radiation detectors, nuclear electronics and other equipment to make precise measurements. This allows the students in a relatively short training period to go through all the stages of preparation of the experimental setup in order to perform the experiment and obtain physical results. The following skills set students will acquire: – Spectrometry of alpha particles and heavy charged fragments with the help of modern semiconductor detectors (pin-diodes), – Learn techniques on the time-of-flight measurements using time registered detectors based on microchannel plates, – Data analysis from modern digitizers. Measurement of time-of-flight spectra with high precision and the study of plasma delays effects in the registration of fragments with high charge in the semiconductor detectors, – Processing of the experimental data and obtaining of the mass spectra of the fission fragments. The set of these virtual labs form the competencies that are necessary for students’ work in a modern experiment in the field of nuclear physics. “Interactive Platform of Nuclear Experiment Modelling” as a multidisciplinary tool in the training of specialists in the fields of ICT and experimental nuclear physics. A new approach for conceptualization and skills development in scientific and engineering project work is proposed. In this computer-based approach libraries of various components of nuclear physics experiment (radioactive sources, various types of detectors, instruments and components of nuclear electronics) are used. This is different from traditional labs with defined equipment and measurement methods at the beginning of work. One of the advantages of this computer-based approach is that students specializing in the field of experimental nuclear physics are able to assemble preferred virtual experimental setup using existing components of the libraries. Using high-level programming languages (C ++, C #, etc.) with the set of libraries students can develop new components of virtual experimental setups. This multidisciplinary tool has possibility to be used by a range of students from different scientific and engineering disciplines e.g. ICT specialists, engineers etc.
</Content>
<field id="content">
In this report, we would like to present a software and hardware complex used to training of university students for their further work in real physical experiments. Our educational tool “Virtual Laboratory of Nuclear Fission” consists of several complementary components: A) General view: – Key ideas in nuclear physics and nuclear structure, – Basic theoretical models of nuclei, – Introduction into instruments and methods for the study of radioactive decays, – Virtual practicum and real measurements. B) Specific tasks: – Physics of spontaneous fission, – Experimental studies of spontaneous fission, – Light Ions Spectrometer (LIS) spectrometer, – Measurements, – Data analysis. Use of this tool during the summer student practice will enable the professionals to be able to prepare experiments in a relatively short time and perform measurements simultaneously with data analysis. We plan to integrate this educational tool into the traditional educational process applying the blended learning model. Virtual labs based on real experimental data for development of skills and competences in nuclear physics experimental techniques. One of the main trends of the modern university education is the inclusion of experimental data and research methods into the educational process. It is crucial to ensure that university graduates are able to engage in research in modern scientific laboratories with relative ease. This project proposes to develop the educational model on the basis of the modern physical setup — Light Ion Spectrometer (LIS).This model will be developed in collaboration with the Flerov Laboratory of Nuclear Reactions. At this model students will be required to study the nuclear physics phenomenon such as spontaneous fission, which forms the basis for the studies of multi-body decay modes. A distinctive feature of this model is its relative “simplicity”, while it uses the most advanced radiation detectors, nuclear electronics and other equipment to make precise measurements. This allows the students in a relatively short training period to go through all the stages of preparation of the experimental setup in order to perform the experiment and obtain physical results. The following skills set students will acquire: – Spectrometry of alpha particles and heavy charged fragments with the help of modern semiconductor detectors (pin-diodes), – Learn techniques on the time-of-flight measurements using time registered detectors based on microchannel plates, – Data analysis from modern digitizers. Measurement of time-of-flight spectra with high precision and the study of plasma delays effects in the registration of fragments with high charge in the semiconductor detectors, – Processing of the experimental data and obtaining of the mass spectra of the fission fragments. The set of these virtual labs form the competencies that are necessary for students’ work in a modern experiment in the field of nuclear physics. “Interactive Platform of Nuclear Experiment Modelling” as a multidisciplinary tool in the training of specialists in the fields of ICT and experimental nuclear physics. A new approach for conceptualization and skills development in scientific and engineering project work is proposed. In this computer-based approach libraries of various components of nuclear physics experiment (radioactive sources, various types of detectors, instruments and components of nuclear electronics) are used. This is different from traditional labs with defined equipment and measurement methods at the beginning of work. One of the advantages of this computer-based approach is that students specializing in the field of experimental nuclear physics are able to assemble preferred virtual experimental setup using existing components of the libraries. Using high-level programming languages (C ++, C #, etc.) with the set of libraries students can develop new components of virtual experimental setups. This multidisciplinary tool has possibility to be used by a range of students from different scientific and engineering disciplines e.g. ICT specialists, engineers etc.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Yury</FirstName>
<FamilyName>Panebrattsev</FamilyName>
<Email>yuri@intergraphics.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Alexandr</FirstName>
<FamilyName>Strekalovsky</FamilyName>
<Email>alex.strek@bk.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Dmitrii</FirstName>
<FamilyName>Kamanin</FamilyName>
<Email>kamanin@jinr.ru</Email>
<Affiliation>JINR, SO &amp; IC Office</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Victoria</FirstName>
<FamilyName>Belaga</FamilyName>
<Email>belaga@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Evgeny</FirstName>
<FamilyName>Dolgy</FamilyName>
<Email>anegin13@yandex.ru</Email>
<Affiliation>University of Dubna</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Ksenia</FirstName>
<FamilyName>Klygina</FamilyName>
<Email>wildksu@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Pavel</FirstName>
<FamilyName>Kochnev</FamilyName>
<Email>kochnevpav@mail.ru</Email>
<Affiliation>
Dubna International University of Nature, Society and Man
</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Ivan</FirstName>
<FamilyName>Vankov</FamilyName>
<Email>ivan.vankov@cern.ch</Email>
<Affiliation>
Institute for Nuclear Research and Nuclear Energy, Bulgarian Academy of Sciences
</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Yury</FirstName>
<FamilyName>Panebrattsev</FamilyName>
<Email>yuri@intergraphics.ru</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<Speaker>
<FirstName>Victoria</FirstName>
<FamilyName>Belaga</FamilyName>
<Email>belaga@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Innovative IT Education</Track>
</abstract>
<abstract>
<Id>196</Id>
<Title>
Novel approach to the particle track reconstruction based on deep learning methods
</Title>
<Content>
A fundamental problem of data processing for high energy and nuclear physics (HENP) experiments is the event reconstruction. The main part of it is finding tracks among a great number of so-called hits produced on sequential co-ordinate planes of tracking detectors. The track recognition problem consists in joining these hits into clusters, each of them joins all hits belonging to the same track, one of many others, discarding noise and fake hits. Such a procedure named tracking is especially difficult for modern HENP experiments with heavy ions where detectors register events with very high multiplicity. Besides, this problem is seriously aggravated due to the famous shortcoming of quite popular multiwired, strip and GEM detectors where the appearance of fake hits is caused by extra spurious crossings of wires or strips, while the number of those fakes is greater for some order of magnitude than for true hits. Here we discuss the novel two steps technique based on hit preprocessing by a sophisticated directed search followed by applying a deep learning neural network. Preliminary results of our approach for simulated events are presented.
</Content>
<field id="content">
A fundamental problem of data processing for high energy and nuclear physics (HENP) experiments is the event reconstruction. The main part of it is finding tracks among a great number of so-called hits produced on sequential co-ordinate planes of tracking detectors. The track recognition problem consists in joining these hits into clusters, each of them joins all hits belonging to the same track, one of many others, discarding noise and fake hits. Such a procedure named tracking is especially difficult for modern HENP experiments with heavy ions where detectors register events with very high multiplicity. Besides, this problem is seriously aggravated due to the famous shortcoming of quite popular multiwired, strip and GEM detectors where the appearance of fake hits is caused by extra spurious crossings of wires or strips, while the number of those fakes is greater for some order of magnitude than for true hits. Here we discuss the novel two steps technique based on hit preprocessing by a sophisticated directed search followed by applying a deep learning neural network. Preliminary results of our approach for simulated events are presented.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Gennady</FirstName>
<FamilyName>Ososkov</FamilyName>
<Email>ososkov@jinr.ru</Email>
<Affiliation>Joint Institute for Nuclear Research</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Pavel</FirstName>
<FamilyName>Goncharov</FamilyName>
<Email>kaliostrogoblin3@gmail.com</Email>
<Affiliation>
Gomel State Technical University, Faculty of Automation and Information Systems, Gomel, Belarus
</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Andrey</FirstName>
<FamilyName>Tsytrinov</FamilyName>
<Email>tsytrin@gstu.by</Email>
<Affiliation>
Gomel State Technical University, Faculty of Automation and Information Systems, Gomel, Belarus
</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Sergey</FirstName>
<FamilyName>Mitsyn</FamilyName>
<Email>svm@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Gennady</FirstName>
<FamilyName>Ososkov</FamilyName>
<Email>ososkov@jinr.ru</Email>
<Affiliation>Joint Institute for Nuclear Research</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Machine Learning Algorithms and Big Data Analytics</Track>
</abstract>
<abstract>
<Id>197</Id>
<Title>
Interrelation communication in the education for robotics
</Title>
<Content>
The report considers the possibility to create a fundamentally new scientifically grounded platform for the educational process in intelligent robotics based on a new kind of intelligent self-organizing robot training device that takes into account the results of IT design development on the basis of computational intelligence toolkit and new types of mechatronics in the educational process. In this educational process, the teacher develops his active knowledge through participation in the creation of new types of mechatronics, algorithmic support, and software for intelligent cognitive control of a particular robot.
</Content>
<field id="content">
The report considers the possibility to create a fundamentally new scientifically grounded platform for the educational process in intelligent robotics based on a new kind of intelligent self-organizing robot training device that takes into account the results of IT design development on the basis of computational intelligence toolkit and new types of mechatronics in the educational process. In this educational process, the teacher develops his active knowledge through participation in the creation of new types of mechatronics, algorithmic support, and software for intelligent cognitive control of a particular robot.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Olga</FirstName>
<FamilyName>Tyatyushkina</FamilyName>
<Email>tyatyushkina@mail.ru</Email>
<Affiliation>
Dubna State University. Institute of system analysis and management
</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Sergey</FirstName>
<FamilyName>Ulyanov</FamilyName>
<Email>ulyanovsv@mail.ru</Email>
<Affiliation>
Dubna State University. Institute of system analysis and management
</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Olga</FirstName>
<FamilyName>Tyatyushkina</FamilyName>
<Email>tyatyushkina@mail.ru</Email>
<Affiliation>
Dubna State University. Institute of system analysis and management
</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Innovative IT Education</Track>
</abstract>
<abstract>
<Id>199</Id>
<Title>
Applying Big Data solutions for log analytics in the PanDA infrastructure
</Title>
<Content>
PanDA is the workflow management system of the ATLAS experiment at the LHC and is responsible for generating, brokering and monitoring up to two million jobs per day across 150 computing centers in the Worldwide LHC Computing Grid. The PanDA core consists of several components deployed centrally on around 20 servers. The daily log volume is around 400GB per day. In certain cases, troubleshooting a particular issue on the raw log files can be compared to searching for a needle in a haystack and requires a high level of expertise. Therefore we decided to build on trending Big Data solutions and utilize the ELK infrastructure (Filebeat, Logstash, Elastic Search and Kibana) to process, index and analyze our log files. This allows to overcome troubleshooting complexity, provides a better interface to the operations team and generates advanced analytics to understand our system. This paper will describe the features of the ELK stack, our infrastructure, optimal configuration settings and filters. We will provide examples of graphs and dashboards generated through the ELK system to demonstrate the potential of the system. Finally, we will show the current integration of Kibana with the PanDA monitoring frontend and other usage possibilities, such as proactive notification of exceptions in the system.
</Content>
<field id="content">
PanDA is the workflow management system of the ATLAS experiment at the LHC and is responsible for generating, brokering and monitoring up to two million jobs per day across 150 computing centers in the Worldwide LHC Computing Grid. The PanDA core consists of several components deployed centrally on around 20 servers. The daily log volume is around 400GB per day. In certain cases, troubleshooting a particular issue on the raw log files can be compared to searching for a needle in a haystack and requires a high level of expertise. Therefore we decided to build on trending Big Data solutions and utilize the ELK infrastructure (Filebeat, Logstash, Elastic Search and Kibana) to process, index and analyze our log files. This allows to overcome troubleshooting complexity, provides a better interface to the operations team and generates advanced analytics to understand our system. This paper will describe the features of the ELK stack, our infrastructure, optimal configuration settings and filters. We will provide examples of graphs and dashboards generated through the ELK system to demonstrate the potential of the system. Finally, we will show the current integration of Kibana with the PanDA monitoring frontend and other usage possibilities, such as proactive notification of exceptions in the system.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Aleksandr</FirstName>
<FamilyName>Alekseev</FamilyName>
<Email>frt@tpu.ru</Email>
<Affiliation>National Research Tomsk Polytechnic University</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Alexei</FirstName>
<FamilyName>Klimentov</FamilyName>
<Email>alexei.klimentov@cern.ch</Email>
<Affiliation>Brookhaven National Lab</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Fernando</FirstName>
<FamilyName>Barreiro Megino</FamilyName>
<Email>fernando.harald.barreiro.megino@cern.ch</Email>
<Affiliation>University of Texas at Arlington</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Tatiana</FirstName>
<FamilyName>Korchuganova</FamilyName>
<Email>tatianakorchuganova@tpu.ru</Email>
<Affiliation>National Research Tomsk Polytechnic University</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Tadashi</FirstName>
<FamilyName>Maeno</FamilyName>
<Email>tmaeno@bnl.gov</Email>
<Affiliation>BNL</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Siarhei</FirstName>
<FamilyName>PADOLSKI</FamilyName>
<Email>spadolski@bnl.gov</Email>
<Affiliation>BNL</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Aleksandr</FirstName>
<FamilyName>Alekseev</FamilyName>
<Email>frt@tpu.ru</Email>
<Affiliation>National Research Tomsk Polytechnic University</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>Distributed Computing. GRID &amp; Cloud Computing</Track>
</abstract>
<abstract>
<Id>200</Id>
<Title>
Predictive analytics as an essential mechanism for situational awareness at the ATLAS Production System
</Title>
<Content>
The workflow management process should be under the control of the certain service that is able to forecast the processing time dynamically according to the status of the processing environment and workflow itself, and to react immediately on any abnormal behaviour of the execution process. Such situational awareness analytic service would provide the possibility to monitor the execution process, to detect the source of any malfunction, and to optimize the management process. The stated service for the second generation of the ATLAS Production System (ProdSys2, an automated scheduling system) is based on predictive analytics approach to estimate the duration of the data processings (in terms of ProdSys2, it is task and chain of tasks) with later usage in decision making processes. Machine learning ensemble methods are chosen to estimate completion time (i.e., “Time To Complete”, TTC) for every (production) task and chain of tasks, thus “abnormal” task processing times would warn about possible failure state of the system. This is the primary phase of the service and its precision is crucial. The first implementation of such analytic service already includes Task TTC Estimator tool and is designed in a way to provide a comprehensive set of options to adjust the analysis process and possibility to extend its functionality.
</Content>
<field id="content">
The workflow management process should be under the control of the certain service that is able to forecast the processing time dynamically according to the status of the processing environment and workflow itself, and to react immediately on any abnormal behaviour of the execution process. Such situational awareness analytic service would provide the possibility to monitor the execution process, to detect the source of any malfunction, and to optimize the management process. The stated service for the second generation of the ATLAS Production System (ProdSys2, an automated scheduling system) is based on predictive analytics approach to estimate the duration of the data processings (in terms of ProdSys2, it is task and chain of tasks) with later usage in decision making processes. Machine learning ensemble methods are chosen to estimate completion time (i.e., “Time To Complete”, TTC) for every (production) task and chain of tasks, thus “abnormal” task processing times would warn about possible failure state of the system. This is the primary phase of the service and its precision is crucial. The first implementation of such analytic service already includes Task TTC Estimator tool and is designed in a way to provide a comprehensive set of options to adjust the analysis process and possibility to extend its functionality.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Mikhail</FirstName>
<FamilyName>Titov</FamilyName>
<Email>mikhail.titov@cern.ch</Email>
<Affiliation>National Research Centre «Kurchatov Institute»</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Dmitry</FirstName>
<FamilyName>Golubkov</FamilyName>
<Email>dmitry.v.golubkov@cern.ch</Email>
<Affiliation>Institute for High Energy Physics</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Fernando</FirstName>
<FamilyName>Barreiro Megino</FamilyName>
<Email>fernando.harald.barreiro.megino@cern.ch</Email>
<Affiliation>University of Texas at Arlington</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Ivan</FirstName>
<FamilyName>Tertychnyy</FamilyName>
<Email>ivan.tertychnyy@cern.ch</Email>
<Affiliation>National Research Centre «Kurchatov Institute»</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Maksim</FirstName>
<FamilyName>Gubin</FamilyName>
<Email>gubin.m.u@gmail.com</Email>
<Affiliation>Tomsk Polytechnic University</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Alexei</FirstName>
<FamilyName>Klimentov</FamilyName>
<Email>alexei.klimentov@cern.ch</Email>
<Affiliation>Brookhaven National Lab</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Mikhail</FirstName>
<FamilyName>Borodin</FamilyName>
<Email>mborodin@cern.ch</Email>
<Affiliation>The University of Iowa (US)</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Mikhail</FirstName>
<FamilyName>Titov</FamilyName>
<Email>mikhail.titov@cern.ch</Email>
<Affiliation>National Research Centre «Kurchatov Institute»</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>Machine Learning Algorithms and Big Data Analytics</Track>
</abstract>
<abstract>
<Id>201</Id>
<Title>
The title of talk is "Status of the project NICA at JINR
</Title>
<Content>
The study of the heavy ion collisions is of the great interest in high energy physics due to expected phase transition from the nucleons to the quark gluon plasma. But to have a full picture of the effect there is a lack of experimental data at low energy region for the nuclei-nucleus collisions. The goal of the NICA project at JINR is to cover the collision energy range from 2 GeV/n to 11 GeV/n for the collisions of the protons and nuclei up to gold. The fixed target and collider NICA experiments will provide data with good quality and enough statistics to clarify physics of this phenomenon.
</Content>
<field id="content">
The study of the heavy ion collisions is of the great interest in high energy physics due to expected phase transition from the nucleons to the quark gluon plasma. But to have a full picture of the effect there is a lack of experimental data at low energy region for the nuclei-nucleus collisions. The goal of the NICA project at JINR is to cover the collision energy range from 2 GeV/n to 11 GeV/n for the collisions of the protons and nuclei up to gold. The fixed target and collider NICA experiments will provide data with good quality and enough statistics to clarify physics of this phenomenon.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Oleg</FirstName>
<FamilyName>Rogachevskiy</FamilyName>
<Email>rogachevsky@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Oleg</FirstName>
<FamilyName>Rogachevskiy</FamilyName>
<Email>rogachevsky@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>Detector &amp; Nuclear Electronics</Track>
</abstract>
<abstract>
<Id>202</Id>
<Title>
Multi-Purpose Detector (MPD) Slow Control System, historical background, present status and plans
</Title>
<Content>
The Multi-Purpose Detector (MPD) is a 4π spectrometer capable of detecting of charged hadrons, electrons and photons in heavy-ion collisions at high luminosity in the energy range of the NICA collider. Among many others one of the crucial tasks necessary for successful operation of such a complex apparatus is providing an adequate monitoring of operational parameters and convenient control of various equipment used in the experiment. In the report presented approaches and basic principles of development of the SlowControl system for the MPD. Tango Control System based approach allows to unify representation and storage of slowcontrol data from many diverse data sources. Presently running BM@N experiment serves as a perfect testbench for the software. Special attention paid to integrity of slowcontrol data and operation stability. Present status and plans of design of slowcontrol system for the MPD is also presented.
</Content>
<field id="content">
The Multi-Purpose Detector (MPD) is a 4π spectrometer capable of detecting of charged hadrons, electrons and photons in heavy-ion collisions at high luminosity in the energy range of the NICA collider. Among many others one of the crucial tasks necessary for successful operation of such a complex apparatus is providing an adequate monitoring of operational parameters and convenient control of various equipment used in the experiment. In the report presented approaches and basic principles of development of the SlowControl system for the MPD. Tango Control System based approach allows to unify representation and storage of slowcontrol data from many diverse data sources. Presently running BM@N experiment serves as a perfect testbench for the software. Special attention paid to integrity of slowcontrol data and operation stability. Present status and plans of design of slowcontrol system for the MPD is also presented.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Vitaly</FirstName>
<FamilyName>Shutov</FamilyName>
<Email>shutov@jinr.ru</Email>
<Affiliation>Borisovich</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Dmitry</FirstName>
<FamilyName>Egorov</FamilyName>
<Email>degorov@jinr.ru</Email>
<Affiliation>JINR VBLHEP</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Peter</FirstName>
<FamilyName>Chumakov</FamilyName>
<Email>peter.chumakov@bk.ru</Email>
<Affiliation>JINR VBLHEP</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Roman V.</FirstName>
<FamilyName>Nagdasev</FamilyName>
<Email>kraken.rus@gmail.com</Email>
<Affiliation>JINR VBLHEP</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Vitaly</FirstName>
<FamilyName>Shutov</FamilyName>
<Email>shutov@jinr.ru</Email>
<Affiliation>Borisovich</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>Triggering, Data Acquisition, Control Systems</Track>
</abstract>
<abstract>
<Id>203</Id>
<Title>
Current status of the geometry database for the CBM experiment
</Title>
<Content>
In this paper we present the current state of developments of the Geometry DB (Geometry Database) for the CBM experiment [1]. At the current moment, the CBM collaboration moves from the stage of prototypes research and tests to the detectors and their components production. A high level control for the manufacturing process is required because of the complexity and high price of the detector components. As a result, there is a need for the development of a database complex for the CBM experiment. In the paper [2] we briefly discussed a complex of Database Management Systems (DBMS) for the CBM collaboration and described a current status of its implementation. The DBMS structure was developed on the basis of databases usage at LHC and other high energy physics experiments [2]. Here we present the current state of developments of the Geometry DB (Geometry Database) for the CBM experiment. The Geometry DB supports the CBM geometry, which describes the CBM experimental setup at the detail level required for simulation of particles transport through the setup using GEANT3 [3]. On the basis of the requirements the Geometry Database [4] has been developed in frameworks of the PostgreSQL and SQLite DBMS. The main purpose of this database is to provide convenient tools for: 1) managing the geometry modules (MVD, STS, RICH, TRD, RPC, ECAL, PSD, Magnet, Beam Pipe); 2) assembling various versions of the CBM setup as a combination of geometry modules and additional files (Field, Materials); 3) providing support of various versions of the CBM setup. It's provided both GUI (Graphical User Interface) and API (Application Programming Interface) tools for CBM users of the Geometry Database. 1. Friman B. et al. Compressed Baryonic Matter in Laboratory Experiments // The CBM Physics Book— 2011. 2. E.P. Akishina, E.I. Alexandrov, I.N. Alexandrov, I.A. Filozova, V. Friese, V.V. Ivanov: Conceptual Consideration for CBM Databases, Communication of JINR, E10-2014-103, Dubna, 2014. 3. GEANT - Detector Description and Simulation Tool, CERN Program Library, Long Write-up, W5013 (1995). 4. User Requirements Document of the Geometry Database for the CBM experiment http://lt-jds.jinr.ru/record/69336?ln=en
</Content>
<field id="content">
In this paper we present the current state of developments of the Geometry DB (Geometry Database) for the CBM experiment [1]. At the current moment, the CBM collaboration moves from the stage of prototypes research and tests to the detectors and their components production. A high level control for the manufacturing process is required because of the complexity and high price of the detector components. As a result, there is a need for the development of a database complex for the CBM experiment. In the paper [2] we briefly discussed a complex of Database Management Systems (DBMS) for the CBM collaboration and described a current status of its implementation. The DBMS structure was developed on the basis of databases usage at LHC and other high energy physics experiments [2]. Here we present the current state of developments of the Geometry DB (Geometry Database) for the CBM experiment. The Geometry DB supports the CBM geometry, which describes the CBM experimental setup at the detail level required for simulation of particles transport through the setup using GEANT3 [3]. On the basis of the requirements the Geometry Database [4] has been developed in frameworks of the PostgreSQL and SQLite DBMS. The main purpose of this database is to provide convenient tools for: 1) managing the geometry modules (MVD, STS, RICH, TRD, RPC, ECAL, PSD, Magnet, Beam Pipe); 2) assembling various versions of the CBM setup as a combination of geometry modules and additional files (Field, Materials); 3) providing support of various versions of the CBM setup. It's provided both GUI (Graphical User Interface) and API (Application Programming Interface) tools for CBM users of the Geometry Database. 1. Friman B. et al. Compressed Baryonic Matter in Laboratory Experiments // The CBM Physics Book— 2011. 2. E.P. Akishina, E.I. Alexandrov, I.N. Alexandrov, I.A. Filozova, V. Friese, V.V. Ivanov: Conceptual Consideration for CBM Databases, Communication of JINR, E10-2014-103, Dubna, 2014. 3. GEANT - Detector Description and Simulation Tool, CERN Program Library, Long Write-up, W5013 (1995). 4. User Requirements Document of the Geometry Database for the CBM experiment http://lt-jds.jinr.ru/record/69336?ln=en
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Evgeny</FirstName>
<FamilyName>Alexandrov</FamilyName>
<Email>aleksand@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Volker</FirstName>
<FamilyName>FRIESE</FamilyName>
<Email>v.friese@gsi.de</Email>
<Affiliation>GSI</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Irina</FirstName>
<FamilyName>Filozova</FamilyName>
<Email>fia@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Igor</FirstName>
<FamilyName>Alexandrov</FamilyName>
<Email>alexand@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Elena</FirstName>
<FamilyName>Akishina</FamilyName>
<Email>aep@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Victor</FirstName>
<FamilyName>Ivanov</FamilyName>
<Email>ivanov@jinr.ru</Email>
<Affiliation>JINR, LIT</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Irina</FirstName>
<FamilyName>Filozova</FamilyName>
<Email>fia@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>
Computing for Large Scale Facilities (LHC, FAIR, NICA, SKA, PIC, XFEL, ELI, etc.)
</Track>
</abstract>
<abstract>
<Id>204</Id>
<Title>
Information Disclosure in Software Intensive Systems
</Title>
<Content>
Research and investigations on computer security problems show that the most malicious problem is the information disclosure. Today this problem is enormous in the context of the new cloud services. The paper is an overview of the main computer security components: attacks, vulnerabilities and weaknesses with a focus on the last ones. An approach to information disclosure weaknesses formalization and its usage for automated weakness’ discovery are discussed.
</Content>
<field id="content">
Research and investigations on computer security problems show that the most malicious problem is the information disclosure. Today this problem is enormous in the context of the new cloud services. The paper is an overview of the main computer security components: attacks, vulnerabilities and weaknesses with a focus on the last ones. An approach to information disclosure weaknesses formalization and its usage for automated weakness’ discovery are discussed.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Vladimir</FirstName>
<FamilyName>Dimitrov</FamilyName>
<Email>cht@fmi.uni-sofia.bg</Email>
<Affiliation>University of Sofia</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Vladimir</FirstName>
<FamilyName>Dimitrov</FamilyName>
<Email>cht@fmi.uni-sofia.bg</Email>
<Affiliation>University of Sofia</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>Distributed Computing. GRID &amp; Cloud Computing</Track>
</abstract>
<abstract>
<Id>205</Id>
<Title>IT for Applied Environmental Research in JINR</Title>
<Content>
The IT in the nuclear research has been focused mainly on mathematical modelling of the nuclear phenomena and on big data analyses. The applied nuclear sciences used for the environmental research brings in a different set of problems where information technologies may significantly improve the research. The ICP Vegetation is an international research program investigating the impacts of air pollutants on crops and (semi-) natural vegetation. Thirty-five parties participate in the program. One of the co-leading institutions of the program is the Frank Laboratory of Nuclear Physics (FLNP) of the JINR. In cooperation with the Laboratory of Information Technologies (LIT) of the JINR, the database system for terrain moss sample data collection and processing was developed. The goal of the research teams from the VŠB-TU Ostrava, the FLNP and the LIT is further development of the database system by adding new functions. These new functions should standardize analyses (statistical toolset) and visualization (GIS toolset) of the samples provided by all research teams.
</Content>
<field id="content">
The IT in the nuclear research has been focused mainly on mathematical modelling of the nuclear phenomena and on big data analyses. The applied nuclear sciences used for the environmental research brings in a different set of problems where information technologies may significantly improve the research. The ICP Vegetation is an international research program investigating the impacts of air pollutants on crops and (semi-) natural vegetation. Thirty-five parties participate in the program. One of the co-leading institutions of the program is the Frank Laboratory of Nuclear Physics (FLNP) of the JINR. In cooperation with the Laboratory of Information Technologies (LIT) of the JINR, the database system for terrain moss sample data collection and processing was developed. The goal of the research teams from the VŠB-TU Ostrava, the FLNP and the LIT is further development of the database system by adding new functions. These new functions should standardize analyses (statistical toolset) and visualization (GIS toolset) of the samples provided by all research teams.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Petr</FirstName>
<FamilyName>Jancik</FamilyName>
<Email>petr.jancik@vsb.cz</Email>
<Affiliation>JINR; VSB - Technical University of Ostrava</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Marina</FirstName>
<FamilyName>Frontasyeva</FamilyName>
<Email>marina@nf.jinr.ru</Email>
<Affiliation>JINR, FLNP</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Jan</FirstName>
<FamilyName>Bitta</FamilyName>
<Email>jan.bitta@vsb.cz</Email>
<Affiliation>VSB-TU Ostrava</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Alexander</FirstName>
<FamilyName>Uzinskiy</FamilyName>
<Email>zalexandr@list.ru</Email>
<Affiliation>JINR, LIT</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>V.</FirstName>
<FamilyName>Svozilik</FamilyName>
<Email>svozilik@jinr.ru</Email>
<Affiliation>Technical University of Ostrava</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Petr</FirstName>
<FamilyName>Jancik</FamilyName>
<Email>petr.jancik@vsb.cz</Email>
<Affiliation>JINR; VSB - Technical University of Ostrava</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>Machine Learning Algorithms and Big Data Analytics</Track>
</abstract>
<abstract>
<Id>206</Id>
<Title>Computing for Large Scale Facilities</Title>
<Content>
Large Scale Science projects normally require massive facilities that are funded through multi-national agreements. The computational requirements for these projects are complex, as the computing technologies have to meet multiple user requirements, and in a scale not yet realised by the current technology trends. The Square Kilometre Array (SKA), CERN etc.. are some examples of the projects, where computing technologies is pushed to the limits. The response of the computer manufacturers and scientific community to such challenges will be covered in this talk. We will also look at the developments in South Africa High Performance Computing and systems in place to ensure efficient support of the SKA computing requirements.
</Content>
<field id="content">
Large Scale Science projects normally require massive facilities that are funded through multi-national agreements. The computational requirements for these projects are complex, as the computing technologies have to meet multiple user requirements, and in a scale not yet realised by the current technology trends. The Square Kilometre Array (SKA), CERN etc.. are some examples of the projects, where computing technologies is pushed to the limits. The response of the computer manufacturers and scientific community to such challenges will be covered in this talk. We will also look at the developments in South Africa High Performance Computing and systems in place to ensure efficient support of the SKA computing requirements.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Happy</FirstName>
<FamilyName>Sithole</FamilyName>
<Email>hsithole@csir.co.za</Email>
<Affiliation>
Centre for High Performance Computing, South Africa
</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Happy</FirstName>
<FamilyName>Sithole</FamilyName>
<Email>hsithole@csir.co.za</Email>
<Affiliation>
Centre for High Performance Computing, South Africa
</Affiliation>
</Speaker>
<ContributionType>Plenary</ContributionType>
<Track>
Computing for Large Scale Facilities (LHC, FAIR, NICA, SKA, PIC, XFEL, ELI, etc.)
</Track>
</abstract>
<abstract>
<Id>207</Id>
<Title>
Clouds of JINR, University of Sofia and INRNE Join Together
</Title>
<Content>
JINR develops a cloud based on OpenNebula that is opened for integration with the clouds from the member states. The paper presents state of the 3 years project that aims to create a backbone of that cloud in Bulgaria. University of Sofia and INRNE participate in that initiative. This is a target project funded by JINR based on the research plan of the institute.
</Content>
<field id="content">
JINR develops a cloud based on OpenNebula that is opened for integration with the clouds from the member states. The paper presents state of the 3 years project that aims to create a backbone of that cloud in Bulgaria. University of Sofia and INRNE participate in that initiative. This is a target project funded by JINR based on the research plan of the institute.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Vladimir</FirstName>
<FamilyName>Korenkov</FamilyName>
<Email>korenkov@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Radoslava</FirstName>
<FamilyName>Goranova</FamilyName>
<Email>rg_test@jinr.ru</Email>
<Affiliation>University of Sofia, Bulgaria</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Kuzma</FirstName>
<FamilyName>Kouzmov</FamilyName>
<Email>kk_test@jinr.ru</Email>
<Affiliation>University of Sofia, Bulgaria</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Nikolay</FirstName>
<FamilyName>Kutovskiy</FamilyName>
<Email>kut@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Nikita</FirstName>
<FamilyName>Balashov</FamilyName>
<Email>balashov.nikita@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Vladimir</FirstName>
<FamilyName>Dimitrov</FamilyName>
<Email>cht@fmi.uni-sofia.bg</Email>
<Affiliation>University of Sofia</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Vladimir</FirstName>
<FamilyName>Dimitrov</FamilyName>
<Email>cht@fmi.uni-sofia.bg</Email>
<Affiliation>University of Sofia</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>Distributed Computing. GRID &amp; Cloud Computing</Track>
</abstract>
<abstract>
<Id>208</Id>
<Title>
Modern SQL and NoSQL database technologies for the ATLAS experiment
</Title>
<Content>
Structured data storage technologies evolve very rapidly in the IT world. LHC experiments, and ATLAS in particular, try to select and use these technologies balancing the performance for a given set of use cases with the availability, ease of use and of getting support, and stability of the product. We definitely and definitively moved from the “one fits all” (or “all has to fit into one”) paradigm to choosing the best solution for each group of data and for the applications that use these data. This talk describes the solutions in use, or under study, for the ATLAS experiment and their selection process and performance.
</Content>
<field id="content">
Structured data storage technologies evolve very rapidly in the IT world. LHC experiments, and ATLAS in particular, try to select and use these technologies balancing the performance for a given set of use cases with the availability, ease of use and of getting support, and stability of the product. We definitely and definitively moved from the “one fits all” (or “all has to fit into one”) paradigm to choosing the best solution for each group of data and for the applications that use these data. This talk describes the solutions in use, or under study, for the ATLAS experiment and their selection process and performance.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Dario</FirstName>
<FamilyName>Barberis</FamilyName>
<Email>dario.barberis@ge.infn.it</Email>
<Affiliation>University and INFN Genova (Italy)</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Dario</FirstName>
<FamilyName>Barberis</FamilyName>
<Email>dario.barberis@ge.infn.it</Email>
<Affiliation>University and INFN Genova (Italy)</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>
Non-relational Databases and Heterogeneous Repositories
</Track>
</abstract>
<abstract>
<Id>209</Id>
<Title>ALFA:  ALICE-FAIR software framework</Title>
<Content>
ALFA is a message queue based framework for online/offline reconstruction. The commonalities between the ALICE and FAIR experiments and their computing requirements led to the development of this framework. Each process in ALFA assumes limited communication and reliance on other processes. Moreover, it does not dictate any application protocols but supports different serialization standards for data exchange between different hardware and software languages, e.g: Protocol Buffers, Flat Buffers, BOOST, MsgPack and ROOT. ALFA has a modular design with separate layers for data transport, process management and deployment, data format, etc. The transport layer in ALFA is called FairMQ, it supports different transports engines like ZeroMQ, nanomsg and shared memory transport. The modular design of ALFA and the interfaces between different layers will be presented.
</Content>
<field id="content">
ALFA is a message queue based framework for online/offline reconstruction. The commonalities between the ALICE and FAIR experiments and their computing requirements led to the development of this framework. Each process in ALFA assumes limited communication and reliance on other processes. Moreover, it does not dictate any application protocols but supports different serialization standards for data exchange between different hardware and software languages, e.g: Protocol Buffers, Flat Buffers, BOOST, MsgPack and ROOT. ALFA has a modular design with separate layers for data transport, process management and deployment, data format, etc. The transport layer in ALFA is called FairMQ, it supports different transports engines like ZeroMQ, nanomsg and shared memory transport. The modular design of ALFA and the interfaces between different layers will be presented.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Mohammad</FirstName>
<FamilyName>Al-Turany</FamilyName>
<Email>mohammad.al-turany@cern.ch</Email>
<Affiliation>GSI/CERN</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Mohammad</FirstName>
<FamilyName>Al-Turany</FamilyName>
<Email>mohammad.al-turany@cern.ch</Email>
<Affiliation>GSI/CERN</Affiliation>
</Speaker>
<ContributionType>Plenary</ContributionType>
<Track>
Computing for Large Scale Facilities (LHC, FAIR, NICA, SKA, PIC, XFEL, ELI, etc.)
</Track>
</abstract>
<abstract>
<Id>210</Id>
<Title>GRID and Cloud Computing at IHEP in China</Title>
<Content>
The distributed computing system at Institute of High Energy Physics (IHEP), Chinese Academy of Sciences, was firstly built based on DIRAC in 2013 and put into production in 2014. This presentation will introduce the development and latest status of this system: the DIRAC-based WMS was extended to support multi-VO scheduling based on VOMS; the general-purpose task submission and management tool was developed to ease the process of bulk submission and management of experiment-specific jobs with modular designs and customized workflow; To support multi-core jobs, different multi-core job scheduling methods have been tested, and their performance have been compared; To monitor and manage the heterogeneous resources in a uniform way, the resources monitoring and automatic management system has been implemented based on the Resource Status Service of DIRAC. The cloud computing provides a new way for high energy physics applications to access a shared pool of configurable computing resources. Based on the requirements from our domestic experiments, IHEP launched a cloud computing project, IHEPCloud, in 2014. This presentation will also introduce the status of IHEPCloud and some ongoing R&amp;D work including resource scheduler based on the affinity model, integration of SDN with OpenStack to achieve configuration flexibility, and performance evaluation etc.
</Content>
<field id="content">
The distributed computing system at Institute of High Energy Physics (IHEP), Chinese Academy of Sciences, was firstly built based on DIRAC in 2013 and put into production in 2014. This presentation will introduce the development and latest status of this system: the DIRAC-based WMS was extended to support multi-VO scheduling based on VOMS; the general-purpose task submission and management tool was developed to ease the process of bulk submission and management of experiment-specific jobs with modular designs and customized workflow; To support multi-core jobs, different multi-core job scheduling methods have been tested, and their performance have been compared; To monitor and manage the heterogeneous resources in a uniform way, the resources monitoring and automatic management system has been implemented based on the Resource Status Service of DIRAC. The cloud computing provides a new way for high energy physics applications to access a shared pool of configurable computing resources. Based on the requirements from our domestic experiments, IHEP launched a cloud computing project, IHEPCloud, in 2014. This presentation will also introduce the status of IHEPCloud and some ongoing R&amp;D work including resource scheduler based on the affinity model, integration of SDN with OpenStack to achieve configuration flexibility, and performance evaluation etc.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Weidong</FirstName>
<FamilyName>Li</FamilyName>
<Email>liwd@ihep.ac.cn</Email>
<Affiliation>IHEP, Beijing</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Weidong</FirstName>
<FamilyName>Li</FamilyName>
<Email>liwd@ihep.ac.cn</Email>
<Affiliation>IHEP, Beijing</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Distributed Computing. GRID &amp; Cloud Computing</Track>
</abstract>
<abstract>
<Id>211</Id>
<Title>
Resource sharing based on HTCondor for multiple experiments
</Title>
<Content>
HTCondor, a scheduler focusing on high throughput computing has been more and more popular in high energy physics computing. The HTCondor cluster with more than 10,000 cpu cores running at computing center, institute of high energy physics in China, supports several HEP experiments, such as JUNO, BES, Atlas, Cms etc. The work nodes owned by the experiments are managed by HTCondor. A sharing pool including the work nodes contributed by all HEP experiments has been created to meet the peak computing requirement from the different experiments during different time periods. To manage the sharing pool, a database is used to store the cluster’s information including nodes and groups attributes. The attributes can be adjusted by the cluster manager and published to both scheduler servers and work nodes via http protocol. A monitoring dog is developed to monitor the work nodes health status and report to the database. Both servers and work nodes update their own configuration based on the attributes published by the database. The whole resource utilization rate of the cluster has been promoted from 50% to more than 80% after the sharing pool is created.
</Content>
<field id="content">
HTCondor, a scheduler focusing on high throughput computing has been more and more popular in high energy physics computing. The HTCondor cluster with more than 10,000 cpu cores running at computing center, institute of high energy physics in China, supports several HEP experiments, such as JUNO, BES, Atlas, Cms etc. The work nodes owned by the experiments are managed by HTCondor. A sharing pool including the work nodes contributed by all HEP experiments has been created to meet the peak computing requirement from the different experiments during different time periods. To manage the sharing pool, a database is used to store the cluster’s information including nodes and groups attributes. The attributes can be adjusted by the cluster manager and published to both scheduler servers and work nodes via http protocol. A monitoring dog is developed to monitor the work nodes health status and report to the database. Both servers and work nodes update their own configuration based on the attributes published by the database. The whole resource utilization rate of the cluster has been promoted from 50% to more than 80% after the sharing pool is created.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Jingyan</FirstName>

<FamilyName>Shi</FamilyName>
<Email>shijy@ihep.ac.cn</Email>
<Affiliation>
INSTITUTE OF HIGH ENERGY PHYSICS, Chinese Academy of Science
</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Jingyan</FirstName>
<FamilyName>Shi</FamilyName>
<Email>shijy@ihep.ac.cn</Email>
<Affiliation>
INSTITUTE OF HIGH ENERGY PHYSICS, Chinese Academy of Science
</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>
Computing for Large Scale Facilities (LHC, FAIR, NICA, SKA, PIC, XFEL, ELI, etc.)
</Track>
</abstract>
<abstract>
<Id>212</Id>
<Title>STAR's GRID Production Framework</Title>
<Content>
STAR's RHIC computing facility provides over 15K dedicated slots for data reconstruction. However this number of slots is not always sufficient to satisfy an ambitious and data challenging Physics program and harvesting resources from outside facilities is paramount to scientific success. However, constraints of remote sites (CPU time limit) do not always always provide the flexibility of a dedicated farm. Though, experiments like STAR have a breadth of smaller datasets (both in Runtime and size) that can be easily offloaded to remote facilities. Scavenged resources optimizes local efficiency and contributes additional computing time to an experiment that runs every year and therefor needs fast turnaround. We will discuss STAR's software stack of our GRID production framework including features dealing with multi-site submission, automated re-submission, job trackingas well as new challenges and possible improvements.
</Content>
<field id="content">
STAR's RHIC computing facility provides over 15K dedicated slots for data reconstruction. However this number of slots is not always sufficient to satisfy an ambitious and data challenging Physics program and harvesting resources from outside facilities is paramount to scientific success. However, constraints of remote sites (CPU time limit) do not always always provide the flexibility of a dedicated farm. Though, experiments like STAR have a breadth of smaller datasets (both in Runtime and size) that can be easily offloaded to remote facilities. Scavenged resources optimizes local efficiency and contributes additional computing time to an experiment that runs every year and therefor needs fast turnaround. We will discuss STAR's software stack of our GRID production framework including features dealing with multi-site submission, automated re-submission, job trackingas well as new challenges and possible improvements.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Levente</FirstName>
<FamilyName>Hajdu</FamilyName>
<Email>lbhajdu@bnl.gov</Email>
<Affiliation>BNL</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Yury</FirstName>
<FamilyName>Panebrattsev</FamilyName>
<Email>yuri@intergraphics.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Jerome</FirstName>
<FamilyName>LAURET</FamilyName>
<Email>jlauret@bnl.gov</Email>
<Affiliation>BNL</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Evgeniy</FirstName>
<FamilyName>KUZNETSOV</FamilyName>
<Email>evgk@test.ru</Email>
<Affiliation>JINR LIT</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Valery</FirstName>
<FamilyName>Mitsyn</FamilyName>
<Email>vvm@mammoth.jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Lidia</FirstName>
<FamilyName>Didenko</FamilyName>
<Email>ld@bnl.gov</Email>
<Affiliation>BNL</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Wayne</FirstName>
<FamilyName>BETTS</FamilyName>
<Email>betts@bnl.gov</Email>
<Affiliation>BNL</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Levente</FirstName>
<FamilyName>Hajdu</FamilyName>
<Email>lbhajdu@bnl.gov</Email>
<Affiliation>BNL</Affiliation>
</Speaker>
<ContributionType>Plenary</ContributionType>
<Track>Distributed Computing. GRID &amp; Cloud Computing</Track>
</abstract>
<abstract>
<Id>213</Id>
<Title>
HybriLIT - the main component of the MICC for heterogeneous computations at JINR
</Title>
<Content>
The decision to extend the JINR MICC by adding a heterogeneous computing cluster devoted to the high performance computing (HPC) in JINR and the JINR Member States follows the modern trends in the world wide computing technologies. Implementation of a heterogeneous cluster that includes nodes with CPU, GPU and MIC-architecture into MICC JINR infrastructure allows follow two main direction of computation acceleration development. It provides users of the cluster with possibilities to use available HPC means. The analysis of the needs for high performance computing in JINR resulted in the definition of three basic tasks to be solved by the heterogeneous HPC cluster: design and implementation of parallel software for computing intensive research; porting to the cluster open software packages and numerical libraries which are already tuned for hybrid architectures; development of new mathematical methods and parallel algorithms adapted to heterogeneous architectures. At the moment, the computing component of the cluster contains four nodes with NVIDIA Tesla K80 graphical processors and four nodes with NVIDIA Tesla K40 and K80 accelerators, a node with Intel Xeon Phi7120P coprocessors and a node with two types of computing accelerators NVIDIA Tesla K20x and Intel Xeon Phi 5110P. All the nodes have two multi-core processors Intel Xeon. Overall, the cluster contains 252 CPU cores, 77184 GPU cores, 182 PHI-cores, it has 2.4 TB RAM and 57.6 TB HDD, and its total capacity is 142 Tflops for operations with single precision and 50 TFlops for double precision. A new component was included in the cluster structure - a virtual desktop system to support users work with applied packages. Deployed was a polygon of eight servers where on the basis of KVM (Kernel-based Virtual Machine) virtual desktops with remote access to the package COMSOL Multiphysics for user groups have been designed. The developed new cluster’s component allows the users to effectively utilize the cluster’s resources conducting intensive calculations from the applied software packages on the computing nodes of the cluster. The software and information environment of the cluster was actively maintained and developed allowing its users to develop software applications, to perform calculations with the help of the latest computational architectures. The total number of its users is 450 people from the JINR laboratories and the JINR Member States. In particular, the cluster resources are used for calculations in the field of quantum chromodynamics, quantum mechanics and molecular dynamics. In addition, software PandaRoot, MpdRoot installed on the cluster allows one to perform calculations in high energy physics. The heterogeneous cluster HybriLIT is used both to perform massively parallel computations and to learn how to use applied software packages and parallel programming technologies. During 2016, over 20 training courses were held which were attended by over 200 specialists from various departments of the Institute, young scientists from the JINR Member States and Russia universities.
</Content>
<field id="content">
The decision to extend the JINR MICC by adding a heterogeneous computing cluster devoted to the high performance computing (HPC) in JINR and the JINR Member States follows the modern trends in the world wide computing technologies. Implementation of a heterogeneous cluster that includes nodes with CPU, GPU and MIC-architecture into MICC JINR infrastructure allows follow two main direction of computation acceleration development. It provides users of the cluster with possibilities to use available HPC means. The analysis of the needs for high performance computing in JINR resulted in the definition of three basic tasks to be solved by the heterogeneous HPC cluster: design and implementation of parallel software for computing intensive research; porting to the cluster open software packages and numerical libraries which are already tuned for hybrid architectures; development of new mathematical methods and parallel algorithms adapted to heterogeneous architectures. At the moment, the computing component of the cluster contains four nodes with NVIDIA Tesla K80 graphical processors and four nodes with NVIDIA Tesla K40 and K80 accelerators, a node with Intel Xeon Phi7120P coprocessors and a node with two types of computing accelerators NVIDIA Tesla K20x and Intel Xeon Phi 5110P. All the nodes have two multi-core processors Intel Xeon. Overall, the cluster contains 252 CPU cores, 77184 GPU cores, 182 PHI-cores, it has 2.4 TB RAM and 57.6 TB HDD, and its total capacity is 142 Tflops for operations with single precision and 50 TFlops for double precision. A new component was included in the cluster structure - a virtual desktop system to support users work with applied packages. Deployed was a polygon of eight servers where on the basis of KVM (Kernel-based Virtual Machine) virtual desktops with remote access to the package COMSOL Multiphysics for user groups have been designed. The developed new cluster’s component allows the users to effectively utilize the cluster’s resources conducting intensive calculations from the applied software packages on the computing nodes of the cluster. The software and information environment of the cluster was actively maintained and developed allowing its users to develop software applications, to perform calculations with the help of the latest computational architectures. The total number of its users is 450 people from the JINR laboratories and the JINR Member States. In particular, the cluster resources are used for calculations in the field of quantum chromodynamics, quantum mechanics and molecular dynamics. In addition, software PandaRoot, MpdRoot installed on the cluster allows one to perform calculations in high energy physics. The heterogeneous cluster HybriLIT is used both to perform massively parallel computations and to learn how to use applied software packages and parallel programming technologies. During 2016, over 20 training courses were held which were attended by over 200 specialists from various departments of the Institute, young scientists from the JINR Member States and Russia universities.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Dmitry</FirstName>
<FamilyName>Podgainy</FamilyName>
<Email>podgainy@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Gheorghe</FirstName>
<FamilyName>Adam</FamilyName>
<Email>adamg@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Vladimir</FirstName>
<FamilyName>Korenkov</FamilyName>
<Email>korenkov@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Oksana</FirstName>
<FamilyName>Streltsova</FamilyName>
<Email>strel@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Tatiana</FirstName>
<FamilyName>Strizh</FamilyName>
<Email>strizh@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Petr</FirstName>
<FamilyName>Zrelov</FamilyName>
<Email>zrelov@jinr.ru</Email>
<Affiliation>LIT JINR</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Dmitry</FirstName>
<FamilyName>Podgainy</FamilyName>
<Email>podgainy@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>
Computations with Hybrid Systems (CPU, GPU, coprocessors)
</Track>
</abstract>
<abstract>
<Id>214</Id>
<Title>Belle II Distributed Computing</Title>
<Content>
The existence of large matter-antimatter asymmetry (CP violation) in the b-quark system as predicted in the Kobayashi-Maskawa theory was established by the B-Factory experiments, Belle and BaBar. However, this cannot explain the magnitude of the matter-antimatter asymmetry of the universe we live in today which indicates undiscovered new physics exists. The Belle II experiment at the SuperKEKB collider in Tsukuba, Japan is designed to investigate for new new physics starting in early 2019 and aims to collect 50 times more data than the Belle experiment. At designed luminosity the Belle II computing must handle tens of PetaByte per year and an estimated aggregate of 350 PB of data. Computing at this scale requires efficient and coordinated use of the compute grids in North America, Asia and Europe and will take advantage of high-speed global networks. We present the general Belle II computing model, the production system with a focus on the distributed data management system. Additionally, we present how U.S. Belle II is using virtualization techniques to augment computing resources by leveraging Leadership Class Facilities (LCFs).
</Content>
<field id="content">
The existence of large matter-antimatter asymmetry (CP violation) in the b-quark system as predicted in the Kobayashi-Maskawa theory was established by the B-Factory experiments, Belle and BaBar. However, this cannot explain the magnitude of the matter-antimatter asymmetry of the universe we live in today which indicates undiscovered new physics exists. The Belle II experiment at the SuperKEKB collider in Tsukuba, Japan is designed to investigate for new new physics starting in early 2019 and aims to collect 50 times more data than the Belle experiment. At designed luminosity the Belle II computing must handle tens of PetaByte per year and an estimated aggregate of 350 PB of data. Computing at this scale requires efficient and coordinated use of the compute grids in North America, Asia and Europe and will take advantage of high-speed global networks. We present the general Belle II computing model, the production system with a focus on the distributed data management system. Additionally, we present how U.S. Belle II is using virtualization techniques to augment computing resources by leveraging Leadership Class Facilities (LCFs).
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Malachi</FirstName>
<FamilyName>Schramm</FamilyName>
<Email>malachi.schram@pnnl.gov</Email>
<Affiliation>PNNL</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Malachi</FirstName>
<FamilyName>Schramm</FamilyName>
<Email>malachi.schram@pnnl.gov</Email>
<Affiliation>PNNL</Affiliation>
</Speaker>
<ContributionType>Plenary</ContributionType>
<Track>
Computing for Large Scale Facilities (LHC, FAIR, NICA, SKA, PIC, XFEL, ELI, etc.)
</Track>
</abstract>
<abstract>
<Id>215</Id>
<Title>
Quantum Relativistic Informatics: Computer Science, Quantum Intelligent Cognitive Control and Smart Robotics Applications
</Title>
<Content>
Quantum relativistic mechanics, quantum thermodynamics and quantum relativistic information theory laws are the background of quantum relativistic informatics. Quantum computing, quantum programming, and quantum algorithm theories are oriented on simulation of quantum relativistic (open) dynamic systems using future quantum computer (Feymann &amp; Manin). Unconventional computational intelligence toolkit for simulation dynamic quantum relativistic system as relativistic navigation of unmanned air vehicle (UAV) and control of quantum relativistic particle with spin ½ (Dirac equation) in gravitation field on classical computer based on IT design of quantum algorithmic gates is developed. We consider any Benchmarks of applications of quantum relativistic informatics. As example from computer science modified Grover’s quantum search algorithm is considered that can be effectively realized on classical computer. Application for extraction of knowledge from big data for model presentation of physical processes is considered. Quantum relativistic logic of physical theory of model correctness interpretation is discussed. Intelligent control of quantum and relativistic dynamic system as example from control systems theory is demonstrated based on the model of quantum fuzzy inferences and corresponding computational intelligence toolkit. Effective applications of developed toolkit presented with intelligent cognitive control of mobile smart robots based on “brain-computer” neurointerface and new toolkit as Soft &amp; Quantum Computing Optimizer of controller’s knowledge base. Experimental results (see figure below) of quantum fuzzy inference application in intelligent cognitive control of mobile collective robots with video are demonstrated. The results are developed in mutual R&amp;D project between Laboratory of Information Technologies, JINR (V.V. Korenkov), Veksler and Baldin Laboratory of High Energy Physics, JINR (G.P. Reshetnikov) and Dubna State University (S.V. Ulyanov).
</Content>
<field id="content">
Quantum relativistic mechanics, quantum thermodynamics and quantum relativistic information theory laws are the background of quantum relativistic informatics. Quantum computing, quantum programming, and quantum algorithm theories are oriented on simulation of quantum relativistic (open) dynamic systems using future quantum computer (Feymann &amp; Manin). Unconventional computational intelligence toolkit for simulation dynamic quantum relativistic system as relativistic navigation of unmanned air vehicle (UAV) and control of quantum relativistic particle with spin ½ (Dirac equation) in gravitation field on classical computer based on IT design of quantum algorithmic gates is developed. We consider any Benchmarks of applications of quantum relativistic informatics. As example from computer science modified Grover’s quantum search algorithm is considered that can be effectively realized on classical computer. Application for extraction of knowledge from big data for model presentation of physical processes is considered. Quantum relativistic logic of physical theory of model correctness interpretation is discussed. Intelligent control of quantum and relativistic dynamic system as example from control systems theory is demonstrated based on the model of quantum fuzzy inferences and corresponding computational intelligence toolkit. Effective applications of developed toolkit presented with intelligent cognitive control of mobile smart robots based on “brain-computer” neurointerface and new toolkit as Soft &amp; Quantum Computing Optimizer of controller’s knowledge base. Experimental results (see figure below) of quantum fuzzy inference application in intelligent cognitive control of mobile collective robots with video are demonstrated. The results are developed in mutual R&amp;D project between Laboratory of Information Technologies, JINR (V.V. Korenkov), Veksler and Baldin Laboratory of High Energy Physics, JINR (G.P. Reshetnikov) and Dubna State University (S.V. Ulyanov).
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Sergey</FirstName>
<FamilyName>Ulyanov</FamilyName>
<Email>ulyanovsv@mail.ru</Email>
<Affiliation>
1Institute of System Analysis and Control, Dubna State University
</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Gennady</FirstName>
<FamilyName>Reshetnikov</FamilyName>
<Email>genresh@mail.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Vladimir</FirstName>
<FamilyName>Korenkov</FamilyName>
<Email>korenkov@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Sergey</FirstName>
<FamilyName>Ulyanov</FamilyName>
<Email>ulyanovsv@mail.ru</Email>
<Affiliation>
1Institute of System Analysis and Control, Dubna State University
</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>
Advanced Technologies for the High-Intensity Domains of Science and Business Applications
</Track>
</abstract>
<abstract>
<Id>217</Id>
<Title>
Automated system to monitor and predict matching of vocational education programs with labour market
</Title>
<Content>
Interaction of labour market and educational system is a complex process, with many parties involved (government, universities, employers, individuals, etc.). Both horizontal and vertical mismatch between skills and qualifications from one side and market’s requirements from another are still widely observed in both developing and developed countries. To discover both qualitative and quantitative correlations between education system and labour market in a reasonable time, we proposed an intellectual system to monitor the demands of employers and match them with the educational standards and programs. The analysis is based on stringing together job requirements and single competencies from the educational standards, the lowest levels of the models of the labour market and the education system correspondingly. To automate the processing as muсh as possible, we used machine learning technologies for semantic parsing. Creation of semantic models is one of the well-known key problems of natural language processing. Since the wording in both requirements and competencies details usually consist of about 10 words, calculation of semantic distance between short sentences lies at the very core of the method. For our task, the way proposed was to deal with the vector representation of words and short sentences. Big Data approaches and technologies are in use for collecting and processing the data. The system being created allows to estimate a need for specific professions for regions, to consider matching of the professional standards with real market jobs, to plan the number of funded places in colleges and universities. Having historical data, it is possible to not only determine current expectations of labour market from the education system, but make some further predictions.
</Content>
<field id="content">
Interaction of labour market and educational system is a complex process, with many parties involved (government, universities, employers, individuals, etc.). Both horizontal and vertical mismatch between skills and qualifications from one side and market’s requirements from another are still widely observed in both developing and developed countries. To discover both qualitative and quantitative correlations between education system and labour market in a reasonable time, we proposed an intellectual system to monitor the demands of employers and match them with the educational standards and programs. The analysis is based on stringing together job requirements and single competencies from the educational standards, the lowest levels of the models of the labour market and the education system correspondingly. To automate the processing as muсh as possible, we used machine learning technologies for semantic parsing. Creation of semantic models is one of the well-known key problems of natural language processing. Since the wording in both requirements and competencies details usually consist of about 10 words, calculation of semantic distance between short sentences lies at the very core of the method. For our task, the way proposed was to deal with the vector representation of words and short sentences. Big Data approaches and technologies are in use for collecting and processing the data. The system being created allows to estimate a need for specific professions for regions, to consider matching of the professional standards with real market jobs, to plan the number of funded places in colleges and universities. Having historical data, it is possible to not only determine current expectations of labour market from the education system, but make some further predictions.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Sergey</FirstName>
<FamilyName>Belov</FamilyName>
<Email>belov@jinr.ru</Email>
<Affiliation>Joint Institute for Nuclear Research</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Irina</FirstName>
<FamilyName>Filozova</FamilyName>
<Email>fia@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Ivan</FirstName>
<FamilyName>Kadochnikov</FamilyName>
<Email>kadivas@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Vladimir</FirstName>
<FamilyName>Korenkov</FamilyName>
<Email>korenkov@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Roman</FirstName>
<FamilyName>Semenov</FamilyName>
<Email>roman@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Petr</FirstName>
<FamilyName>Zrelov</FamilyName>
<Email>zrelov@jinr.ru</Email>
<Affiliation>LIT JINR</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Sergey</FirstName>
<FamilyName>Belov</FamilyName>
<Email>belov@jinr.ru</Email>
<Affiliation>Joint Institute for Nuclear Research</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>Machine Learning Algorithms and Big Data Analytics</Track>
</abstract>
<abstract>
<Id>218</Id>
<Title>
Combined analysis of storage and CPU resources at CERN
</Title>
<Content>
CERN provides a significant part of the storage and cpu resources used for LHC analysis and  is, similar to many other WLCG sites, preparing for a significant requirement increase in LHC run 3.  In this context, an analysis working group has been formed at CERN IT with the goal to enhance  science throughput by increasing the efficiency of storage and cpu services via a systematic statistical  analysis of operational metrics. Starting from a more quantitative understanding of the use of available IT  resources, we aim to support a joined optimisation with the LHC experiments and the joined planning  of upcoming investments. I this talk we will describe the Hadoop based infrastructure used for preprocessing medium and  long term (1 - 48 months) metric collections, some of the various tools used for aggregate performance  analysis and prediction and we will conclude with some results obtained with this new infrastructure.
</Content>
<field id="content">
CERN provides a significant part of the storage and cpu resources used for LHC analysis and  is, similar to many other WLCG sites, preparing for a significant requirement increase in LHC run 3.  In this context, an analysis working group has been formed at CERN IT with the goal to enhance  science throughput by increasing the efficiency of storage and cpu services via a systematic statistical  analysis of operational metrics. Starting from a more quantitative understanding of the use of available IT  resources, we aim to support a joined optimisation with the LHC experiments and the joined planning  of upcoming investments. I this talk we will describe the Hadoop based infrastructure used for preprocessing medium and  long term (1 - 48 months) metric collections, some of the various tools used for aggregate performance  analysis and prediction and we will conclude with some results obtained with this new infrastructure.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Dirk</FirstName>
<FamilyName>Duellmann</FamilyName>
<Email>dirk.duellmann@cern.ch</Email>
<Affiliation>CERN</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Dirk</FirstName>
<FamilyName>Duellmann</FamilyName>
<Email>dirk.duellmann@cern.ch</Email>
<Affiliation>CERN</Affiliation>
</Speaker>
<ContributionType>Plenary</ContributionType>
<Track>
Computing for Large Scale Facilities (LHC, FAIR, NICA, SKA, PIC, XFEL, ELI, etc.)
</Track>
</abstract>
<abstract>
<Id>219</Id>
<Title>
Lightweight Sites in the Worldwide LHC Computing Grid
</Title>
<Content>
One of the goals of the WLCG Operations Coordination activities is to help simplify what the majority of the WLCG sites, i.e. the smaller ones, need to accomplish to be able to contribute resources in a useful manner, i.e. with large benefits compared to efforts invested. This contribution describes different areas of activities which aim to allow sites to be run with minimal oversight and operational effort from people at the sites themselves as well as beyond. These areas include several scenarios for deployment and management of the site resources, multiple paradigms for providing resources in general, both for compute and data management, as well as R&amp;D activities involving data mining and machine learning algorithms for a better understanding of the monitoring and logging information, up to trend analysis which would allow further automation of operational tasks.
</Content>
<field id="content">
One of the goals of the WLCG Operations Coordination activities is to help simplify what the majority of the WLCG sites, i.e. the smaller ones, need to accomplish to be able to contribute resources in a useful manner, i.e. with large benefits compared to efforts invested. This contribution describes different areas of activities which aim to allow sites to be run with minimal oversight and operational effort from people at the sites themselves as well as beyond. These areas include several scenarios for deployment and management of the site resources, multiple paradigms for providing resources in general, both for compute and data management, as well as R&amp;D activities involving data mining and machine learning algorithms for a better understanding of the monitoring and logging information, up to trend analysis which would allow further automation of operational tasks.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Maarten</FirstName>
<FamilyName>Litmaath</FamilyName>
<Email>maarten.litmaath@cern.ch</Email>
<Affiliation>CERN</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Maarten</FirstName>
<FamilyName>Litmaath</FamilyName>
<Email>maarten.litmaath@cern.ch</Email>
<Affiliation>CERN</Affiliation>
</Speaker>
<ContributionType>Plenary</ContributionType>
<Track>Distributed Computing. GRID &amp; Cloud Computing</Track>
</abstract>
<abstract>
<Id>220</Id>
<Title>Evolution of tools for WLCG operations</Title>
<Content>
The WLCG infrastructure combines computing resources of more than 170 centers in 42 countries all over the world. Smooth operations of so huge and heterogeneous infrastructure is a complicated task which is performed by a distributed team. Constant growth of the amount of the computing resources and technology evolution which introduces new types of the resources like HPC and commercial clouds with the simultaneous decrease of the effort which can be dedicated to the operational tasks represent a challenge for the WLCG operations. The contribution will describe current development of the systems used for WLCG operations which include monitoring , accounting and information system.
</Content>
<field id="content">
The WLCG infrastructure combines computing resources of more than 170 centers in 42 countries all over the world. Smooth operations of so huge and heterogeneous infrastructure is a complicated task which is performed by a distributed team. Constant growth of the amount of the computing resources and technology evolution which introduces new types of the resources like HPC and commercial clouds with the simultaneous decrease of the effort which can be dedicated to the operational tasks represent a challenge for the WLCG operations. The contribution will describe current development of the systems used for WLCG operations which include monitoring , accounting and information system.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Julia</FirstName>
<FamilyName>Andreeva</FamilyName>
<Email>julia.andreeva@cern.ch</Email>
<Affiliation>CERN</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Julia</FirstName>
<FamilyName>Andreeva</FamilyName>
<Email>julia.andreeva@cern.ch</Email>
<Affiliation>CERN</Affiliation>
</Speaker>
<ContributionType>Plenary</ContributionType>
<Track>Distributed Computing. GRID &amp; Cloud Computing</Track>
</abstract>
<abstract>
<Id>221</Id>
<Title>
Computing Resource Information Catalog: the ATLAS Grid Information system evolution for other communities
</Title>
<Content>
The Worldwide LHC Computing Grid infrastructure links about 200 participating computing centers affiliated with several partner projects. It is built by integrating heterogeneous computer and storage resources in diverse data centers all over the world and provides CPU and storage capacity to the LHC experiments to perform data processing and physics analysis. In order to be used by the LHC experiments a central information system is required. It should provide description of the topology of the WLCG infrastructure as well as configuration data which are needed by the various WLCG software components and Experiment oriented services and applications. This contribution describes the evolution of the ATLAS Grid Information system (AGIS) into common Computing Resource Information Catalog (CRIC), the framework which is designed to describe the topology of the LHC Experiments computing models, providing unified description of resources and services used by experiments applications. CRIC collects information from various information providers (like GOCDB, OIM, central BDII and experiment-specific information systems), performs validation and provides a consistent set of WebUI and API to the LHC VOs for service discovery and usage configuration. The main requirements for CRIC are simplicity, agility and robustness. CRIC should be able to be quickly adapted to new types of computing resources, new information sources, and allow for new data structures to be implemented easily following the evolution of the computing models and operations of the experiments. Experiments are more and more relying on newer types of resources like opportunistic cloud or HPC resources, which by their nature are more dynamic and not integrated in any WLCG existing framework: also these resources need to be described in CRIC to allow the experiments to effectively exploit them. The implementation of CRIC was inspired by the successful experience with the ATLAS Grid Information System. In this note we describe recent developments of CRIC functionalities, definition of the main information model and the overall architecture of the system, which in particular provides clean functional decoupling between physical distributed computing capabilities and resources used by particular experiment into two parts: ● A core part which describes all physical service endpoints and provides a single entry point for experiments service discovery. ● Optional Experiment-specific extensions, implemented as plugins. They describe how the physical resources are used by the experiments and contain additional attributes and configuration which are required by the experiments for operations and organization of their data and workflows. CRIC not only provides a current view of the WLCG infrastructure, but also keeps track of performed changes and audit information. Its administration interface allows authorized users to make changes. Authentication and authorization are subject to experiment policies in terms of data access and update privileges.
</Content>
<field id="content">
The Worldwide LHC Computing Grid infrastructure links about 200 participating computing centers affiliated with several partner projects. It is built by integrating heterogeneous computer and storage resources in diverse data centers all over the world and provides CPU and storage capacity to the LHC experiments to perform data processing and physics analysis. In order to be used by the LHC experiments a central information system is required. It should provide description of the topology of the WLCG infrastructure as well as configuration data which are needed by the various WLCG software components and Experiment oriented services and applications. This contribution describes the evolution of the ATLAS Grid Information system (AGIS) into common Computing Resource Information Catalog (CRIC), the framework which is designed to describe the topology of the LHC Experiments computing models, providing unified description of resources and services used by experiments applications. CRIC collects information from various information providers (like GOCDB, OIM, central BDII and experiment-specific information systems), performs validation and provides a consistent set of WebUI and API to the LHC VOs for service discovery and usage configuration. The main requirements for CRIC are simplicity, agility and robustness. CRIC should be able to be quickly adapted to new types of computing resources, new information sources, and allow for new data structures to be implemented easily following the evolution of the computing models and operations of the experiments. Experiments are more and more relying on newer types of resources like opportunistic cloud or HPC resources, which by their nature are more dynamic and not integrated in any WLCG existing framework: also these resources need to be described in CRIC to allow the experiments to effectively exploit them. The implementation of CRIC was inspired by the successful experience with the ATLAS Grid Information System. In this note we describe recent developments of CRIC functionalities, definition of the main information model and the overall architecture of the system, which in particular provides clean functional decoupling between physical distributed computing capabilities and resources used by particular experiment into two parts: ● A core part which describes all physical service endpoints and provides a single entry point for experiments service discovery. ● Optional Experiment-specific extensions, implemented as plugins. They describe how the physical resources are used by the experiments and contain additional attributes and configuration which are required by the experiments for operations and organization of their data and workflows. CRIC not only provides a current view of the WLCG infrastructure, but also keeps track of performed changes and audit information. Its administration interface allows authorized users to make changes. Authentication and authorization are subject to experiment policies in terms of data access and update privileges.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Alexey</FirstName>
<FamilyName>Anisenkov</FamilyName>
<Email>alexey.anisenkov@cern.ch</Email>
<Affiliation>BINP</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Alexey</FirstName>
<FamilyName>Anisenkov</FamilyName>
<Email>alexey.anisenkov@cern.ch</Email>
<Affiliation>BINP</Affiliation>
</Speaker>
<ContributionType>Plenary</ContributionType>
<Track>Research Data Infrastructures</Track>
</abstract>
<abstract>
<Id>222</Id>
<Title>Experience with containers for OSG NovA jobs</Title>
<Content>
Today's physics experiments strongly rely on computing not only during data taking periods, but huge amount of computing resources is necessary later for offline data analysis to obtain precise physics measurements out of the enormous amount of recorded raw data and Monte-Carlo simulations. Large collaborations with members from many countries are essential for successful research on complex experimental infrastructure and within such organization it was natural to use distributed computing model. Besides broad physics program Institute of Physics of the Czech Academy of Sciences (FZU) also serves as a regional computing center that supports grid computing for several big experiments (WLCG, OSG, ...) and local user's analysis. It is becoming difficult to provide optimal uniform computing environment to the growing number of supported user groups and their different or even contradictory requirements. Also we would like to explore new features that comes with modern systems, but often software used by experiments is not certified for latest version and experiments in their final phase doesn't really want any changes in their computing environment. To satisfy all our users requirements, most efficient use of modern hardware and optimal utilization of all resources in our cluster we decided to upgrade our local batch system to HTCondor. HTCondor provides us means to run jobs in isolated and per experiment specific environment by utilizing lightweight container technology. With jobs running in containers we can still accept OSG NOvA grid jobs while at the same time we install modern OS on our new hardware.
</Content>
<field id="content">
Today's physics experiments strongly rely on computing not only during data taking periods, but huge amount of computing resources is necessary later for offline data analysis to obtain precise physics measurements out of the enormous amount of recorded raw data and Monte-Carlo simulations. Large collaborations with members from many countries are essential for successful research on complex experimental infrastructure and within such organization it was natural to use distributed computing model. Besides broad physics program Institute of Physics of the Czech Academy of Sciences (FZU) also serves as a regional computing center that supports grid computing for several big experiments (WLCG, OSG, ...) and local user's analysis. It is becoming difficult to provide optimal uniform computing environment to the growing number of supported user groups and their different or even contradictory requirements. Also we would like to explore new features that comes with modern systems, but often software used by experiments is not certified for latest version and experiments in their final phase doesn't really want any changes in their computing environment. To satisfy all our users requirements, most efficient use of modern hardware and optimal utilization of all resources in our cluster we decided to upgrade our local batch system to HTCondor. HTCondor provides us means to run jobs in isolated and per experiment specific environment by utilizing lightweight container technology. With jobs running in containers we can still accept OSG NOvA grid jobs while at the same time we install modern OS on our new hardware.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Petr</FirstName>
<FamilyName>Vokac</FamilyName>
<Email>vokac@fnal.gov</Email>
<Affiliation>
Institute of Physics of the Czech Academy of Sciences
</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Petr</FirstName>
<FamilyName>Vokac</FamilyName>
<Email>vokac@fnal.gov</Email>
<Affiliation>
Institute of Physics of the Czech Academy of Sciences
</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>
Computing for Large Scale Facilities (LHC, FAIR, NICA, SKA, PIC, XFEL, ELI, etc.)
</Track>
</abstract>
<abstract>
<Id>223</Id>
<Title>Status and Future of WLCG</Title>
<Content>
The LHC science program has utilized WLCG, a globally federated computing infrastructure, for the last 10 years to enable its ~10k scientists to publish more than 1000 physics publications in peer reviewed journals. This infrastructure has grown to provide ~750k cores, 400 PB disk space, 600 PB of archival storage, as well as high capacity networks to connect all of these. Taking 2016 as a reference, the community processed roughly 10 Trillion collision events, often requiring multiple runs across parts of the primary data. Naïve projections from current practice to the HL-LHC data volumes, taking into account Moore’s law cost reductions of 10-20% per year, predict that computing hardware needs will exceed a flat hardware budget scenario by a factor 10-25. To achieve an efficiency gain at such a scale the community is rethinking the overall LHC computing models. These also have to enable the efficient use of new technologies and take into account the changes in the way computing resources can be provisioned. The presentation will cover the evolution of WLCGs and the current status of the discussion of future computing models.
</Content>
<field id="content">
The LHC science program has utilized WLCG, a globally federated computing infrastructure, for the last 10 years to enable its ~10k scientists to publish more than 1000 physics publications in peer reviewed journals. This infrastructure has grown to provide ~750k cores, 400 PB disk space, 600 PB of archival storage, as well as high capacity networks to connect all of these. Taking 2016 as a reference, the community processed roughly 10 Trillion collision events, often requiring multiple runs across parts of the primary data. Naïve projections from current practice to the HL-LHC data volumes, taking into account Moore’s law cost reductions of 10-20% per year, predict that computing hardware needs will exceed a flat hardware budget scenario by a factor 10-25. To achieve an efficiency gain at such a scale the community is rethinking the overall LHC computing models. These also have to enable the efficient use of new technologies and take into account the changes in the way computing resources can be provisioned. The presentation will cover the evolution of WLCGs and the current status of the discussion of future computing models.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>markus</FirstName>
<FamilyName>schulz</FamilyName>
<Email>markus.schulz@cern.ch</Email>
<Affiliation>CERN</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>markus</FirstName>
<FamilyName>schulz</FamilyName>
<Email>markus.schulz@cern.ch</Email>
<Affiliation>CERN</Affiliation>
</Speaker>
<ContributionType>Plenary</ContributionType>
<Track>
Computing for Large Scale Facilities (LHC, FAIR, NICA, SKA, PIC, XFEL, ELI, etc.)
</Track>
</abstract>
<abstract>
<Id>224</Id>
<Title>JINR Grid Infrastructure: Tier-1 &amp; Tier-2</Title>
<Content>
The JINR grid infrastructure is a main component of the JINR Multifunctional Information and Computing Complex (MICC). There are two grid-sites: the Tier-1 for the CMS experiment at LHC and the Tier-2 which provides support to the virtual organizations (VOs) concerning the JINR participation in experiments at LHC (ATLAS, ALICE, CMS, LHCb), FAIR (CBM, PANDA), and other VOs (NICA, STAR, COMPASS, NOvA) within large-scale international collaborations with JINR researchers. The grid center resources of the MICC JINR are a part of the global grid infrastructure of the Worldwide LHC Computing Grid (WLCG) (), developed for the LHC experiments. Up to 2015 the main element of the JINR grid infrastructure was the Tier-2 centre, one of the best resource centers of the Russian Data Intensive Grid (RDIG) and a part of the global grid infrastructure of WLCG and a member of the European EGI infrastructure. The official inauguration of the JINR Tier-1 for the CMS experiment in March 2015 marked a significant enhancement of the JINR grid computing infrastructure. This was an important addition to both the global system for processing LHC data that come from the Tier-0 center at CERN and from the Tier-1 and Tier-2 centers of the global grid infrastructure of the CMS experiment. The present status of the JINR grid infrastructure and plans for future development will be presented.
</Content>
<field id="content">
The JINR grid infrastructure is a main component of the JINR Multifunctional Information and Computing Complex (MICC). There are two grid-sites: the Tier-1 for the CMS experiment at LHC and the Tier-2 which provides support to the virtual organizations (VOs) concerning the JINR participation in experiments at LHC (ATLAS, ALICE, CMS, LHCb), FAIR (CBM, PANDA), and other VOs (NICA, STAR, COMPASS, NOvA) within large-scale international collaborations with JINR researchers. The grid center resources of the MICC JINR are a part of the global grid infrastructure of the Worldwide LHC Computing Grid (WLCG) (), developed for the LHC experiments. Up to 2015 the main element of the JINR grid infrastructure was the Tier-2 centre, one of the best resource centers of the Russian Data Intensive Grid (RDIG) and a part of the global grid infrastructure of WLCG and a member of the European EGI infrastructure. The official inauguration of the JINR Tier-1 for the CMS experiment in March 2015 marked a significant enhancement of the JINR grid computing infrastructure. This was an important addition to both the global system for processing LHC data that come from the Tier-0 center at CERN and from the Tier-1 and Tier-2 centers of the global grid infrastructure of the CMS experiment. The present status of the JINR grid infrastructure and plans for future development will be presented.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>N.S.</FirstName>
<FamilyName>Astakhov</FamilyName>
<Email>test@test.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Andrey</FirstName>
<FamilyName>Baginyan</FamilyName>
<Email>bagish@mail.ru</Email>
<Affiliation>ccnp</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>E.A.</FirstName>
<FamilyName>Kuznetsov</FamilyName>
<Email>eakuz@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Valery</FirstName>
<FamilyName>Mitsyn</FamilyName>
<Email>vvm@mammoth.jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Igor</FirstName>
<FamilyName>Pelevanyuk</FamilyName>
<Email>gavelock@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Artem</FirstName>
<FamilyName>Petrosyan</FamilyName>
<Email>artem.petrosyan@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>S.V.</FirstName>
<FamilyName>Shmatov</FamilyName>
<Email>sergei.shmatov@cern.ch</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Tatiana</FirstName>
<FamilyName>Strizh</FamilyName>
<Email>strizh@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>V.V.</FirstName>
<FamilyName>Trofimov</FamilyName>
<Email>tvv@test.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Nikolay</FirstName>
<FamilyName>Voytishin</FamilyName>
<Email>voitishinn@gmail.com</Email>
<Affiliation>LIT</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Victor</FirstName>
<FamilyName>Zhiltsov</FamilyName>
<Email>zhiltsov@jinr.ru</Email>
<Affiliation>Lead Engineer</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Nikita</FirstName>
<FamilyName>Balashov</FamilyName>
<Email>balashov.nikita@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Sergey</FirstName>
<FamilyName>Belov</FamilyName>
<Email>belov@jinr.ru</Email>
<Affiliation>Joint Institute for Nuclear Research</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Andrey</FirstName>
<FamilyName>Dolbilov</FamilyName>
<Email>dolbilov@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>A.O.</FirstName>
<FamilyName>Golunov</FamilyName>
<Email>test111@test.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Natalia</FirstName>
<FamilyName>Gromova</FamilyName>
<Email>grom@jinr.ru</Email>
<Affiliation>Ivanovna</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Ivan</FirstName>
<FamilyName>Kadochnikov</FamilyName>
<Email>kadivas@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Ivan</FirstName>
<FamilyName>Kashunin</FamilyName>
<Email>miramir@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Vladimir</FirstName>
<FamilyName>Korenkov</FamilyName>
<Email>korenkov@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Tatiana</FirstName>
<FamilyName>Strizh</FamilyName>
<Email>strizh@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>Plenary</ContributionType>
<Track>Distributed Computing. GRID &amp; Cloud Computing</Track>
</abstract>
<abstract>
<Id>225</Id>
<Title>
Improving site efficiency by integrating storage nodes and batch processing
</Title>
<Content>
The T0 at CERN operates large storage and computing farms for the LHC community. For economic reasons the hardware of the disk servers is, with respect to CPU and memory, virtually identical to the one used in the batch nodes. Monitoring data showed that these nodes are not running anywhere close to their computational limit. Proof of concept tests have been conducted by Andrey Kiryanov showing that more than 80% of the node capacity can be used for computational tasks while creating no detrimental effect on the peak I/O rates. These results have been show at HEPIX 20017. Our team at CERN is expanding the concept, in the BEER (Batch on EOS Extra Resources ) project, to be ready to be integrated into the production service. The approach to partition the resources, the strategy for configuration management and results with production workloads will be shown.
</Content>
<field id="content">
The T0 at CERN operates large storage and computing farms for the LHC community. For economic reasons the hardware of the disk servers is, with respect to CPU and memory, virtually identical to the one used in the batch nodes. Monitoring data showed that these nodes are not running anywhere close to their computational limit. Proof of concept tests have been conducted by Andrey Kiryanov showing that more than 80% of the node capacity can be used for computational tasks while creating no detrimental effect on the peak I/O rates. These results have been show at HEPIX 20017. Our team at CERN is expanding the concept, in the BEER (Batch on EOS Extra Resources ) project, to be ready to be integrated into the production service. The approach to partition the resources, the strategy for configuration management and results with production workloads will be shown.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>markus</FirstName>
<FamilyName>schulz</FamilyName>
<Email>markus.schulz@cern.ch</Email>
<Affiliation>CERN</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Andrey</FirstName>
<FamilyName>Kiryanov</FamilyName>
<Email>globus@pnpi.nw.ru</Email>
<Affiliation>PNPI</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Luca</FirstName>
<FamilyName>Mascetti</FamilyName>
<Email>luca.mascetti@cern.ch</Email>
<Affiliation>CERN</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Massimo</FirstName>
<FamilyName>Lamanna</FamilyName>
<Email>massimo.lamanna@cern.ch</Email>
<Affiliation>CERN</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>David</FirstName>
<FamilyName>Smith</FamilyName>
<Email>david@test.com</Email>
<Affiliation>CERN</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Ben</FirstName>
<FamilyName>Jones</FamilyName>
<Email>ben@test.com</Email>
<Affiliation>CERN</Affiliation>
</Co-Author>
<Speaker>
<FirstName>markus</FirstName>
<FamilyName>schulz</FamilyName>
<Email>markus.schulz@cern.ch</Email>
<Affiliation>CERN</Affiliation>
</Speaker>
<ContributionType>Plenary</ContributionType>
<Track>
Computing for Large Scale Facilities (LHC, FAIR, NICA, SKA, PIC, XFEL, ELI, etc.)
</Track>
</abstract>
<abstract>
<Id>226</Id>
<Title>
Multy-level monitoring system for Multifunctional Information and Computing Complex at JINR
</Title>
<Content>
Multifunctional Information and Computing Complex(MICC) is one of the basic scientific facilities of Joint Institute for Nuclear Research. It provides a 24×7 implementation of a vast range of competitive research conducted at JINR at a global level. MICC consists of for major components: grid-infrastructure, central computing complex, JINR private cloud and high performance heterogeneous cluster HybriLIT. All major components rely on network and engineering infrastructure. It is important to supervise all of the components on three levels: hardware level, network level, and service level. Currently there are many monitoring systems built on different technologies which are used by different user groups and administrators. All monitoring systems are mostly independent despite the fact that some systems collect the same monitoring data. Their independence makes it difficult to see the whole picture of effectiveness and bottle-necks of MICC because the data are scattered among many systems. The role of multy-level monitoring system for MICC is to unite existing systems and solve that problem: provide high level information about the whole computing complex and its services. All MICC current monitoring systems, approaches and methods are described and analyzed in this work: monitoring platforms, data collection methods, data storage, visualization, notification and analytics.
</Content>
<field id="content">
Multifunctional Information and Computing Complex(MICC) is one of the basic scientific facilities of Joint Institute for Nuclear Research. It provides a 24×7 implementation of a vast range of competitive research conducted at JINR at a global level. MICC consists of for major components: grid-infrastructure, central computing complex, JINR private cloud and high performance heterogeneous cluster HybriLIT. All major components rely on network and engineering infrastructure. It is important to supervise all of the components on three levels: hardware level, network level, and service level. Currently there are many monitoring systems built on different technologies which are used by different user groups and administrators. All monitoring systems are mostly independent despite the fact that some systems collect the same monitoring data. Their independence makes it difficult to see the whole picture of effectiveness and bottle-necks of MICC because the data are scattered among many systems. The role of multy-level monitoring system for MICC is to unite existing systems and solve that problem: provide high level information about the whole computing complex and its services. All MICC current monitoring systems, approaches and methods are described and analyzed in this work: monitoring platforms, data collection methods, data storage, visualization, notification and analytics.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Igor</FirstName>
<FamilyName>Pelevanyuk</FamilyName>
<Email>gavelock@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Nikolay</FirstName>
<FamilyName>Astakhov</FamilyName>
<Email>aann@test.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Natalia</FirstName>
<FamilyName>Gromova</FamilyName>
<Email>grom@jinr.ru</Email>
<Affiliation>Ivanovna</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Ivan</FirstName>
<FamilyName>Kadochnikov</FamilyName>
<Email>kadivas@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Ivan</FirstName>
<FamilyName>Kashunin</FamilyName>
<Email>miramir@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Vladimir</FirstName>
<FamilyName>Korenkov</FamilyName>
<Email>korenkov@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Nikolay</FirstName>
<FamilyName>Kutovskiy</FamilyName>
<Email>kut@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Aleksandr</FirstName>
<FamilyName>Mayorov</FamilyName>
<Email>sanek_majorov@mail.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Valery</FirstName>
<FamilyName>Mitsyn</FamilyName>
<Email>vvm@mammoth.jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Sergey</FirstName>
<FamilyName>Shmatov</FamilyName>
<Email>sergei.shmatov@cern.ch</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Tatiana</FirstName>
<FamilyName>Strizh</FamilyName>
<Email>strizh@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Vladimir</FirstName>
<FamilyName>Trofimov</FamilyName>
<Email>tvv@test.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Andrey</FirstName>
<FamilyName>Baginyan</FamilyName>
<Email>bagish@mail.ru</Email>
<Affiliation>ccnp</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Martin</FirstName>
<FamilyName>Vala</FamilyName>
<Email>mvala@saske.sk</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Nikolay</FirstName>
<FamilyName>Voytishin</FamilyName>
<Email>voitishinn@gmail.com</Email>
<Affiliation>LIT</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Victor</FirstName>
<FamilyName>Zhiltsov</FamilyName>
<Email>zhiltsov@jinr.ru</Email>
<Affiliation>Lead Engineer</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Nikita</FirstName>
<FamilyName>Balashov</FamilyName>
<Email>balashov.nikita@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Dmitry</FirstName>
<FamilyName>Belyakov</FamilyName>
<Email>dmitry@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Sergey</FirstName>
<FamilyName>Belov</FamilyName>
<Email>belov@jinr.ru</Email>
<Affiliation>Joint Institute for Nuclear Research</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Yurii</FirstName>
<FamilyName>Butenko</FamilyName>
<Email>gohas94@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Andrey</FirstName>
<FamilyName>Dolbilov</FamilyName>
<Email>dolbilov@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Alexei</FirstName>
<FamilyName>Golunov</FamilyName>
<Email>aagg@jtest.com</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Alexandr</FirstName>
<FamilyName>Baranov</FamilyName>
<Email>baranov@jinr.ru</Email>
<Affiliation>(JINR)</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Igor</FirstName>
<FamilyName>Pelevanyuk</FamilyName>
<Email>gavelock@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>Distributed Computing. GRID &amp; Cloud Computing</Track>
</abstract>
<abstract>
<Id>227</Id>
<Title>JINR Member States cloud infrastructure</Title>
<Content>
One of the possible ways to speed up scientific research projects which JINR and organizations from its Member States participate in is to join computational resources. It can be done in a few ways one of which is to build distributed cloud infrastructures integrating local private clouds of JINR and organizations from its Member States. To implement such scenario a cloud bursting based approach was chosen since it provides more flexibility in resources management of each cloud integrated into such distributed infrastructure. For the time being a few private clouds from some organizations from JINR Member States has been already integrated with the JINR one with help of custom cloud bursting driver developed by the JINR team. Various aspects of that activity are covered in more details in the article as well as implemented and planned changes in the JINR cloud such as a smart scheduler deployment in production, migration from DRBD high-availability (HA) to native OpenNebula HA, Ceph storage deployment and VMs' disks migration to it and others.
</Content>
<field id="content">
One of the possible ways to speed up scientific research projects which JINR and organizations from its Member States participate in is to join computational resources. It can be done in a few ways one of which is to build distributed cloud infrastructures integrating local private clouds of JINR and organizations from its Member States. To implement such scenario a cloud bursting based approach was chosen since it provides more flexibility in resources management of each cloud integrated into such distributed infrastructure. For the time being a few private clouds from some organizations from JINR Member States has been already integrated with the JINR one with help of custom cloud bursting driver developed by the JINR team. Various aspects of that activity are covered in more details in the article as well as implemented and planned changes in the JINR cloud such as a smart scheduler deployment in production, migration from DRBD high-availability (HA) to native OpenNebula HA, Ceph storage deployment and VMs' disks migration to it and others.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Nikita</FirstName>
<FamilyName>Balashov</FamilyName>
<Email>balashov.nikita@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Alexandr</FirstName>
<FamilyName>Baranov</FamilyName>
<Email>baranov@jinr.ru</Email>
<Affiliation>(JINR)</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Nikolay</FirstName>
<FamilyName>Kutovskiy</FamilyName>
<Email>kut@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Yelena</FirstName>
<FamilyName>Mazhitova</FamilyName>
<Email>emazhitova@jinr.ru</Email>
<Affiliation>Joint Institute for Nuclear Research</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Roman</FirstName>
<FamilyName>Semenov</FamilyName>
<Email>roman@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Nikita</FirstName>
<FamilyName>Balashov</FamilyName>
<Email>balashov.nikita@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>Distributed Computing. GRID &amp; Cloud Computing</Track>
</abstract>
<abstract>
<Id>228</Id>
<Title>
SALSA - Scalable Adaptive Large Structures Analysis
</Title>
<Content>
Data environments are growing exponentially and the complexity of data analysis is becoming critical issue. The goal of SALSA project is to provide tools to make connection between human and computer to understand and learn from each other. Analysis of diﬀerent parameters in N-dimensional space should be made easy and intuitive. Task distribution system has to be adaptive to the enviroment where analysis is done and has provide easy access and interactivity to the user. SALSA contains distribution network system that can constructed at level of clusters, nodes, processes and threads and will be able to build any tree strucure. User interface is implemented as web service that can connect to SALSA network and distribute tasks to workers. Web application is using latest web techonlogies like Angular, WebSockets to provide interactivity and dynamism. JavaScript ROOT (JSROOT) package is used as analysis interface. EOS storage support with JSROOT is included to provide prossibility to browse ﬁles and view results on web browser. Users can create, delete, start and stop tasks. The web application has several templates for diﬀerent types of user tasks that makes it possible to quickly create new task and submit it to the SALSA network.
</Content>
<field id="content">
Data environments are growing exponentially and the complexity of data analysis is becoming critical issue. The goal of SALSA project is to provide tools to make connection between human and computer to understand and learn from each other. Analysis of diﬀerent parameters in N-dimensional space should be made easy and intuitive. Task distribution system has to be adaptive to the enviroment where analysis is done and has provide easy access and interactivity to the user. SALSA contains distribution network system that can constructed at level of clusters, nodes, processes and threads and will be able to build any tree strucure. User interface is implemented as web service that can connect to SALSA network and distribute tasks to workers. Web application is using latest web techonlogies like Angular, WebSockets to provide interactivity and dynamism. JavaScript ROOT (JSROOT) package is used as analysis interface. EOS storage support with JSROOT is included to provide prossibility to browse ﬁles and view results on web browser. Users can create, delete, start and stop tasks. The web application has several templates for diﬀerent types of user tasks that makes it possible to quickly create new task and submit it to the SALSA network.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Martin</FirstName>
<FamilyName>Vala</FamilyName>
<Email>mvala@saske.sk</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>M.</FirstName>
<FamilyName>Fedor</FamilyName>
<Email>fm@test.com</Email>
<Affiliation>SPSEKE, Kosice, Slovakia</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Aleksandr</FirstName>
<FamilyName>Mayorov</FamilyName>
<Email>sanek_majorov@mail.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Yurii</FirstName>
<FamilyName>Butenko</FamilyName>
<Email>gohas94@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Martin</FirstName>
<FamilyName>Vala</FamilyName>
<Email>mvala@saske.sk</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>
Computations with Hybrid Systems (CPU, GPU, coprocessors)
</Track>
</abstract>
</AbstractBook>